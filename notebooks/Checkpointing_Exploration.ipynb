{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Impact from Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint as pp\n",
    "from time import time as tt\n",
    "\n",
    "# External imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter, segment_csr, scatter_add\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_cluster import knn_graph, radius_graph\n",
    "import trackml.dataset\n",
    "import torch_geometric\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Pick up local packages\n",
    "sys.path.append('..')\n",
    "\n",
    "# Local imports\n",
    "from prepare_utils import *\n",
    "from performance_utils import *\n",
    "from toy_utils import *\n",
    "from models import *\n",
    "from trainers import *\n",
    "%matplotlib inline\n",
    "\n",
    "# Get rid of RuntimeWarnings, gross\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import wandb\n",
    "import faiss\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "torch_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Scrubbed Filter-ready Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_dir, num):\n",
    "    all_events = os.listdir(input_dir)\n",
    "    all_events = sorted([os.path.join(input_dir, event) for event in all_events])\n",
    "    loaded_events = [torch.load(event, map_location=torch.device('cpu')) for event in all_events[:num]]\n",
    "\n",
    "    return loaded_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_cut = 0\n",
    "train_number = 100\n",
    "test_number = 20\n",
    "load_dir = \"/global/cscratch1/sd/danieltm/ExaTrkX/trackml_processed/filter_processed/\"\n",
    "basename = os.path.join(load_dir, str(pt_cut) + \"_pt_cut_endcaps_connected\")\n",
    "train_path = os.path.join(basename, \"train\")\n",
    "test_path = os.path.join(basename, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 368 ms, sys: 2.53 s, total: 2.9 s\n",
      "Wall time: 7.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "train_dataset = load_dataset(train_path, train_number)\n",
    "test_dataset = load_dataset(test_path, test_number)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Memory Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     22,
     46,
     237,
     305,
     370
    ]
   },
   "outputs": [],
   "source": [
    "class EdgeNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which computes weights for edges of the graph.\n",
    "    For each edge, it selects the associated nodes' features\n",
    "    and applies some fully-connected network layers with a final\n",
    "    sigmoid activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=8, hidden_activation='Tanh',\n",
    "                 layer_norm=True):\n",
    "        super(EdgeNetwork, self).__init__()\n",
    "        self.network = make_mlp(input_dim*2,\n",
    "                                [hidden_dim, hidden_dim, hidden_dim, 1],\n",
    "                                hidden_activation=hidden_activation,\n",
    "                                output_activation=None,\n",
    "                                layer_norm=layer_norm)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Select the features of the associated nodes\n",
    "        start, end = inputs[1]\n",
    "        edge_inputs = torch.cat([inputs[0][start], inputs[0][end]], dim=1)\n",
    "        return self.network(edge_inputs).squeeze(-1)\n",
    "\n",
    "class NodeNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which computes new node features on the graph.\n",
    "    For each node, it aggregates the neighbor node features\n",
    "    (separately on the input and output side), and combines\n",
    "    them with the node's previous features in a fully-connected\n",
    "    network to compute the new features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_activation='Tanh',\n",
    "                 layer_norm=True):\n",
    "        super(NodeNetwork, self).__init__()\n",
    "        self.network = make_mlp(input_dim*3, [output_dim]*4,\n",
    "                                hidden_activation=hidden_activation,\n",
    "                                output_activation=None,\n",
    "                                layer_norm=layer_norm)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        start, end = inputs[2]\n",
    "        # Aggregate edge-weighted incoming/outgoing features\n",
    "        mi = scatter_add(inputs[1][:, None] * inputs[0][start], end, dim=0, dim_size=inputs[0].shape[0])\n",
    "        mo = scatter_add(inputs[1][:, None] * inputs[0][end], start, dim=0, dim_size=inputs[0].shape[0])\n",
    "        node_inputs = torch.cat([mi, mo, inputs[0]], dim=1)\n",
    "        return self.network(node_inputs)\n",
    "\n",
    "class ResAGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Segment classification graph neural network model.\n",
    "    Consists of an input network, an edge network, and a node network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, hidden_dim=8, n_graph_iters=3,\n",
    "                 hidden_activation=\"Tanh\", layer_norm=True):\n",
    "        super(ResAGNN, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "        # Setup the input network\n",
    "        self.input_network = make_mlp(in_channels, [hidden_dim],\n",
    "                                      output_activation=hidden_activation,\n",
    "                                      layer_norm=layer_norm)\n",
    "        # Setup the edge network\n",
    "        self.edge_network = EdgeNetwork(in_channels + hidden_dim, in_channels + hidden_dim,\n",
    "                                        hidden_activation, layer_norm=layer_norm)\n",
    "        # Setup the node layers\n",
    "        self.node_network = NodeNetwork(in_channels + hidden_dim, hidden_dim,\n",
    "                                        hidden_activation, layer_norm=layer_norm)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"Apply forward pass of the model\"\"\"\n",
    "        input_x = x\n",
    "        x = self.input_network(x)\n",
    "        # Shortcut connect the inputs onto the hidden representation\n",
    "        x = torch.cat([x, input_x], dim=-1)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # Loop over iterations of edge and node networks\n",
    "        for i in range(self.n_graph_iters):\n",
    "            x_inital = x\n",
    "            \n",
    "            # Apply edge network\n",
    "            e = torch.sigmoid(self.edge_network((x, edge_index)))\n",
    "        \n",
    "            # Apply node network\n",
    "            x = self.node_network((x, e, edge_index))\n",
    "            \n",
    "            # Shortcut connect the inputs onto the hidden representation\n",
    "            x = torch.cat([x, input_x], dim=-1)  \n",
    "            \n",
    "            x = x_inital + x\n",
    "        \n",
    "        return self.edge_network((x, edge_index))\n",
    "\n",
    "class ResAGNN_Stripped(nn.Module):\n",
    "    \"\"\"\n",
    "    A message-passing graph network which takes a graph with:\n",
    "    - bi-directional edges\n",
    "    - node features, no edge features\n",
    "\n",
    "    and applies the following modules:\n",
    "    - a graph encoder (no message passing)\n",
    "    - recurrent edge and node networks\n",
    "    - an edge classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3, hidden_dim=8, n_graph_iters=3,\n",
    "                 hidden_activation=\"Tanh\", layer_norm=True):\n",
    "        super(ResAGNN_Stripped, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "\n",
    "        # The node encoder transforms input node features to the hidden space\n",
    "        self.node_encoder = make_mlp(in_channels, [hidden_dim],\n",
    "                                      output_activation=hidden_activation,\n",
    "                                      layer_norm=layer_norm)\n",
    "\n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_network = make_mlp(2*(in_channels + hidden_dim),\n",
    "                                     [hidden_dim]*3+[1],\n",
    "                                     layer_norm=layer_norm,\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hidden_activation)\n",
    "\n",
    "        # The node network computes new node features\n",
    "        self.node_network = make_mlp((in_channels + hidden_dim)*2,\n",
    "                                     [hidden_dim]*4,\n",
    "                                     layer_norm=layer_norm,\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hidden_activation)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        # Encode the graph features into the hidden space\n",
    "        input_x = x\n",
    "        x = self.node_encoder(x)\n",
    "        x = torch.cat([x, input_x], dim=-1)\n",
    "        \n",
    "        start, end = edge_index\n",
    "        \n",
    "        # Loop over graph iterations\n",
    "        for i in range(self.n_graph_iters):\n",
    "\n",
    "            # Previous hidden state\n",
    "            x0 = x\n",
    "\n",
    "            # Compute new edge score\n",
    "            edge_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "            e = torch.sigmoid(self.edge_network(edge_inputs))\n",
    "            \n",
    "            # Sum weighted node features coming into each node\n",
    "#             weighted_messages_in = scatter_add(e * x[start], end, dim=0, dim_size=x.shape[0])\n",
    "#             weighted_messages_out = scatter_add(e * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "            \n",
    "            weighted_messages = scatter_add(e * x[start], end, dim=0, dim_size=x.shape[0]) + scatter_add(e * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "\n",
    "            # Compute new node features\n",
    "#             node_inputs = torch.cat([x, weighted_messages_in, weighted_messages_out], dim=1)\n",
    "            node_inputs = torch.cat([x, weighted_messages], dim=1)\n",
    "            x = self.node_network(node_inputs)\n",
    "\n",
    "            # Residual connection\n",
    "            x = torch.cat([x, input_x], dim=-1)\n",
    "            x = x + x0\n",
    "\n",
    "        # Compute final edge scores; use original edge directions only\n",
    "        clf_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "        return checkpoint(self.edge_network, clf_inputs).squeeze(-1)\n",
    "    \n",
    "class CheckResAGNN_Stripped(nn.Module):\n",
    "    \"\"\"\n",
    "    A message-passing graph network which takes a graph with:\n",
    "    - bi-directional edges\n",
    "    - node features, no edge features\n",
    "\n",
    "    and applies the following modules:\n",
    "    - a graph encoder (no message passing)\n",
    "    - recurrent edge and node networks\n",
    "    - an edge classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3, hidden_dim=8, n_graph_iters=3,\n",
    "                 hidden_activation=\"Tanh\", layer_norm=True):\n",
    "        super(CheckResAGNN_Stripped, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "\n",
    "        # The node encoder transforms input node features to the hidden space\n",
    "        self.node_encoder = make_mlp(in_channels, [hidden_dim],\n",
    "                                      output_activation=hidden_activation,\n",
    "                                      layer_norm=layer_norm)\n",
    "\n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_network = make_mlp(2*(in_channels + hidden_dim),\n",
    "                                     [hidden_dim]*3+[1],\n",
    "                                     layer_norm=layer_norm,\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hidden_activation)\n",
    "\n",
    "        # The node network computes new node features\n",
    "        self.node_network = make_mlp((in_channels + hidden_dim)*2,\n",
    "                                     [hidden_dim]*4,\n",
    "                                     layer_norm=layer_norm,\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hidden_activation)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        # Encode the graph features into the hidden space\n",
    "        input_x = x\n",
    "        x = self.node_encoder(x)\n",
    "        x = torch.cat([x, input_x], dim=-1)\n",
    "        \n",
    "        start, end = edge_index\n",
    "        \n",
    "        # Loop over graph iterations\n",
    "        for i in range(self.n_graph_iters):\n",
    "\n",
    "            # Previous hidden state\n",
    "            x0 = x\n",
    "\n",
    "            # Compute new edge score\n",
    "            edge_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "            e = checkpoint(self.edge_network, edge_inputs)\n",
    "            e = torch.sigmoid(e)\n",
    "            \n",
    "            # Sum weighted node features coming into each node\n",
    "#             weighted_messages_in = scatter_add(e * x[start], end, dim=0, dim_size=x.shape[0])\n",
    "#             weighted_messages_out = scatter_add(e * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "            \n",
    "            weighted_messages = scatter_add(e * x[start], end, dim=0, dim_size=x.shape[0]) + scatter_add(e * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "\n",
    "            # Compute new node features\n",
    "#             node_inputs = torch.cat([x, weighted_messages_in, weighted_messages_out], dim=1)\n",
    "            node_inputs = torch.cat([x, weighted_messages], dim=1)\n",
    "            x = checkpoint(self.node_network, node_inputs)\n",
    "\n",
    "            # Residual connection\n",
    "            x = torch.cat([x, input_x], dim=-1)\n",
    "            x = x + x0\n",
    "\n",
    "        # Compute final edge scores; use original edge directions only\n",
    "        clf_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "        return checkpoint(self.edge_network, clf_inputs).squeeze(-1)\n",
    "\n",
    "class CheckResAGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Segment classification graph neural network model.\n",
    "    Consists of an input network, an edge network, and a node network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, hidden_dim=8, n_graph_iters=3,\n",
    "                 hidden_activation=\"Tanh\", layer_norm=True):\n",
    "        super(CheckResAGNN, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "        # Setup the input network\n",
    "        self.input_network = make_mlp(in_channels, [hidden_dim],\n",
    "                                      output_activation=hidden_activation,\n",
    "                                      layer_norm=layer_norm)\n",
    "        # Setup the edge network\n",
    "        self.edge_network = EdgeNetwork(in_channels + hidden_dim, in_channels + hidden_dim,\n",
    "                                        hidden_activation, layer_norm=layer_norm)\n",
    "        # Setup the node layers\n",
    "        self.node_network = NodeNetwork(in_channels + hidden_dim, hidden_dim,\n",
    "                                        hidden_activation, layer_norm=layer_norm)\n",
    "        self.dummy_tensor = torch.ones(1, dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "    def custom_forward(self, inputs):\n",
    "        # Apply edge network\n",
    "#         print(\"x in check (before)\", inputs[0], inputs[0].requires_grad)\n",
    "        e = self.edge_network((inputs[0], inputs[1]))\n",
    "        e = torch.sigmoid(e)\n",
    "        x = self.node_network((inputs[0], e, inputs[1]))\n",
    "#         print(\"e in check\", e, e.requires_grad)\n",
    "#         print(\"x in check (after)\", x, x.requires_grad)\n",
    "        # Apply node network\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"Apply forward pass of the model\"\"\"\n",
    "        input_x = x\n",
    "        x = self.input_network(x)\n",
    "        # Shortcut connect the inputs onto the hidden representation\n",
    "        x = torch.cat([x, input_x], dim=-1)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # Loop over iterations of edge and node networks\n",
    "        for i in range(self.n_graph_iters):\n",
    "            \n",
    "            x_inital = x\n",
    "#             print(\"x before check\", x, x.requires_grad)\n",
    "            x = checkpoint(self.custom_forward, (x, edge_index, self.dummy_tensor))\n",
    "#             x = self.custom_forward((x, edge_index))\n",
    "#             print(\"x after check\", x, x.requires_grad)\n",
    "#             x.requires_grad_(True)\n",
    "            # Apply edge network\n",
    "#             e = checkpoint(self.edge_network, (x, edge_index))\n",
    "#             e = self.edge_network((x, edge_index))\n",
    "#             e = torch.sigmoid(e)\n",
    "        \n",
    "            # Apply node network\n",
    "#             x = checkpoint(self.node_network, (x, e, edge_index))\n",
    "#             x = self.node_network((x, e, edge_index))\n",
    "            \n",
    "            # Shortcut connect the inputs onto the hidden representation\n",
    "            x = torch.cat([x, input_x], dim=-1)  \n",
    "            \n",
    "            x = x_inital + x\n",
    "            \n",
    "        e = self.edge_network((x, edge_index))\n",
    "#         e = torch.sigmoid(e)\n",
    "        \n",
    "        return e\n",
    "    \n",
    "class MPNN_Network(nn.Module):\n",
    "    \"\"\"\n",
    "    A message-passing graph network which takes a graph with:\n",
    "    - bi-directional edges\n",
    "    - node features, no edge features\n",
    "\n",
    "    and applies the following modules:\n",
    "    - a graph encoder (no message passing)\n",
    "    - recurrent edge and node networks\n",
    "    - an edge classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_node_dim, hidden_edge_dim, in_layers, node_layers, edge_layers,\n",
    "                 n_graph_iters=1, layer_norm=True):\n",
    "        super(MPNN_Network, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "\n",
    "        # The node encoder transforms input node features to the hidden space\n",
    "        self.node_encoder = make_mlp(input_dim, [hidden_node_dim]*in_layers)\n",
    "\n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_network = make_mlp(2*hidden_node_dim,\n",
    "                                     [hidden_edge_dim]*edge_layers,\n",
    "                                     layer_norm=layer_norm)\n",
    "\n",
    "        # The node network computes new node features\n",
    "        self.node_network = make_mlp(hidden_node_dim + hidden_edge_dim,\n",
    "                                     [hidden_node_dim]*node_layers,\n",
    "                                     layer_norm=layer_norm)\n",
    "\n",
    "        # The edge classifier computes final edge scores\n",
    "        self.edge_classifier = make_mlp(2*hidden_node_dim,\n",
    "                                        [hidden_edge_dim, 1],\n",
    "                                        output_activation=None)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        # Encode the graph features into the hidden space\n",
    "        x = self.node_encoder(x)\n",
    "\n",
    "        # Loop over graph iterations\n",
    "        for i in range(self.n_graph_iters):\n",
    "\n",
    "            # Previous hidden state\n",
    "            x0 = x\n",
    "\n",
    "            # Compute new edge features\n",
    "            edge_inputs = torch.cat([x[edge_index[0]], x[edge_index[1]]], dim=1)\n",
    "            e = self.edge_network(edge_inputs)\n",
    "\n",
    "            # Sum edge features coming into each node\n",
    "            aggr_messages = scatter_add(e, edge_index[1], dim=0, dim_size=x.shape[0])\n",
    "\n",
    "            # Compute new node features\n",
    "            node_inputs = torch.cat([x, aggr_messages], dim=1)\n",
    "            x = self.node_network(node_inputs)\n",
    "\n",
    "            # Residual connection\n",
    "            x = x + x0\n",
    "\n",
    "        # Compute final edge scores; use original edge directions only\n",
    "        start_idx, end_idx = edge_index\n",
    "        clf_inputs = torch.cat([x[start_idx], x[end_idx]], dim=1)\n",
    "        return self.edge_classifier(clf_inputs).squeeze(-1)\n",
    "    \n",
    "class CheckMPNN_Network(nn.Module):\n",
    "    \"\"\"\n",
    "    A message-passing graph network which takes a graph with:\n",
    "    - bi-directional edges\n",
    "    - node features, no edge features\n",
    "\n",
    "    and applies the following modules:\n",
    "    - a graph encoder (no message passing)\n",
    "    - recurrent edge and node networks\n",
    "    - an edge classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_node_dim, hidden_edge_dim, in_layers, node_layers, edge_layers,\n",
    "                 n_graph_iters=1, layer_norm=True):\n",
    "        super(CheckMPNN_Network, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "\n",
    "        # The node encoder transforms input node features to the hidden space\n",
    "        self.node_encoder = make_mlp(input_dim, [hidden_node_dim]*in_layers)\n",
    "\n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_network = make_mlp(2*hidden_node_dim,\n",
    "                                     [hidden_edge_dim]*edge_layers,\n",
    "                                     layer_norm=layer_norm)\n",
    "\n",
    "        # The node network computes new node features\n",
    "        self.node_network = make_mlp(hidden_node_dim + hidden_edge_dim,\n",
    "                                     [hidden_node_dim]*node_layers,\n",
    "                                     layer_norm=layer_norm)\n",
    "\n",
    "        # The edge classifier computes final edge scores\n",
    "        self.edge_classifier = make_mlp(2*hidden_node_dim,\n",
    "                                        [hidden_edge_dim, 1],\n",
    "                                        output_activation=None)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        # Encode the graph features into the hidden space\n",
    "        x = self.node_encoder(x)\n",
    "\n",
    "        # Loop over graph iterations\n",
    "        for i in range(self.n_graph_iters):\n",
    "\n",
    "            # Previous hidden state\n",
    "            x0 = x\n",
    "\n",
    "            # Compute new edge features\n",
    "            edge_inputs = torch.cat([x[edge_index[0]], x[edge_index[1]]], dim=1)\n",
    "            e = checkpoint(self.edge_network, edge_inputs)\n",
    "\n",
    "            # Sum edge features coming into each node\n",
    "            aggr_messages = scatter_add(e, edge_index[1], dim=0, dim_size=x.shape[0])\n",
    "\n",
    "            # Compute new node features\n",
    "            node_inputs = torch.cat([x, aggr_messages], dim=1)\n",
    "            x = checkpoint(self.node_network, node_inputs)\n",
    "\n",
    "            # Residual connection\n",
    "            x = x + x0\n",
    "\n",
    "        # Compute final edge scores; use original edge directions only\n",
    "        start_idx, end_idx = edge_index\n",
    "        clf_inputs = torch.cat([x[start_idx], x[end_idx]], dim=1)\n",
    "        return checkpoint(self.edge_classifier, clf_inputs).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.max_memory_allocated() /1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmurnanedaniel\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201015_124454-b9ijufce\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdry-snowflake-188\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/murnanedaniel/CheckpointExploration\" target=\"_blank\">https://wandb.ai/murnanedaniel/CheckpointExploration</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/murnanedaniel/CheckpointExploration/runs/b9ijufce\" target=\"_blank\">https://wandb.ai/murnanedaniel/CheckpointExploration/runs/b9ijufce</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "m_configs = {\"in_channels\": 3, \"hidden_dim\": 16, \"n_graph_iters\": 8}\n",
    "model = ResAGNN_Stripped(**m_configs).to(device)\n",
    "\n",
    "# m_configs = {\"input_dim\": 3, \"hidden_node_dim\": 64, \"hidden_edge_dim\": 64, \"in_layers\": 1, \"node_layers\": 3, \"edge_layers\": 3, \"n_graph_iters\": 8, \"layer_norm\": True}\n",
    "# model = MPNN_Network(**m_configs).to(device)\n",
    "\n",
    "model_name = wandb.init(project=\"CheckpointExploration\", config=m_configs)\n",
    "wandb.watch(model, log='all')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=5)\n",
    "\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.4269, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "1 tensor(1.4209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "2 tensor(1.3824, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "3 tensor(1.3996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "4 tensor(1.3730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "5 tensor(1.3636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "6 tensor(1.3566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "7 tensor(1.3566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "8 tensor(1.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "9 tensor(1.3475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "10 tensor(1.3379, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "11 tensor(1.3619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "12 tensor(1.3606, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "13 tensor(1.3383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "14 tensor(1.3490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "15 tensor(1.3509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "16 tensor(1.3091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "17 tensor(1.3279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "18 tensor(1.3302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "19 tensor(1.3395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "20 tensor(1.3485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "21 tensor(1.3355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "22 tensor(1.3260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "23 tensor(1.3185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "24 tensor(1.3167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "25 tensor(1.3129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "26 tensor(1.3110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "27 tensor(1.2891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "28 tensor(1.2616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "29 tensor(1.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "30 tensor(1.2680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "31 tensor(1.2792, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "32 tensor(1.2970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "33 tensor(1.2274, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "34 tensor(1.2617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "35 tensor(1.2797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "36 tensor(1.2486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "37 tensor(1.2804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "38 tensor(1.2533, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "39 tensor(1.2410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "40 tensor(1.2433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "41 tensor(1.2712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "42 tensor(1.2631, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "43 tensor(1.2648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "44 tensor(1.2379, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "45 tensor(1.2613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "46 tensor(1.2342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "47 tensor(1.2395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "48 tensor(1.2154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "49 tensor(1.2385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "50 tensor(1.2260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "51 tensor(1.2423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "52 tensor(1.2148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "53 tensor(1.1596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "54 tensor(1.2367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "55 tensor(1.2491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "56 tensor(1.2268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "57 tensor(1.2402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "58 tensor(1.2337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "59 tensor(1.2369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "60 tensor(1.2234, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "61 tensor(1.2206, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "62 tensor(1.2175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "63 tensor(1.2293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "64 tensor(1.2460, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "65 tensor(1.2261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "66 tensor(1.2026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "67 tensor(1.2141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "68 tensor(1.2055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "69 tensor(1.2450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "70 tensor(1.2148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "71 tensor(1.1835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "72 tensor(1.2249, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "73 tensor(1.2553, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "74 tensor(1.2433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "75 tensor(1.2342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "76 tensor(1.2624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "77 tensor(1.2420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "78 tensor(1.2206, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "79 tensor(1.2094, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "80 tensor(1.2401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "81 tensor(1.2417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "82 tensor(1.2264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "83 tensor(1.2036, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "84 tensor(1.2354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "85 tensor(1.1994, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "86 tensor(1.2298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "87 tensor(1.2188, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "88 tensor(1.2234, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "89 tensor(1.2277, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "90 tensor(1.2099, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "91 tensor(1.2086, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "92 tensor(1.1695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "93 tensor(1.2128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "94 tensor(1.2218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "95 tensor(1.2323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "96 tensor(1.2345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "97 tensor(1.2041, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "98 tensor(1.2289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(1.2285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Memory max (GB): 8.780958652496338 - time: 1.0369899272918701\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, e = batch.x.to(device), batch.edge_index.to(device)\n",
    "    weight = torch.tensor((~batch.y.bool()).sum() / batch.y.sum())\n",
    "    combined_y = batch.y_pid.float()         \n",
    "#     x.requires_grad_(True)\n",
    "    \n",
    "    output = model(x, e)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(output, combined_y.to(device), pos_weight = weight)\n",
    "    total_loss += loss.item()\n",
    "    print(i, loss)\n",
    "    tic = tt()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     break\n",
    "    \n",
    "print(\"Memory max (GB):\", torch.cuda.max_memory_allocated() / 1024**3, \"- time:\", tt()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_not_checkpointed = output.data.clone()\n",
    "grad_not_checkpointed = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_not_checkpointed[name] = param.grad.data.clone()\n",
    "    else:\n",
    "        print(name, \"is NONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1709, 0.1630, 0.1718,  ..., 0.2525, 0.2060, 0.2111], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_not_checkpointed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1709, 0.1630, 0.1718,  ..., 0.2525, 0.2060, 0.2111], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_checkpointed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_encoder.0.weight': tensor([[-1.5066e-03, -7.5187e-04, -2.2855e-03],\n",
       "         [ 4.5522e-04, -1.0468e-03, -2.0531e-03],\n",
       "         [-7.0288e-04, -4.6534e-04, -4.4859e-04],\n",
       "         [-4.4661e-04, -1.1200e-03,  8.9562e-04],\n",
       "         [ 4.0046e-04,  1.4957e-03,  4.7784e-04],\n",
       "         [ 1.7397e-03,  1.2750e-03, -6.1366e-04],\n",
       "         [ 1.1351e-03,  4.6967e-04, -2.6968e-04],\n",
       "         [ 4.4861e-04, -2.5037e-03,  3.3685e-03],\n",
       "         [-1.8265e-03,  2.8013e-04,  1.3742e-04],\n",
       "         [-4.4511e-05, -6.4947e-04,  2.5390e-03],\n",
       "         [ 1.0493e-04, -4.2207e-04, -4.8594e-03],\n",
       "         [-1.5353e-05, -1.5799e-04,  3.1580e-03],\n",
       "         [ 8.5638e-04,  2.9611e-03, -3.0343e-04],\n",
       "         [ 9.0987e-04,  3.5355e-04,  5.5008e-04],\n",
       "         [ 3.4069e-04, -8.3479e-05, -5.7403e-04],\n",
       "         [-1.8485e-03,  3.6549e-04,  2.8088e-04]], device='cuda:0'),\n",
       " 'node_encoder.0.bias': tensor([-1.0034e-03,  4.6029e-03,  5.1182e-04,  5.9158e-04,  1.5166e-03,\n",
       "          1.0189e-03,  4.4330e-03,  4.3456e-03, -6.1968e-03,  1.7150e-04,\n",
       "          5.3703e-03, -2.8836e-03, -9.4906e-03,  7.3239e-05, -5.7473e-04,\n",
       "         -2.4863e-03], device='cuda:0'),\n",
       " 'node_encoder.1.weight': tensor([ 9.9291e-04, -5.6682e-04,  5.6536e-05,  9.5336e-04, -2.2453e-04,\n",
       "          2.4505e-04,  1.3654e-03,  2.0172e-04, -6.7914e-04, -4.2318e-04,\n",
       "         -6.7828e-04, -6.0849e-04, -1.0910e-03,  5.7362e-04,  5.1472e-04,\n",
       "          8.4871e-05], device='cuda:0'),\n",
       " 'node_encoder.1.bias': tensor([-7.5766e-05,  2.1466e-03,  6.4645e-04,  5.4691e-04,  1.1254e-03,\n",
       "          6.8350e-04,  2.4741e-03,  2.3992e-03, -2.1007e-03,  3.6469e-04,\n",
       "          2.3877e-03, -9.0526e-04, -4.4660e-03,  2.7401e-04,  4.0418e-05,\n",
       "         -5.3065e-04], device='cuda:0'),\n",
       " 'edge_network.0.weight': tensor([[ 9.7919e-03,  5.0549e-03,  5.7377e-03, -7.3867e-04, -3.5066e-03,\n",
       "          -8.2730e-04,  1.9231e-03, -6.7834e-03, -3.4318e-03,  1.1731e-03,\n",
       "          -1.0288e-02,  4.6736e-03,  9.7351e-03, -1.9147e-03, -2.2950e-04,\n",
       "          -3.8127e-04, -1.1399e-02, -2.7692e-03,  2.4572e-02,  1.0100e-02,\n",
       "           5.8296e-03,  4.1189e-03, -3.5195e-04, -4.2721e-03, -1.3579e-03,\n",
       "           2.3782e-03, -5.9238e-03, -3.5626e-03,  2.2041e-03, -1.0646e-02,\n",
       "           4.7228e-03,  9.2995e-03, -1.6864e-03,  4.6627e-04, -1.5422e-04,\n",
       "          -1.1012e-02, -4.5645e-03,  2.3656e-02],\n",
       "         [-3.2675e-03, -2.3826e-03, -5.4209e-03, -2.1907e-03,  1.3170e-03,\n",
       "           1.5182e-03, -5.8282e-04,  7.5801e-03,  3.6135e-03, -6.8034e-04,\n",
       "           7.4044e-03, -6.1785e-03, -6.3952e-03, -2.4673e-03, -6.0523e-04,\n",
       "          -5.1098e-04, -5.1263e-04,  3.2633e-03,  2.3060e-03, -3.4334e-03,\n",
       "          -2.4195e-03, -5.7377e-03, -1.8368e-03,  8.3738e-04,  1.9369e-03,\n",
       "          -2.0983e-04,  6.9667e-03,  3.7154e-03, -6.5419e-04,  7.2760e-03,\n",
       "          -6.1154e-03, -6.0952e-03, -2.1405e-03, -7.0942e-04,  6.0309e-04,\n",
       "          -4.1951e-04,  5.0891e-03,  4.0713e-03],\n",
       "         [-3.8034e-03, -5.9968e-03, -1.1787e-03,  2.8496e-04,  2.5078e-03,\n",
       "           1.1057e-03, -1.1609e-03,  5.6695e-03,  4.8681e-03, -1.9273e-03,\n",
       "           1.2664e-02, -8.6842e-03, -5.4552e-03,  1.5235e-04,  1.3400e-03,\n",
       "          -1.3119e-03, -3.1105e-03, -4.8572e-04, -7.6307e-03, -4.7043e-03,\n",
       "          -6.4985e-03, -2.9651e-03,  9.5851e-04,  2.6359e-03,  1.1113e-03,\n",
       "          -6.9044e-04,  5.4406e-03,  5.2135e-03, -1.6224e-03,  1.2366e-02,\n",
       "          -8.8890e-03, -7.0411e-03,  3.6848e-04,  1.1529e-03, -3.5112e-04,\n",
       "          -3.3249e-03, -6.6500e-03, -3.6045e-03],\n",
       "         [-2.1633e-03, -4.1706e-03,  1.5089e-03, -1.4669e-03,  1.0295e-03,\n",
       "           5.3739e-03,  4.9599e-04,  6.3481e-03,  8.1264e-03, -3.3034e-03,\n",
       "           2.0550e-02, -1.3418e-02, -3.2779e-04,  1.7961e-03,  4.5040e-03,\n",
       "           6.2703e-04, -2.0337e-02, -6.7060e-03,  2.8648e-03, -2.3748e-03,\n",
       "          -3.5098e-03,  8.0572e-04, -1.0325e-03,  8.4697e-05,  5.3680e-03,\n",
       "           8.4901e-04,  6.5513e-03,  8.3078e-03, -3.1479e-03,  2.0896e-02,\n",
       "          -1.4059e-02, -4.5349e-04,  3.4780e-03,  4.9383e-03,  1.0139e-03,\n",
       "          -2.0770e-02, -8.7522e-03, -6.0883e-04],\n",
       "         [-4.9854e-03, -8.0867e-04, -6.3690e-03,  2.3122e-03,  2.1752e-03,\n",
       "          -4.9832e-03, -1.5231e-04,  2.2757e-03, -1.4308e-03,  3.1125e-03,\n",
       "          -7.7527e-03,  3.9475e-03, -6.2175e-03, -2.9900e-03,  3.9273e-05,\n",
       "          -2.7868e-04,  1.7824e-02, -1.8713e-03,  2.5377e-03, -4.9124e-03,\n",
       "          -1.3324e-03, -6.1523e-03,  1.3704e-03,  2.3420e-03, -5.1025e-03,\n",
       "          -5.0792e-04,  2.1776e-03, -1.8899e-03,  2.3650e-03, -8.6494e-03,\n",
       "           4.5633e-03, -7.1943e-03, -4.2906e-03, -1.7625e-03, -1.0729e-03,\n",
       "           1.7844e-02,  2.1184e-03,  4.5146e-03],\n",
       "         [-5.1168e-03, -1.8206e-03, -1.0266e-02,  9.3685e-04,  2.1550e-03,\n",
       "          -4.4561e-03,  1.3866e-04,  6.0944e-03, -6.1764e-04,  3.3926e-03,\n",
       "          -5.8793e-03,  2.6256e-03, -8.9824e-03, -5.3195e-03, -4.1600e-04,\n",
       "           7.0668e-04,  1.8713e-02,  8.2841e-03,  2.0947e-03, -5.1610e-03,\n",
       "          -2.4859e-03, -9.3720e-03, -3.4918e-04,  3.1175e-03, -4.5366e-03,\n",
       "          -9.6083e-04,  7.0657e-03, -9.5260e-04,  2.0272e-03, -6.0547e-03,\n",
       "           2.3719e-03, -1.0267e-02, -6.7024e-03, -3.1953e-03, -1.0864e-03,\n",
       "           1.8770e-02,  8.8426e-03,  7.0884e-03],\n",
       "         [ 1.7452e-03, -1.7916e-03,  6.3006e-03,  9.6864e-04, -8.3380e-04,\n",
       "           1.0500e-03, -8.9648e-04, -4.1143e-03,  4.9644e-04, -3.0978e-03,\n",
       "           4.7846e-03, -1.6782e-03,  3.0777e-03,  4.2070e-03, -1.2166e-04,\n",
       "          -8.3234e-04, -9.2005e-03, -2.0270e-03, -1.0576e-02,  2.0104e-03,\n",
       "          -2.2571e-03,  5.1353e-03,  1.6924e-03, -1.0519e-03,  8.2529e-04,\n",
       "          -3.5617e-04, -4.4602e-03,  9.8279e-04, -2.0454e-03,  4.7002e-03,\n",
       "          -1.9059e-03,  2.8638e-03,  3.9210e-03,  1.0958e-03,  2.1210e-04,\n",
       "          -9.5673e-03, -3.0843e-03, -7.2264e-03],\n",
       "         [ 1.2892e-03,  8.4618e-04, -2.0581e-03, -7.6990e-04, -9.9112e-04,\n",
       "          -7.0041e-04, -6.0216e-04,  1.1841e-03, -1.4025e-03,  2.7016e-04,\n",
       "          -3.2604e-03,  1.1270e-03, -1.7255e-03, -1.7281e-03, -3.1517e-03,\n",
       "          -1.0703e-03,  4.5466e-03,  4.8423e-03,  2.9591e-03,  1.1556e-03,\n",
       "           5.6747e-04, -1.2512e-03, -2.8681e-05, -4.5088e-04, -4.7801e-04,\n",
       "          -3.4651e-04,  2.9518e-04, -1.2690e-03,  4.9130e-04, -2.7609e-03,\n",
       "           1.4961e-03, -4.4683e-04, -1.6307e-03, -1.8324e-03, -4.1311e-04,\n",
       "           4.7161e-03,  4.3636e-03,  4.6721e-04],\n",
       "         [-1.3150e-02, -7.9854e-03, -1.1374e-02,  3.4712e-04,  5.2057e-03,\n",
       "           1.5470e-03, -8.7879e-04,  1.1319e-02,  7.0388e-03, -2.5379e-04,\n",
       "           1.6202e-02, -9.6787e-03, -1.5657e-02, -2.0087e-05,  2.7621e-03,\n",
       "           1.6396e-03,  7.4973e-03,  5.8049e-04, -2.0468e-02, -1.3106e-02,\n",
       "          -8.8144e-03, -1.0532e-02, -6.5086e-04,  5.7009e-03,  1.6000e-03,\n",
       "          -1.8100e-03,  1.1875e-02,  7.1334e-03, -1.6083e-03,  1.6294e-02,\n",
       "          -1.0329e-02, -1.6613e-02, -1.2051e-03,  9.2332e-05,  4.2705e-04,\n",
       "           7.4632e-03,  3.2165e-03, -1.4531e-02],\n",
       "         [ 1.6406e-02,  1.9264e-02,  1.2322e-02,  6.0434e-04, -6.1359e-03,\n",
       "          -7.1272e-03,  5.4160e-04, -2.1508e-02, -2.0506e-02,  5.8648e-03,\n",
       "          -4.5263e-02,  3.1002e-02,  2.1522e-02,  5.7330e-04, -6.5190e-03,\n",
       "          -1.9701e-03,  1.9299e-02, -9.6184e-04,  1.3160e-02,  1.6045e-02,\n",
       "           1.9916e-02,  1.5207e-02,  6.5320e-04, -4.3525e-03, -7.1021e-03,\n",
       "           6.4377e-04, -2.1487e-02, -2.0775e-02,  6.1826e-03, -4.4510e-02,\n",
       "           3.2271e-02,  2.4598e-02,  6.2540e-04, -3.2216e-03, -2.7020e-03,\n",
       "           2.0300e-02, -2.9369e-03,  5.1564e-03],\n",
       "         [-5.5571e-03,  1.9657e-03, -7.3590e-04, -1.2380e-03,  1.7050e-03,\n",
       "           2.1667e-03, -8.0640e-04, -6.8087e-05, -8.9156e-04, -9.4335e-04,\n",
       "           2.1316e-03,  1.7810e-03, -1.4416e-03,  4.3079e-03,  2.6655e-04,\n",
       "           1.1550e-03,  4.2838e-03,  1.2510e-04, -2.0427e-02, -5.2512e-03,\n",
       "           1.3610e-03,  7.3505e-05, -8.7215e-04,  1.8784e-03,  2.4018e-03,\n",
       "          -4.8450e-04, -1.2741e-03, -8.5392e-04, -8.4364e-04,  2.2836e-03,\n",
       "           2.2441e-03, -4.8753e-04,  4.0983e-03,  1.0475e-03,  1.8850e-03,\n",
       "           4.7269e-03,  1.3651e-03, -2.0784e-02],\n",
       "         [-1.8744e-03, -7.7950e-03,  1.0091e-03,  9.7709e-04,  1.3496e-03,\n",
       "           3.5094e-03,  2.2928e-03,  4.8946e-03,  9.9818e-03, -2.2923e-03,\n",
       "           1.8865e-02, -1.3680e-02, -1.1749e-03, -9.2271e-04,  6.2987e-03,\n",
       "           1.5247e-03, -2.1637e-02, -7.9971e-03,  1.1935e-02, -1.8500e-03,\n",
       "          -6.6525e-03, -1.5430e-03,  6.7225e-05, -4.0156e-04,  3.0840e-03,\n",
       "           1.8078e-03,  5.8092e-03,  9.3436e-03, -2.3448e-03,  1.8038e-02,\n",
       "          -1.4113e-02, -3.6337e-03,  3.5710e-04,  4.1760e-03,  1.3110e-03,\n",
       "          -2.2688e-02, -4.5602e-03,  8.0669e-03],\n",
       "         [ 1.3001e-02,  5.2688e-03,  1.9178e-02,  8.7307e-04, -3.3921e-03,\n",
       "          -1.5823e-03, -2.5605e-03, -1.4952e-02, -8.3253e-03, -3.6037e-03,\n",
       "          -1.0920e-02,  9.6378e-03,  1.7421e-02,  5.4294e-03, -3.9709e-03,\n",
       "          -4.1638e-03, -7.3404e-03, -2.2795e-03, -6.9856e-03,  1.2879e-02,\n",
       "           5.8053e-03,  1.8097e-02,  2.2442e-03, -3.4501e-03, -1.5906e-03,\n",
       "          -1.2760e-03, -1.5728e-02, -7.7830e-03, -1.4265e-03, -1.0616e-02,\n",
       "           9.9885e-03,  1.8650e-02,  6.4537e-03,  3.3546e-04, -2.1785e-03,\n",
       "          -7.2017e-03, -6.3476e-03, -9.0791e-03],\n",
       "         [-5.4681e-04,  1.9559e-04,  2.4319e-03,  6.2855e-04, -1.1673e-03,\n",
       "           1.3606e-03,  8.0322e-04, -3.6582e-03,  5.9769e-04, -1.2759e-03,\n",
       "           9.0141e-04,  9.5389e-04,  3.0584e-03,  2.9333e-03,  1.3129e-03,\n",
       "           2.3073e-03, -4.8919e-03,  2.3835e-03, -3.9068e-03,  4.2640e-04,\n",
       "           2.1400e-04,  2.4844e-03,  1.9411e-04, -1.5534e-03,  1.1363e-03,\n",
       "           5.4406e-04, -3.8571e-03,  3.1051e-04, -1.1806e-03,  6.2845e-04,\n",
       "           1.0138e-03,  2.8639e-03,  2.6550e-03,  1.0780e-03,  1.8393e-03,\n",
       "          -5.1235e-03,  4.2522e-03, -4.8698e-03],\n",
       "         [-4.6589e-03, -3.7163e-03, -5.2330e-03, -1.0317e-03,  1.3607e-03,\n",
       "           3.0917e-03,  2.3374e-04,  7.6321e-03,  5.7880e-03, -7.2915e-04,\n",
       "           1.2690e-02, -9.3478e-03, -6.5367e-03, -6.4850e-04,  1.5036e-03,\n",
       "           7.3904e-04, -5.1020e-03, -1.2172e-03, -6.2923e-04, -5.1981e-03,\n",
       "          -3.7411e-03, -5.0288e-03, -6.3582e-04,  1.4103e-03,  3.2875e-03,\n",
       "           3.7701e-04,  7.5555e-03,  6.0237e-03, -1.0787e-03,  1.2942e-02,\n",
       "          -9.6353e-03, -6.4091e-03, -1.3831e-04,  1.3388e-03,  8.1714e-04,\n",
       "          -5.2389e-03, -3.2752e-03, -3.2149e-04],\n",
       "         [ 2.8908e-03,  3.8720e-03, -5.8520e-03, -4.9700e-04, -2.7788e-03,\n",
       "          -1.0468e-03,  1.2111e-03, -1.9140e-03, -3.9053e-03,  4.2937e-03,\n",
       "          -1.2830e-02,  6.9165e-03, -8.9975e-04, -3.3884e-03, -3.0132e-03,\n",
       "           1.8200e-03,  1.1367e-02,  6.8363e-03,  8.1927e-03,  3.3744e-03,\n",
       "           4.0181e-03, -3.3391e-03, -1.4221e-03, -2.4747e-03, -5.8357e-04,\n",
       "           4.2254e-05, -1.0062e-03, -3.9450e-03,  2.6823e-03, -1.2187e-02,\n",
       "           6.3755e-03,  3.6681e-04, -4.1629e-03, -5.0000e-03, -1.5040e-04,\n",
       "           1.1525e-02,  1.0924e-02,  8.0042e-03]], device='cuda:0'),\n",
       " 'edge_network.0.bias': tensor([ 0.0019, -0.0015, -0.0018, -0.0029,  0.0004, -0.0002,  0.0001,  0.0005,\n",
       "         -0.0035,  0.0076, -0.0005, -0.0028,  0.0036,  0.0002, -0.0025,  0.0014],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.1.weight': tensor([ 0.0045, -0.0001, -0.0015,  0.0033,  0.0048, -0.0011,  0.0005,  0.0002,\n",
       "         -0.0023, -0.0011,  0.0004, -0.0013, -0.0009,  0.0003, -0.0025,  0.0044],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.1.bias': tensor([ 0.0048, -0.0028, -0.0036, -0.0045,  0.0008, -0.0017,  0.0005,  0.0005,\n",
       "         -0.0079,  0.0151, -0.0022, -0.0040,  0.0081,  0.0011, -0.0048,  0.0027],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.3.weight': tensor([[-5.0765e-03, -1.2208e-03, -1.8040e-03, -8.1050e-03,  3.0643e-03,\n",
       "           3.5662e-03,  3.3014e-03,  1.5058e-03, -8.2702e-04, -2.6053e-03,\n",
       "           2.8161e-03, -1.0286e-04,  1.8317e-04, -1.8550e-03,  7.6871e-04,\n",
       "           2.6074e-03],\n",
       "         [-3.7010e-03,  3.2638e-04, -1.1155e-03, -5.7439e-03,  1.8119e-03,\n",
       "           3.2923e-03,  5.2561e-04,  1.0536e-03, -1.1093e-03, -7.8354e-04,\n",
       "           2.7339e-03, -4.7540e-04,  6.9909e-04, -2.2730e-03, -9.3080e-04,\n",
       "           1.7846e-03],\n",
       "         [ 3.7436e-03, -2.6944e-03, -1.1849e-03,  6.5303e-03, -4.2139e-03,\n",
       "           9.1761e-04, -1.4038e-04, -2.5661e-04,  3.0126e-03, -8.2801e-04,\n",
       "          -3.0724e-03,  1.9846e-03,  3.8855e-03, -2.0575e-03, -3.7994e-03,\n",
       "          -2.1021e-03],\n",
       "         [-1.6635e-02,  2.4330e-03,  1.4884e-02,  7.0627e-03, -1.9798e-03,\n",
       "          -1.0309e-02,  3.6854e-03, -4.8470e-03,  4.2091e-03, -5.3485e-03,\n",
       "           1.2455e-03, -1.2708e-03, -8.3951e-03,  6.0887e-03,  8.1738e-03,\n",
       "           8.1745e-03],\n",
       "         [-9.5005e-03,  1.3340e-02,  5.0611e-03, -1.0586e-02,  1.5128e-02,\n",
       "          -1.0589e-02, -1.5826e-03, -8.9253e-03,  1.6303e-03,  3.2133e-03,\n",
       "           8.3372e-04, -9.4803e-03, -1.4563e-02,  8.8561e-03,  1.1149e-02,\n",
       "           1.0690e-02],\n",
       "         [ 9.0430e-03, -2.4072e-03, -1.3195e-02, -1.2928e-02,  4.9018e-03,\n",
       "           1.3469e-02, -5.2869e-04,  6.5069e-03, -7.8337e-03,  3.0910e-03,\n",
       "           3.3530e-03,  9.4041e-04,  7.1900e-03, -6.8588e-03, -7.1682e-03,\n",
       "          -6.6631e-03],\n",
       "         [-2.7503e-03,  1.7580e-03, -2.0278e-03, -9.2758e-03,  6.1019e-03,\n",
       "           4.4035e-04,  1.5399e-03, -8.2605e-04,  4.9771e-04, -6.5680e-04,\n",
       "           1.5611e-03, -1.3658e-03, -2.6207e-03, -3.8818e-05,  2.4908e-03,\n",
       "           4.2943e-03],\n",
       "         [-3.2959e-03,  7.8468e-03,  5.4162e-03,  4.0354e-03,  5.5122e-03,\n",
       "          -1.2424e-02, -2.9847e-03, -1.0346e-02,  6.0526e-03,  3.4322e-03,\n",
       "          -3.8494e-03, -5.4936e-03, -1.0165e-02,  7.2732e-03,  6.6768e-03,\n",
       "           7.5819e-03],\n",
       "         [-4.6430e-04, -5.9935e-03,  5.2723e-03,  1.3035e-02, -8.2647e-03,\n",
       "          -1.3467e-03,  3.5723e-03,  3.2489e-03,  1.4740e-03, -6.1610e-03,\n",
       "          -3.4259e-03,  7.0280e-03,  4.2284e-03,  1.2079e-03, -2.9572e-03,\n",
       "          -4.8477e-03],\n",
       "         [-1.5791e-03,  2.2147e-03,  3.7286e-03,  1.5140e-03,  1.9076e-03,\n",
       "          -3.3459e-03,  2.8038e-04, -2.1980e-03,  3.4177e-05, -4.0573e-04,\n",
       "           2.1042e-03, -2.7092e-03, -3.4441e-03,  1.5561e-03,  2.8328e-03,\n",
       "           2.8870e-03],\n",
       "         [-1.3786e-03, -7.5092e-03,  2.1926e-03,  6.4392e-03, -6.5945e-03,\n",
       "           3.7625e-03,  4.9316e-03,  5.7985e-03, -3.6946e-04, -6.1414e-03,\n",
       "          -5.7356e-05,  6.3414e-03,  5.7403e-03, -1.6990e-03, -5.2710e-03,\n",
       "          -5.5847e-03],\n",
       "         [ 3.4134e-03,  8.0808e-03,  1.8103e-03,  2.9295e-03,  5.7382e-03,\n",
       "          -1.4165e-02, -5.3231e-03, -6.6842e-03,  7.6352e-03,  5.0027e-03,\n",
       "          -7.7295e-03, -3.6297e-03, -1.0113e-02,  8.9011e-03,  7.3701e-03,\n",
       "           2.7132e-03],\n",
       "         [ 1.1297e-02, -1.0162e-03, -3.6784e-03,  5.7247e-03, -5.0372e-03,\n",
       "           1.6647e-03, -3.8814e-03, -6.5876e-04, -5.2846e-04,  3.7455e-03,\n",
       "          -3.7865e-04, -1.3577e-03,  3.2381e-03, -2.9791e-03, -2.9595e-03,\n",
       "          -3.2265e-03],\n",
       "         [-2.5694e-02,  1.0944e-02,  1.0573e-02, -1.5244e-02,  1.4739e-02,\n",
       "          -1.0109e-02,  3.3729e-03, -8.5050e-03,  6.7458e-03, -3.5699e-03,\n",
       "           2.9209e-03, -5.5857e-03, -1.7219e-02,  6.9415e-03,  1.2197e-02,\n",
       "           1.6600e-02],\n",
       "         [ 4.2127e-02, -2.2818e-02, -2.6291e-02,  1.1318e-02, -3.0034e-02,\n",
       "           3.4187e-02, -8.3099e-03,  2.1589e-02, -2.0491e-02,  1.0433e-02,\n",
       "           2.8886e-03,  1.0922e-02,  3.8788e-02, -2.3248e-02, -2.6870e-02,\n",
       "          -3.1994e-02],\n",
       "         [ 4.5052e-04, -3.2836e-03,  3.5886e-04,  3.2944e-03, -2.7816e-03,\n",
       "           9.8914e-04,  1.5414e-03,  3.5438e-03, -1.3241e-04, -2.4176e-03,\n",
       "          -1.9439e-03,  4.2547e-03,  2.5676e-03,  1.8490e-04, -1.7028e-03,\n",
       "          -2.9147e-03]], device='cuda:0'),\n",
       " 'edge_network.3.bias': tensor([-0.0056, -0.0060,  0.0032,  0.0153,  0.0068, -0.0232, -0.0027,  0.0163,\n",
       "          0.0057,  0.0036, -0.0027,  0.0175,  0.0014,  0.0086, -0.0372, -0.0010],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.4.weight': tensor([ 8.2120e-04, -3.0040e-05,  1.1651e-03, -3.8065e-03, -8.1002e-04,\n",
       "         -2.6934e-03,  1.9857e-03, -2.4507e-03, -2.8926e-03, -2.7099e-04,\n",
       "         -9.7525e-04,  1.7267e-03,  6.1893e-04,  4.9329e-03, -2.0993e-03,\n",
       "          7.1367e-05], device='cuda:0'),\n",
       " 'edge_network.4.bias': tensor([-0.0019, -0.0023,  0.0005,  0.0059,  0.0027, -0.0080, -0.0011,  0.0057,\n",
       "          0.0023,  0.0017, -0.0012,  0.0057, -0.0002,  0.0026, -0.0146, -0.0006],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.6.weight': tensor([[-4.5433e-03, -2.6884e-03,  7.1066e-03,  5.8977e-03,  3.9526e-03,\n",
       "          -1.0119e-02, -2.4606e-03,  3.1832e-03, -8.6884e-03,  7.0238e-03,\n",
       "          -5.8872e-03,  8.8217e-03,  1.9374e-03, -6.6740e-03,  1.7866e-03,\n",
       "           5.2213e-03],\n",
       "         [-3.2390e-03, -1.2374e-03,  4.1853e-03,  2.8662e-03,  3.8929e-05,\n",
       "          -5.0569e-03, -1.0374e-03,  7.4943e-04, -5.0745e-03,  1.3967e-03,\n",
       "          -2.9983e-03,  2.8414e-03,  1.6330e-03, -8.8865e-04,  3.7607e-03,\n",
       "           3.0621e-03],\n",
       "         [ 7.9460e-03,  7.8731e-03, -8.6397e-03, -2.6224e-03, -1.0796e-02,\n",
       "           1.5700e-02,  8.1773e-03, -7.8243e-05,  1.1811e-02, -1.1575e-02,\n",
       "           2.1710e-03, -1.7551e-02, -9.8531e-03,  1.4594e-02, -4.9682e-03,\n",
       "          -2.3577e-03],\n",
       "         [ 3.0294e-03, -8.5539e-04, -1.0356e-02, -9.3885e-03,  1.1198e-02,\n",
       "           7.4556e-03, -2.6353e-03, -4.1968e-03,  8.3599e-03,  2.3133e-03,\n",
       "           1.2362e-02,  8.4528e-04,  1.1025e-03, -8.9379e-03, -7.6861e-03,\n",
       "          -8.6775e-03],\n",
       "         [ 5.7155e-03,  3.7693e-03, -2.7712e-03, -1.7578e-03, -1.0611e-02,\n",
       "           8.9796e-03,  4.7176e-03, -1.5452e-04,  7.1882e-03, -7.8671e-03,\n",
       "          -7.8298e-04, -9.7417e-03, -3.9289e-03,  1.1208e-02, -7.0152e-04,\n",
       "          -1.7966e-03],\n",
       "         [ 4.8372e-03,  2.4400e-03, -2.0357e-03,  1.6953e-03, -7.0973e-03,\n",
       "           5.8480e-03,  5.3497e-03,  1.3892e-03,  5.8141e-03, -4.6158e-03,\n",
       "          -2.9409e-03, -7.1547e-03, -5.8858e-03,  7.7139e-03, -3.0401e-03,\n",
       "           1.2851e-04],\n",
       "         [ 1.0949e-02,  4.9255e-03, -1.1308e-02, -2.0196e-03,  1.3138e-04,\n",
       "           1.6345e-02,  9.0468e-03,  1.1916e-03,  1.5826e-02, -4.4486e-03,\n",
       "           3.2017e-03, -1.2796e-02, -9.9701e-03,  5.6706e-03, -1.5106e-02,\n",
       "          -6.3415e-03],\n",
       "         [-3.9295e-03, -1.3024e-02,  1.1294e-02,  3.8105e-03,  7.9964e-03,\n",
       "          -1.3590e-02, -8.0676e-04,  1.9322e-03, -1.4051e-02,  1.0423e-02,\n",
       "          -5.8991e-03,  1.6157e-02,  8.0992e-03, -1.0967e-02,  6.6062e-03,\n",
       "           6.0286e-03],\n",
       "         [-1.3512e-02, -3.4312e-03,  1.1505e-02,  6.2904e-03,  5.7593e-03,\n",
       "          -1.9567e-02, -9.0788e-03,  1.5988e-03, -1.5844e-02,  9.8452e-03,\n",
       "          -5.6736e-03,  1.5679e-02,  8.3995e-03, -1.0476e-02,  8.8851e-03,\n",
       "           6.6491e-03],\n",
       "         [ 8.3012e-03, -1.3370e-03, -1.8543e-03,  3.2680e-03, -3.5896e-03,\n",
       "           5.8374e-03,  9.7149e-03,  3.4863e-03,  8.1104e-03, -4.5490e-04,\n",
       "          -6.3480e-03, -4.6522e-03, -9.4480e-03,  4.7222e-03, -6.8898e-03,\n",
       "           2.9559e-04],\n",
       "         [ 6.3999e-03,  6.5133e-03, -7.5281e-03, -2.6047e-03, -6.5808e-03,\n",
       "           1.2773e-02,  5.1385e-03,  9.1536e-05,  1.0134e-02, -7.8670e-03,\n",
       "           2.2999e-03, -1.3258e-02, -7.9049e-03,  9.8203e-03, -4.5545e-03,\n",
       "          -2.5925e-03],\n",
       "         [-2.7836e-03, -1.3691e-03,  8.5170e-03,  8.6061e-03,  4.9483e-04,\n",
       "          -1.1224e-02, -1.2185e-03,  4.9763e-03, -7.3223e-03,  6.0173e-03,\n",
       "          -1.0107e-02,  8.0926e-03, -2.9067e-04, -3.6216e-03,  7.3861e-04,\n",
       "           5.9041e-03],\n",
       "         [ 8.0989e-04, -6.9276e-03, -3.0038e-03, -6.8920e-03,  9.9012e-03,\n",
       "          -4.1712e-04, -8.5890e-04, -5.1677e-03,  2.1697e-03,  3.2572e-03,\n",
       "           6.9944e-03,  4.1825e-03,  7.9369e-03, -5.2490e-03, -1.7677e-04,\n",
       "          -8.3064e-03],\n",
       "         [-9.2276e-03,  3.3408e-03, -1.5278e-03, -6.9102e-03,  9.3305e-04,\n",
       "          -2.1921e-03, -1.0355e-02, -6.7004e-03, -5.9484e-03, -5.2392e-03,\n",
       "           1.0892e-02, -1.1835e-03,  1.0102e-02,  1.2522e-04,  1.1106e-02,\n",
       "          -2.4215e-03],\n",
       "         [-6.5430e-03,  4.6749e-03,  4.1572e-04, -2.5530e-03, -1.3283e-03,\n",
       "          -2.8110e-03, -9.7950e-03, -2.0845e-03, -4.8722e-03, -5.7958e-04,\n",
       "           5.2570e-03,  3.0507e-03,  2.5669e-03, -4.4592e-03,  4.9055e-03,\n",
       "           2.0633e-03],\n",
       "         [-4.2102e-03, -2.6668e-03,  6.0003e-03,  2.3141e-03, -4.0203e-04,\n",
       "          -7.9611e-03, -3.8982e-03, -2.1648e-04, -7.6131e-03,  2.3704e-03,\n",
       "          -2.5415e-03,  6.6672e-03,  5.5040e-03, -2.5808e-03,  5.3343e-03,\n",
       "           3.1411e-03]], device='cuda:0'),\n",
       " 'edge_network.6.bias': tensor([-0.0075, -0.0036,  0.0042,  0.0133,  0.0005, -0.0026,  0.0034, -0.0063,\n",
       "         -0.0079, -0.0061,  0.0037, -0.0125,  0.0075,  0.0118,  0.0052, -0.0031],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.7.weight': tensor([ 8.1443e-05, -4.8358e-04,  2.7098e-03,  4.8567e-03,  1.6428e-03,\n",
       "         -2.8053e-04,  4.4395e-03, -6.1429e-04,  3.1465e-04,  7.3715e-04,\n",
       "          4.5184e-04,  2.0222e-04,  3.9616e-03,  3.2288e-04,  3.8758e-03,\n",
       "          9.2059e-04], device='cuda:0'),\n",
       " 'edge_network.7.bias': tensor([-6.5642e-04, -4.6778e-04,  1.5609e-03,  4.3609e-03,  1.3484e-03,\n",
       "         -1.0708e-05,  1.7880e-03, -1.5346e-04, -1.2183e-03,  1.7734e-04,\n",
       "          1.4563e-03, -3.2885e-04,  3.1995e-03,  4.0786e-03,  2.8582e-03,\n",
       "          1.0678e-03], device='cuda:0'),\n",
       " 'edge_network.9.weight': tensor([[-0.0048,  0.0218,  0.0292, -0.0422,  0.0332, -0.0024,  0.0554,  0.0173,\n",
       "           0.0013,  0.0027,  0.0115, -0.0032, -0.0561, -0.0136, -0.0512, -0.0029]],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.9.bias': tensor([-0.0019], device='cuda:0'),\n",
       " 'node_network.0.weight': tensor([[ 3.6475e-03,  1.5523e-03,  7.3013e-03,  3.9050e-03,  3.5898e-03,\n",
       "          -2.5764e-03, -7.5666e-04, -7.0515e-03, -5.3828e-03, -2.4456e-03,\n",
       "          -5.5435e-03,  3.8273e-03,  4.4545e-03,  5.3798e-03, -6.8196e-04,\n",
       "          -3.5125e-03,  1.3510e-03, -6.6318e-03, -7.7556e-03,  9.4945e-03,\n",
       "           8.1018e-03,  1.2600e-02,  5.6615e-03,  4.0985e-03, -2.1961e-03,\n",
       "          -1.0105e-03, -1.5192e-02, -1.1702e-02, -2.4541e-03, -1.7681e-02,\n",
       "           1.0019e-02,  1.3270e-02,  8.1337e-03, -4.5094e-03, -3.5887e-03,\n",
       "           3.6703e-03, -1.0088e-02, -7.8292e-03,  9.8619e-03,  7.4940e-03,\n",
       "           1.2303e-02,  5.7991e-03,  3.0629e-03, -3.1252e-03, -8.0549e-04,\n",
       "          -1.6063e-02, -1.1063e-02, -3.4535e-05, -1.6161e-02,  1.1016e-02,\n",
       "           1.3878e-02,  7.9697e-03, -2.6466e-04, -3.2182e-03,  3.0028e-03,\n",
       "          -1.1597e-02, -7.5886e-03],\n",
       "         [-1.7322e-04, -4.5071e-04,  1.3165e-04,  4.8802e-04,  3.9492e-04,\n",
       "          -7.7600e-04, -1.9113e-04, -1.2148e-04, -1.1783e-05, -8.1522e-05,\n",
       "           1.5148e-03, -2.7350e-04, -2.9385e-04,  4.6535e-04,  5.7504e-04,\n",
       "          -3.4393e-04, -4.3443e-04, -1.0137e-03, -2.0672e-03, -2.3643e-03,\n",
       "          -5.8882e-03, -2.0946e-03,  1.3696e-03,  1.8877e-03, -3.6217e-03,\n",
       "          -1.3784e-03,  5.5842e-03,  3.8221e-03, -6.0046e-04,  1.1779e-02,\n",
       "          -4.8357e-03, -5.2201e-03, -2.5133e-03,  3.1563e-03, -2.2346e-03,\n",
       "          -1.5925e-03, -2.5581e-03, -3.5866e-03, -9.5863e-04, -4.3087e-03,\n",
       "          -1.1502e-03,  8.2035e-04,  6.9885e-04, -2.9852e-03, -1.4384e-03,\n",
       "           2.6269e-03,  1.5503e-03, -8.6270e-04,  8.1956e-03, -2.3920e-03,\n",
       "          -3.7839e-03, -8.7744e-04,  1.6277e-03, -1.6699e-03, -1.0147e-03,\n",
       "           1.2861e-03, -5.7760e-03],\n",
       "         [-1.2332e-03,  2.9200e-04,  9.3593e-04,  1.3517e-03,  9.2799e-04,\n",
       "           3.1245e-04,  8.5201e-04, -3.9170e-04,  4.3820e-04, -1.0587e-03,\n",
       "          -1.8458e-03, -3.3454e-04,  5.0943e-04,  6.8351e-04, -4.6893e-04,\n",
       "           1.7329e-04,  8.8723e-04, -1.4909e-03,  4.4526e-04,  2.0449e-03,\n",
       "           4.8859e-03,  5.1645e-03,  3.7980e-03,  1.5037e-03,  3.2425e-04,\n",
       "           1.8774e-03, -6.1669e-03, -3.9170e-03, -1.2278e-03, -1.5533e-02,\n",
       "           4.5203e-03,  6.5549e-03,  2.4351e-03, -3.6350e-03,  8.6856e-05,\n",
       "           7.4713e-03, -2.4917e-03,  2.5689e-03,  7.4071e-05,  3.1960e-03,\n",
       "           3.2631e-03,  3.6799e-03,  2.4522e-03, -6.6549e-04,  1.8400e-03,\n",
       "          -4.0341e-03, -2.2701e-03, -5.4900e-04, -1.0375e-02,  2.9511e-03,\n",
       "           3.8273e-03,  1.8002e-03, -9.7356e-04, -3.0985e-04,  5.2482e-03,\n",
       "          -6.4227e-03,  1.6466e-03],\n",
       "         [-9.5542e-04, -1.3053e-03, -6.5204e-04, -8.0452e-04, -3.4140e-04,\n",
       "          -3.0601e-04, -2.7583e-04,  5.8369e-04,  5.6618e-04, -1.5237e-04,\n",
       "           2.8518e-03, -4.4203e-04, -1.6710e-03, -2.0001e-04,  6.5207e-04,\n",
       "          -6.7533e-05, -1.1324e-03,  1.8603e-03, -2.7010e-03, -1.1047e-02,\n",
       "          -1.2154e-02, -1.0166e-02, -4.8705e-03,  1.1201e-03, -2.0026e-03,\n",
       "          -1.5468e-03,  1.3139e-02,  1.0749e-02,  6.1950e-04,  3.2460e-02,\n",
       "          -1.1524e-02, -1.6646e-02, -4.3869e-03,  8.4897e-03, -5.9807e-04,\n",
       "          -1.5075e-02,  1.0956e-04, -9.2205e-03, -1.0852e-02, -9.7965e-03,\n",
       "          -1.0393e-02, -3.6232e-03,  1.6951e-03, -1.7048e-03, -3.3609e-04,\n",
       "           1.2045e-02,  9.8299e-03,  2.9712e-05,  2.6611e-02, -1.0941e-02,\n",
       "          -1.6223e-02, -3.8404e-03,  5.8162e-03, -3.1983e-04, -1.4265e-02,\n",
       "          -4.0789e-03, -4.4578e-03],\n",
       "         [-2.1908e-03, -1.4142e-03, -3.7632e-03, -2.1414e-03, -1.7311e-03,\n",
       "           7.1464e-04, -1.3240e-04,  3.3372e-03,  2.3656e-03,  9.9402e-04,\n",
       "           4.4580e-03, -1.8079e-03, -2.8388e-03, -1.9425e-03,  6.5865e-04,\n",
       "           1.6515e-03, -6.8063e-04,  4.2988e-03, -9.3180e-06, -1.4891e-03,\n",
       "          -2.5709e-04, -4.9381e-04, -1.7855e-03, -1.6773e-03, -5.7535e-04,\n",
       "          -4.8271e-04,  7.3866e-04, -3.7705e-04,  2.1245e-04, -5.8583e-04,\n",
       "           1.8619e-03, -1.1358e-03,  1.9162e-04,  8.5093e-04, -4.9740e-05,\n",
       "           4.3936e-03,  5.2487e-03, -4.7821e-03, -3.8309e-04, -3.1387e-04,\n",
       "          -3.2325e-04, -3.6452e-03, -2.4132e-03,  5.2000e-04, -1.6154e-03,\n",
       "           5.4243e-04, -1.2846e-03, -3.3384e-04, -1.4227e-04,  2.2787e-03,\n",
       "          -8.7684e-04,  2.0392e-05, -9.6746e-04,  3.4106e-04,  4.9469e-03,\n",
       "           1.2043e-02, -8.3538e-03],\n",
       "         [-4.3410e-03, -1.1364e-03, -7.9533e-03, -3.0456e-03, -2.4221e-03,\n",
       "           2.5981e-03,  8.3676e-04,  7.2978e-03,  5.8796e-03,  3.2384e-03,\n",
       "           5.5098e-03, -4.5541e-03, -4.5638e-03, -5.0005e-03,  1.5683e-03,\n",
       "           3.5288e-03, -1.3657e-03, -1.7959e-04,  9.9362e-03, -5.0742e-03,\n",
       "          -2.6796e-03, -7.4430e-03, -2.3657e-03, -2.1440e-03,  2.7939e-03,\n",
       "           1.0283e-03,  8.8022e-03,  7.4561e-03,  2.4274e-03,  5.3602e-03,\n",
       "          -6.7212e-03, -5.3077e-03, -5.4396e-03,  1.3035e-03,  3.0858e-03,\n",
       "           1.3665e-03, -4.1822e-04,  1.2285e-02, -5.5435e-03, -3.5458e-03,\n",
       "          -8.8618e-03, -3.5081e-03, -2.2616e-03,  2.8892e-03,  4.7819e-04,\n",
       "           1.0711e-02,  7.5938e-03,  1.7018e-03,  7.3444e-03, -7.9359e-03,\n",
       "          -7.4946e-03, -6.2185e-03, -5.7573e-04,  2.6591e-03,  8.7809e-04,\n",
       "           2.7355e-03,  1.2015e-02],\n",
       "         [ 1.0254e-03,  2.5894e-03,  3.9532e-03,  3.8006e-03,  1.4451e-03,\n",
       "           9.3901e-04,  2.2540e-03, -3.4970e-03, -8.5369e-04, -1.9970e-03,\n",
       "          -7.4965e-03,  2.2886e-04,  4.4811e-03,  1.8301e-03, -2.3876e-03,\n",
       "           1.7959e-04,  4.1873e-04, -4.2899e-03,  5.2476e-03, -2.2124e-03,\n",
       "           5.5767e-03,  1.1671e-03,  2.4392e-03,  1.8822e-03,  6.0013e-03,\n",
       "           4.2876e-03, -2.7013e-03,  2.9316e-03, -2.1884e-03, -2.2586e-03,\n",
       "          -4.4297e-03,  5.1475e-03,  1.5617e-03, -3.8718e-03,  4.3709e-03,\n",
       "          -1.1509e-02, -9.2677e-03,  8.4799e-03, -2.3442e-03,  4.8839e-03,\n",
       "           1.3518e-03,  2.9169e-03,  2.2559e-03,  4.1763e-03,  4.0392e-03,\n",
       "          -1.7779e-03,  3.0020e-03, -1.6659e-03, -1.2457e-03, -4.1953e-03,\n",
       "           4.4369e-03,  1.8592e-03, -1.5549e-03,  2.7385e-03, -1.1430e-02,\n",
       "          -1.3654e-02,  1.0230e-02],\n",
       "         [-1.3122e-03, -1.2669e-03, -1.0968e-03, -1.4784e-04,  5.0229e-04,\n",
       "          -2.5311e-05,  4.0829e-05,  1.6427e-03,  1.2197e-03, -3.1844e-04,\n",
       "           2.4161e-03, -1.5063e-03, -2.0558e-03, -3.5964e-04,  5.0891e-04,\n",
       "          -2.0124e-04, -4.5195e-05,  3.9544e-04, -1.2958e-03, -2.1575e-03,\n",
       "          -2.2979e-03, -2.3017e-03,  6.9008e-04, -7.1196e-05,  5.3631e-04,\n",
       "           8.7405e-04,  2.8517e-03,  2.6799e-03, -1.2180e-03,  2.8663e-03,\n",
       "          -2.5930e-03, -2.8912e-03, -9.4196e-04, -1.5403e-05,  7.5845e-04,\n",
       "           1.8790e-03,  5.4231e-03, -2.4167e-03, -2.1529e-03, -1.3909e-03,\n",
       "          -2.7317e-03,  6.4424e-04,  4.5813e-05,  9.9278e-04,  1.0844e-03,\n",
       "           2.7803e-03,  2.4209e-03, -1.1299e-03,  8.4388e-04, -2.5453e-03,\n",
       "          -3.1704e-03, -1.0336e-03, -1.0992e-03,  5.8877e-04,  2.3047e-03,\n",
       "           4.5423e-03, -7.4375e-05],\n",
       "         [ 2.4050e-03,  3.7396e-03,  4.6705e-03,  3.0821e-03, -2.0085e-04,\n",
       "           1.2661e-03,  1.7610e-03, -4.3338e-03, -1.7812e-03, -1.6028e-03,\n",
       "          -9.7216e-03,  1.4297e-03,  6.6953e-03,  1.5501e-03, -3.6570e-03,\n",
       "           1.2472e-03,  8.8896e-04, -1.7934e-03,  6.7920e-03,  2.3658e-03,\n",
       "           5.6516e-03,  4.9869e-03,  1.9908e-03,  9.0625e-04,  2.4209e-03,\n",
       "           1.9425e-03, -4.8593e-03, -1.2454e-03, -1.2741e-03, -8.6279e-03,\n",
       "           3.5242e-04,  8.0472e-03,  5.6422e-04, -4.4169e-03,  1.7029e-03,\n",
       "          -5.7005e-03, -5.4349e-03,  1.0740e-02,  5.1599e-03,  6.0478e-03,\n",
       "           6.5487e-03,  1.1105e-03, -4.4818e-04,  1.8758e-03,  9.0385e-04,\n",
       "          -6.5811e-03, -2.7937e-03,  8.1249e-05, -8.9103e-03,  2.3489e-03,\n",
       "           1.0600e-02,  7.8748e-04, -2.9851e-03,  1.3441e-03, -6.4085e-03,\n",
       "          -4.7285e-03,  1.1653e-02],\n",
       "         [-4.0359e-03, -1.4744e-03, -7.5598e-03, -3.9642e-03, -2.1528e-03,\n",
       "           1.9216e-03, -2.0632e-04,  6.8077e-03,  4.6010e-03,  2.1621e-03,\n",
       "           7.8096e-03, -3.8986e-03, -5.1551e-03, -4.0121e-03,  6.8008e-04,\n",
       "           2.7796e-03, -1.8358e-03,  5.0596e-03,  2.0320e-03, -6.3302e-03,\n",
       "          -4.6137e-03, -8.4915e-03, -3.1485e-03, -2.6110e-03,  2.5200e-03,\n",
       "           2.9366e-05,  1.0340e-02,  8.0215e-03,  8.0029e-04,  1.1140e-02,\n",
       "          -7.2756e-03, -6.7839e-03, -5.0188e-03,  8.3210e-04,  4.0577e-03,\n",
       "           1.6409e-03,  9.1131e-03,  3.2346e-04, -6.9636e-03, -4.7679e-03,\n",
       "          -7.3624e-03, -2.9441e-03, -1.2776e-03,  2.7704e-03, -3.2970e-05,\n",
       "           9.3605e-03,  6.8235e-03, -1.6016e-03,  9.9629e-03, -6.9811e-03,\n",
       "          -7.6833e-03, -3.6743e-03, -1.8122e-03,  3.5533e-03,  1.8720e-03,\n",
       "           1.2500e-02, -4.4227e-03],\n",
       "         [ 1.7790e-04, -9.6063e-04, -2.1301e-03, -3.2092e-03, -9.9403e-04,\n",
       "          -8.4556e-04, -1.9197e-03,  1.4385e-03, -7.6426e-04,  2.1525e-03,\n",
       "           2.4478e-03,  2.1493e-03, -2.6542e-03, -1.0654e-03,  2.3519e-03,\n",
       "          -9.7950e-04,  1.3250e-03,  2.7946e-03, -3.8700e-03,  1.0331e-02,\n",
       "           7.6415e-03,  9.7839e-03,  1.5775e-03, -2.0010e-03, -5.0098e-03,\n",
       "          -1.8909e-03, -1.2673e-02, -1.5306e-02,  2.5386e-03, -3.2763e-02,\n",
       "           1.9001e-02,  9.7695e-03,  5.3602e-03, -1.6100e-03, -5.3274e-03,\n",
       "           2.5133e-02,  2.3024e-03, -7.5655e-04,  8.3274e-03,  4.3019e-03,\n",
       "           8.7391e-03,  2.5485e-03, -8.4557e-04, -4.7179e-03, -1.4352e-03,\n",
       "          -9.0923e-03, -1.1709e-02,  1.9879e-03, -2.6133e-02,  1.5035e-02,\n",
       "           8.3264e-03,  3.0261e-03,  8.5446e-05, -4.4080e-03,  2.4130e-02,\n",
       "           3.6284e-03, -1.3095e-03],\n",
       "         [ 5.7806e-03,  2.0089e-03,  3.7519e-03, -2.6785e-04, -1.0076e-03,\n",
       "          -2.5431e-04, -1.0461e-03, -4.0482e-03, -3.9291e-03,  5.2185e-04,\n",
       "          -5.5013e-03,  3.5338e-03,  4.5708e-03,  5.4309e-04, -1.4513e-03,\n",
       "          -1.0983e-03,  1.5368e-04,  1.0974e-03,  3.0809e-03,  5.0471e-03,\n",
       "          -2.0404e-03, -2.1656e-03, -4.4305e-03, -2.6000e-03,  5.4121e-04,\n",
       "          -2.0778e-03,  9.2252e-05, -9.6918e-04,  2.9839e-03,  7.6929e-03,\n",
       "          -7.7361e-04, -1.6944e-03, -1.9078e-03,  1.2333e-03, -1.3826e-04,\n",
       "          -1.1292e-02,  2.7877e-03,  7.4925e-03,  6.2730e-03, -6.3301e-04,\n",
       "          -9.0080e-04, -3.5693e-03, -2.8388e-03,  6.0019e-04, -1.7509e-03,\n",
       "           5.7716e-04, -5.4039e-04,  3.8209e-03,  5.4044e-03, -1.0611e-03,\n",
       "           1.0623e-03, -2.7890e-03,  1.5914e-03, -5.0267e-04, -1.0223e-02,\n",
       "          -2.5254e-03,  1.4047e-02],\n",
       "         [ 1.8300e-03, -6.5745e-04,  1.4361e-03, -7.8429e-04,  6.8374e-05,\n",
       "          -1.5521e-03, -1.4561e-03, -1.2724e-03, -2.0520e-03, -2.4998e-04,\n",
       "           1.2779e-03,  1.8072e-03, -1.8838e-04,  8.2888e-04,  3.2248e-04,\n",
       "          -1.8580e-03, -9.3258e-05,  2.2435e-03, -5.7367e-03,  7.0993e-05,\n",
       "          -3.9857e-03, -8.3900e-04, -1.2383e-03, -1.0383e-03, -5.5546e-04,\n",
       "          -1.4992e-03,  2.7478e-03,  1.2000e-03, -5.9734e-04,  7.6005e-03,\n",
       "          -2.3504e-03, -3.1144e-03, -6.2872e-05,  1.0400e-03, -7.0673e-04,\n",
       "          -2.6674e-03,  4.3247e-03, -2.0188e-03,  5.2711e-04, -2.5165e-03,\n",
       "           1.3434e-03,  3.7707e-04, -2.8527e-04, -3.0485e-04, -6.5743e-04,\n",
       "           3.3040e-04,  1.0203e-04, -1.6262e-03,  3.0194e-03, -8.7461e-04,\n",
       "          -7.6141e-04,  9.1842e-04, -2.1776e-04, -4.5447e-04, -7.5201e-04,\n",
       "           3.6667e-03, -3.3358e-03],\n",
       "         [ 1.0707e-03,  7.8375e-04,  2.0073e-03,  1.2895e-03,  5.1734e-04,\n",
       "          -4.3258e-05,  3.7500e-04, -1.6043e-03, -1.1134e-03, -7.3375e-04,\n",
       "          -3.4732e-03,  1.0454e-03,  1.6821e-03,  7.7692e-04, -7.3973e-04,\n",
       "          -4.4418e-04,  1.1391e-03, -6.2939e-04,  1.3057e-03,  3.9299e-03,\n",
       "           4.4321e-03,  2.8568e-03,  1.8330e-03, -2.3305e-04, -4.4703e-04,\n",
       "           9.7516e-04, -4.8200e-03, -4.5751e-03,  4.8383e-04, -1.2710e-02,\n",
       "           5.9274e-03,  4.5739e-03,  6.3911e-04, -2.0667e-03, -4.2417e-04,\n",
       "           4.9542e-03,  8.3935e-04,  3.9793e-03,  4.0032e-03,  3.9337e-03,\n",
       "           2.6529e-03,  1.1924e-03, -5.4838e-04, -6.3172e-04,  3.9217e-04,\n",
       "          -3.8637e-03, -3.9411e-03,  1.1459e-03, -1.0398e-02,  5.4863e-03,\n",
       "           4.5521e-03,  1.8174e-05, -8.6276e-04, -6.6996e-04,  4.4357e-03,\n",
       "           3.3747e-04,  4.1370e-03],\n",
       "         [-2.9320e-03, -1.6312e-03, -2.0258e-03, -2.0761e-04,  7.6321e-04,\n",
       "          -2.5655e-04,  9.1678e-05,  2.2248e-03,  2.0217e-03,  2.9339e-04,\n",
       "           4.9512e-03, -1.8244e-03, -2.9298e-03, -9.5779e-05,  2.1407e-03,\n",
       "           5.9850e-05, -6.1256e-04, -2.8004e-03, -3.2959e-03, -3.9256e-03,\n",
       "          -1.8735e-03, -1.1784e-03, -1.3163e-05,  1.5530e-03, -5.5865e-04,\n",
       "          -4.1172e-04,  2.9047e-03,  2.2503e-03, -7.2065e-04,  7.1367e-03,\n",
       "          -1.9505e-03, -3.3911e-03,  9.2332e-04,  2.6983e-03, -1.1752e-03,\n",
       "           7.2999e-04, -3.6791e-03, -1.0907e-02, -5.1965e-03, -2.1997e-03,\n",
       "          -3.1892e-03, -7.2070e-04,  5.5574e-04,  8.0214e-04, -1.9073e-04,\n",
       "           3.0999e-03,  3.0811e-03, -7.4714e-04,  8.2570e-03, -3.0160e-03,\n",
       "          -4.9439e-03,  1.2761e-03,  1.7014e-03,  5.7561e-04, -1.2625e-04,\n",
       "          -6.4107e-04, -1.1636e-02],\n",
       "         [ 1.2367e-03, -6.6879e-04,  9.9316e-04,  6.5566e-04,  6.4079e-04,\n",
       "          -1.1164e-03, -2.2717e-04, -1.0121e-03, -1.2035e-03, -7.2225e-04,\n",
       "           3.4490e-04,  6.1981e-04, -4.2522e-05,  6.1805e-04, -7.1587e-05,\n",
       "          -1.1146e-03,  3.6217e-05,  1.0795e-03, -2.1082e-03,  1.3162e-03,\n",
       "          -4.9997e-04, -1.3856e-03, -1.5074e-03, -5.7534e-04, -1.7124e-04,\n",
       "          -7.1646e-04, -7.8831e-04, -1.0191e-03,  2.1485e-04,  4.1254e-03,\n",
       "           7.7146e-04, -1.1776e-03,  4.6213e-04,  5.2118e-04,  1.8015e-04,\n",
       "          -3.4025e-03,  3.7889e-03, -4.3515e-03,  1.6746e-04, -3.8450e-04,\n",
       "          -1.2891e-03, -1.0783e-03,  1.5206e-04, -4.9161e-04, -4.7527e-04,\n",
       "          -6.6086e-04, -8.0178e-04, -2.1677e-04,  3.7272e-03,  8.2598e-04,\n",
       "          -1.7461e-03,  7.5763e-04,  4.9129e-04, -2.4756e-04, -2.5997e-03,\n",
       "           2.9089e-03, -6.7743e-03]], device='cuda:0'),\n",
       " 'node_network.0.bias': tensor([ 0.0053, -0.0003,  0.0017, -0.0014, -0.0032, -0.0053,  0.0057, -0.0006,\n",
       "          0.0056, -0.0060, -0.0037,  0.0018, -0.0006,  0.0024, -0.0022,  0.0008],\n",
       "        device='cuda:0'),\n",
       " 'node_network.1.weight': tensor([ 9.8672e-04,  9.2226e-04,  8.3651e-04, -1.1705e-03,  1.7172e-05,\n",
       "         -9.1584e-04,  8.7258e-05,  7.1084e-04,  1.6701e-04, -2.5302e-03,\n",
       "          1.9923e-03,  6.8224e-04, -3.7284e-05, -1.2182e-03,  1.9553e-03,\n",
       "         -3.6045e-04], device='cuda:0'),\n",
       " 'node_network.1.bias': tensor([ 0.0058, -0.0018,  0.0024, -0.0067, -0.0016, -0.0043,  0.0013, -0.0015,\n",
       "          0.0033, -0.0050,  0.0042, -0.0007, -0.0006,  0.0024, -0.0030, -0.0009],\n",
       "        device='cuda:0'),\n",
       " 'node_network.3.weight': tensor([[ 3.6743e-03,  2.1622e-03,  1.4985e-04, -4.1238e-03, -2.9466e-03,\n",
       "           4.3805e-03,  1.3188e-03, -1.8000e-03, -7.9539e-04,  4.3338e-03,\n",
       "           1.9772e-03, -5.2784e-03, -3.0620e-03, -1.0103e-05, -1.1691e-03,\n",
       "           3.3132e-04],\n",
       "         [-7.4324e-05,  1.0292e-03, -1.1225e-04, -1.2613e-03, -1.3261e-03,\n",
       "           3.0672e-04,  1.4974e-03, -2.4236e-04, -2.0411e-04, -3.1093e-04,\n",
       "           1.2069e-03, -1.3444e-03, -1.9869e-03,  1.0561e-03, -3.2126e-04,\n",
       "           3.1062e-04],\n",
       "         [ 2.7810e-03, -1.8301e-04, -1.4029e-03, -2.5484e-03, -1.7583e-03,\n",
       "           2.7707e-03,  1.6947e-03, -5.8867e-04, -1.0373e-03,  2.1036e-03,\n",
       "           4.5657e-04, -2.4111e-03, -1.2497e-03,  1.0803e-03, -1.8549e-03,\n",
       "           7.8964e-04],\n",
       "         [-1.7002e-03,  3.4594e-03,  4.6410e-04, -2.0939e-03,  5.9387e-04,\n",
       "          -7.4877e-03,  1.1469e-03,  6.6326e-03,  5.2348e-03, -4.8310e-03,\n",
       "          -4.9062e-03,  3.4266e-04, -3.2736e-03,  1.0884e-03,  6.8267e-03,\n",
       "          -6.6988e-04],\n",
       "         [ 5.2150e-03,  9.2613e-03, -6.7796e-03, -4.2045e-03, -1.4866e-03,\n",
       "           3.8093e-03, -2.6072e-03,  1.8699e-03, -4.7217e-03,  2.9739e-03,\n",
       "           1.2061e-02, -6.3000e-03, -7.1670e-03,  3.5657e-03,  1.6896e-03,\n",
       "          -6.8789e-03],\n",
       "         [-1.1644e-03, -1.9455e-04, -5.4302e-03, -2.2070e-03, -2.8224e-04,\n",
       "          -8.9671e-05,  5.6762e-03, -7.2162e-04,  2.3412e-03, -3.7766e-03,\n",
       "           1.1878e-03, -3.4136e-03, -1.2523e-03,  1.1197e-03,  2.2904e-03,\n",
       "           4.6905e-03],\n",
       "         [-4.5404e-03, -1.0362e-03,  1.8257e-03,  4.2365e-03,  3.7185e-03,\n",
       "          -5.9313e-03, -8.5122e-04,  2.4013e-03,  2.4077e-03, -3.0379e-03,\n",
       "          -3.6018e-03,  2.3414e-03, -6.2800e-04,  1.3256e-03,  3.4947e-03,\n",
       "           1.2773e-03],\n",
       "         [-5.1427e-03,  1.1855e-02, -1.1031e-02, -8.5181e-03,  6.0379e-03,\n",
       "          -1.4684e-02,  2.9043e-03,  1.3821e-02,  6.8320e-03, -1.4068e-02,\n",
       "           5.7639e-03, -3.0934e-03, -1.2923e-02,  1.0854e-02,  1.7413e-02,\n",
       "          -8.7009e-04],\n",
       "         [-4.4166e-03, -1.0943e-02,  4.6833e-03,  1.0532e-03,  6.3086e-04,\n",
       "           4.1048e-03,  2.4701e-03, -1.1309e-02,  3.3277e-03,  3.3888e-03,\n",
       "          -5.3070e-03,  1.0329e-03,  1.2046e-02, -2.6706e-03, -4.1796e-03,\n",
       "           9.6546e-03],\n",
       "         [ 1.0015e-02,  2.7275e-03,  2.0494e-03,  2.8812e-03, -4.4010e-03,\n",
       "           1.1346e-02, -7.0151e-03, -1.6359e-03, -1.1132e-02,  8.9064e-03,\n",
       "           5.6023e-03, -4.4637e-04, -6.4456e-04, -6.8793e-03, -1.1020e-02,\n",
       "          -8.6991e-03],\n",
       "         [-2.4516e-03, -6.1230e-03,  2.5912e-03,  6.1470e-03,  1.8555e-03,\n",
       "          -4.7477e-04, -1.5096e-03, -3.5582e-03, -5.4945e-04, -7.7330e-05,\n",
       "          -3.5606e-03,  5.4773e-03,  6.0444e-03, -3.3535e-03, -2.6278e-03,\n",
       "           1.2832e-03],\n",
       "         [-2.8554e-03,  2.8332e-03, -3.3185e-03, -1.9516e-03,  4.4711e-03,\n",
       "          -7.1034e-03,  9.0646e-04,  7.7590e-03,  3.4710e-03, -7.9666e-03,\n",
       "           1.9753e-03, -1.9960e-03, -5.9136e-03,  3.7665e-03,  7.7624e-03,\n",
       "           1.3819e-05],\n",
       "         [-3.3578e-03, -5.5978e-04,  7.2306e-04,  3.3619e-03,  8.9950e-04,\n",
       "          -5.2509e-03,  5.7991e-04,  2.9989e-03,  2.0825e-03, -3.1400e-03,\n",
       "          -2.6430e-03,  3.5678e-03, -1.5840e-05,  1.1757e-03,  2.6154e-03,\n",
       "           6.5758e-06],\n",
       "         [-1.8047e-03,  2.5412e-04, -2.6744e-04,  1.5576e-04, -1.8097e-04,\n",
       "          -3.3341e-03,  1.6679e-03, -2.4689e-03,  2.1465e-03, -1.8944e-03,\n",
       "          -1.9344e-03,  1.6275e-03,  3.1171e-03,  7.0468e-04,  1.7282e-03,\n",
       "           2.1185e-03],\n",
       "         [ 1.0305e-02, -2.5620e-03,  6.2501e-03,  2.4072e-03, -6.3156e-03,\n",
       "           1.4943e-02, -5.3367e-03, -4.9108e-03, -7.8641e-03,  1.4012e-02,\n",
       "           1.5433e-03, -1.5244e-03,  3.2635e-03, -8.8197e-03, -1.3122e-02,\n",
       "          -5.5595e-03],\n",
       "         [-4.4826e-03, -1.1980e-02,  9.6048e-03,  6.6660e-03,  4.9026e-04,\n",
       "           2.6948e-03, -2.5427e-03, -8.2468e-03, -1.5389e-03,  3.3850e-03,\n",
       "          -9.8211e-03,  1.1418e-02,  1.3645e-02, -4.0031e-03, -9.5254e-03,\n",
       "           2.2014e-03]], device='cuda:0'),\n",
       " 'node_network.3.bias': tensor([ 0.0020,  0.0001,  0.0012, -0.0070, -0.0037, -0.0041, -0.0052, -0.0250,\n",
       "          0.0088,  0.0125,  0.0035, -0.0086, -0.0052, -0.0015,  0.0178,  0.0144],\n",
       "        device='cuda:0'),\n",
       " 'node_network.4.weight': tensor([ 3.1948e-04, -2.6858e-04, -4.2460e-04,  1.1070e-03, -3.6392e-04,\n",
       "          3.1167e-04, -1.5125e-03, -5.4937e-04,  3.0528e-03, -8.5846e-04,\n",
       "         -2.3143e-03, -2.0569e-05, -1.2004e-03,  2.1001e-03,  1.0367e-03,\n",
       "         -1.8564e-03], device='cuda:0'),\n",
       " 'node_network.4.bias': tensor([ 0.0007,  0.0003,  0.0003, -0.0030, -0.0010, -0.0016, -0.0019, -0.0086,\n",
       "          0.0024,  0.0056,  0.0014, -0.0029, -0.0022, -0.0008,  0.0067,  0.0047],\n",
       "        device='cuda:0'),\n",
       " 'node_network.6.weight': tensor([[ 8.1942e-03,  8.6568e-03,  8.3785e-03, -6.2643e-03, -9.0764e-04,\n",
       "          -9.7120e-03, -5.7708e-03, -6.6371e-03,  7.3435e-03, -1.5736e-02,\n",
       "          -1.1671e-02,  3.9052e-03,  1.8134e-02,  1.2063e-02, -3.8847e-03,\n",
       "          -9.6602e-03],\n",
       "         [ 1.1158e-03,  6.6882e-03, -1.7273e-03, -7.6199e-03, -7.7303e-03,\n",
       "           1.2767e-02,  1.3269e-02,  7.5559e-03, -8.7571e-03,  1.4066e-02,\n",
       "           1.2887e-04, -1.0741e-02, -3.8176e-03, -1.5632e-02,  4.1775e-03,\n",
       "          -5.6340e-03],\n",
       "         [-7.9776e-04, -8.4297e-03,  5.9570e-04,  4.4320e-03,  7.6958e-03,\n",
       "          -8.2951e-03, -7.8030e-03, -3.7014e-03,  3.7461e-03, -1.0943e-02,\n",
       "           2.4761e-03,  7.0233e-03, -1.6439e-04,  8.8647e-03, -2.4057e-03,\n",
       "           7.1552e-03],\n",
       "         [ 7.4458e-03,  1.1148e-02,  2.2894e-03, -7.0994e-03, -4.9871e-03,\n",
       "           5.2621e-04,  2.9884e-03,  2.8268e-03,  5.4486e-04, -3.7902e-03,\n",
       "          -9.2744e-03, -4.2087e-03,  9.9047e-03, -1.8407e-03,  2.5573e-03,\n",
       "          -1.0602e-02],\n",
       "         [-5.0254e-03, -8.3552e-03, -1.6010e-03,  7.4146e-03,  4.7860e-03,\n",
       "          -3.8197e-03, -5.3119e-03, -3.8177e-03,  1.7024e-03, -4.4105e-04,\n",
       "           6.3501e-03,  5.1957e-03, -5.4528e-03,  6.7680e-03, -2.4166e-03,\n",
       "           9.5074e-03],\n",
       "         [ 4.2516e-03,  7.6921e-03,  8.2052e-04, -6.1368e-03, -4.3139e-03,\n",
       "           2.5087e-03,  2.1122e-03,  5.0153e-04,  1.4223e-03,  6.4638e-04,\n",
       "          -5.2509e-03, -1.1486e-03,  6.4023e-03, -2.3283e-03,  2.5277e-04,\n",
       "          -8.8964e-03],\n",
       "         [ 1.5657e-03,  4.5132e-04,  9.5768e-04,  1.7021e-03,  1.4619e-04,\n",
       "          -9.2926e-04,  1.2448e-04,  3.7808e-04, -1.2475e-06, -4.3545e-03,\n",
       "          -9.5927e-04, -1.2421e-03,  1.9012e-03, -9.7571e-04,  8.6664e-04,\n",
       "           6.2786e-05],\n",
       "         [-1.4535e-03,  1.6745e-03,  1.5893e-03,  2.2305e-03, -1.0307e-03,\n",
       "           3.5481e-03,  3.2212e-03,  3.0177e-03, -4.6823e-03, -8.5942e-06,\n",
       "          -1.5542e-03, -4.0744e-03, -1.5537e-03, -4.4395e-03,  3.4116e-03,\n",
       "           2.6255e-03],\n",
       "         [-1.8551e-03, -2.1165e-03, -3.4516e-03, -4.1440e-03, -7.0813e-04,\n",
       "           1.7823e-03, -3.5043e-04, -1.1223e-03,  1.4133e-03,  7.8433e-03,\n",
       "           4.4616e-03,  4.2034e-03, -4.5160e-03,  1.8349e-03, -3.1663e-03,\n",
       "          -1.6265e-03],\n",
       "         [-1.2396e-02, -1.4612e-02, -6.3753e-03,  2.6042e-03,  2.4177e-03,\n",
       "           3.1998e-03,  3.4848e-03,  3.1284e-04, -4.9854e-03,  1.8218e-02,\n",
       "           1.8311e-02,  3.3323e-03, -2.1501e-02,  2.5560e-04, -5.3056e-03,\n",
       "           1.4491e-02],\n",
       "         [ 3.1800e-03,  6.0156e-03, -4.4475e-04, -3.2659e-03, -2.1236e-03,\n",
       "           1.4940e-03,  9.3152e-04,  7.8863e-04, -7.4916e-04,  3.5564e-04,\n",
       "          -4.4065e-03, -2.6788e-03,  4.4886e-03, -1.3995e-03,  1.6169e-03,\n",
       "          -4.9055e-03],\n",
       "         [ 2.2699e-03,  1.4795e-03,  3.2097e-03,  2.6279e-03, -1.3545e-03,\n",
       "          -7.1423e-03,  3.5641e-05,  1.1007e-03,  3.4969e-03, -7.5699e-03,\n",
       "          -1.4076e-03, -1.7679e-03,  2.9866e-03,  1.9362e-03, -1.5955e-03,\n",
       "           1.5792e-03],\n",
       "         [ 8.9401e-03,  6.1154e-03, -7.9795e-04, -1.7331e-03,  2.1949e-03,\n",
       "          -6.4471e-03, -6.1513e-03, -2.1421e-03,  5.2315e-03, -9.7011e-03,\n",
       "          -8.8943e-03,  1.6583e-03,  1.1732e-02,  6.2704e-03,  1.2068e-03,\n",
       "          -8.0918e-03],\n",
       "         [ 2.6644e-03,  4.7453e-03, -8.0908e-04, -1.5487e-03,  8.3856e-04,\n",
       "           1.7412e-03, -4.6423e-03, -2.9299e-03,  2.5547e-03, -1.3696e-03,\n",
       "          -4.5784e-03,  1.4306e-03,  6.1958e-03,  1.5663e-03,  1.4066e-03,\n",
       "          -6.3305e-03],\n",
       "         [-3.6857e-03, -4.4414e-03,  6.4704e-04,  5.0485e-03,  1.4354e-03,\n",
       "          -1.0257e-03, -3.1275e-03,  2.1193e-04,  1.1045e-03, -1.0313e-03,\n",
       "           2.9071e-03,  2.7382e-03, -3.1935e-03,  1.2537e-03, -9.1253e-04,\n",
       "           6.3217e-03],\n",
       "         [-1.4414e-02, -1.6712e-02, -3.2809e-03,  1.1752e-02,  3.6413e-03,\n",
       "           9.8034e-03,  6.9905e-03,  3.6563e-03, -9.3850e-03,  1.3817e-02,\n",
       "           1.3362e-02, -3.6252e-03, -2.1546e-02, -1.4197e-02,  4.1907e-03,\n",
       "           1.4004e-02]], device='cuda:0'),\n",
       " 'node_network.6.bias': tensor([ 0.0196,  0.0038, -0.0083,  0.0138, -0.0088,  0.0094,  0.0005,  0.0009,\n",
       "         -0.0038, -0.0238,  0.0069,  0.0015,  0.0100,  0.0069, -0.0056, -0.0230],\n",
       "        device='cuda:0'),\n",
       " 'node_network.7.weight': tensor([-0.0010,  0.0018, -0.0014,  0.0011,  0.0019,  0.0007, -0.0012, -0.0007,\n",
       "         -0.0007,  0.0022, -0.0014,  0.0008, -0.0033,  0.0011,  0.0011, -0.0027],\n",
       "        device='cuda:0'),\n",
       " 'node_network.7.bias': tensor([ 0.0067, -0.0002, -0.0030,  0.0048, -0.0038,  0.0030, -0.0004, -0.0007,\n",
       "         -0.0024, -0.0093,  0.0017, -0.0002,  0.0033,  0.0023, -0.0019, -0.0104],\n",
       "        device='cuda:0'),\n",
       " 'node_network.9.weight': tensor([[ 8.9388e-04, -1.0105e-02, -1.7736e-04,  3.6366e-03,  1.7039e-02,\n",
       "          -1.1548e-02, -1.0004e-02,  5.0462e-04, -3.2814e-03,  7.2303e-03,\n",
       "           4.3243e-03,  8.9732e-03,  2.2431e-04, -1.6037e-04,  9.3190e-04,\n",
       "          -7.5687e-03],\n",
       "         [-4.0037e-03, -5.5275e-04,  5.7970e-03,  2.8310e-03,  6.5612e-03,\n",
       "           1.9834e-04, -8.2850e-03,  2.2520e-03,  4.5639e-03,  1.8186e-03,\n",
       "          -5.0114e-03,  2.4004e-03, -6.0802e-03,  4.2071e-03, -2.8071e-03,\n",
       "          -2.3301e-03],\n",
       "         [ 3.4406e-04, -7.8809e-03, -4.1172e-03,  3.0177e-03,  9.8089e-03,\n",
       "          -8.2449e-03, -6.3799e-03, -1.5798e-03, -8.2949e-03,  1.0118e-02,\n",
       "           7.1587e-03,  4.5237e-03,  5.5993e-03, -2.7578e-03,  6.0022e-03,\n",
       "          -8.4271e-03],\n",
       "         [ 1.4249e-03,  8.5158e-04, -5.5035e-03, -1.3290e-04,  1.4844e-03,\n",
       "          -3.4017e-03, -1.0527e-03, -2.9373e-03, -5.8302e-03,  4.0628e-03,\n",
       "           6.9574e-03,  4.1545e-04,  6.0468e-03, -3.7513e-03,  2.4709e-03,\n",
       "          -4.5075e-03],\n",
       "         [-2.9675e-03,  1.5451e-03,  9.2173e-03, -1.7152e-03,  1.3723e-02,\n",
       "          -6.5097e-04, -1.2492e-02,  4.9238e-03,  9.1055e-03, -2.5265e-03,\n",
       "          -7.1286e-03,  8.0740e-03, -9.7250e-03,  5.2780e-03, -9.3632e-03,\n",
       "          -1.3177e-03],\n",
       "         [ 3.5162e-03,  4.4629e-03,  5.9468e-03, -7.5760e-03,  4.2989e-03,\n",
       "           1.4166e-03, -2.3344e-03,  3.2902e-03,  1.0710e-02, -1.0375e-02,\n",
       "          -5.8063e-03,  4.9514e-03, -8.0888e-03,  3.6814e-03, -1.4046e-02,\n",
       "           6.6159e-03],\n",
       "         [-7.5538e-03,  2.2856e-03,  1.8026e-02,  5.9026e-03,  5.1886e-03,\n",
       "           7.3845e-03, -9.4668e-03,  4.4329e-03,  1.5120e-02, -6.6662e-03,\n",
       "          -1.8026e-02,  2.8928e-03, -1.9356e-02,  1.0651e-02, -6.0897e-03,\n",
       "           5.6495e-04],\n",
       "         [-4.2953e-03,  5.0014e-03,  6.6417e-03,  2.5114e-03, -3.7729e-03,\n",
       "           4.1628e-03, -1.4252e-03,  1.9839e-03,  6.2691e-03, -5.1046e-03,\n",
       "          -6.4562e-03, -2.3382e-03, -7.3959e-03,  3.3538e-03, -2.4179e-03,\n",
       "           2.8391e-03],\n",
       "         [ 5.7024e-04, -4.1169e-03, -7.9805e-03,  8.6116e-04, -4.7550e-04,\n",
       "          -4.0316e-03,  1.9630e-03, -1.4207e-03, -7.8618e-03,  5.8299e-03,\n",
       "           7.6347e-03, -1.0723e-03,  8.8097e-03, -4.8639e-03,  6.1966e-03,\n",
       "          -2.1649e-03],\n",
       "         [ 2.0046e-03,  5.7449e-03, -5.1608e-03, -2.3664e-03, -1.0469e-02,\n",
       "           3.6982e-03,  7.5012e-03, -2.5237e-03, -7.9731e-04, -3.0327e-03,\n",
       "           2.8144e-03, -4.1627e-03,  4.0661e-03, -3.1043e-03,  1.1656e-04,\n",
       "           2.2233e-03],\n",
       "         [-4.9516e-03, -4.4423e-03,  1.1707e-02,  6.2295e-03,  1.9807e-02,\n",
       "          -3.7394e-03, -1.9623e-02,  4.6019e-03,  6.8551e-03,  3.8852e-03,\n",
       "          -7.3635e-03,  9.7722e-03, -1.3071e-02,  6.6487e-03, -6.4026e-03,\n",
       "          -6.0038e-03],\n",
       "         [-7.5144e-05,  9.4294e-04, -3.7832e-03, -4.0939e-04, -6.0823e-03,\n",
       "           1.4720e-03,  5.2091e-03, -1.2266e-03, -2.8917e-03,  4.6189e-04,\n",
       "           3.0702e-03, -3.9325e-03,  4.2231e-03, -2.0971e-03,  3.6459e-03,\n",
       "           9.4434e-04],\n",
       "         [ 1.0122e-02, -4.7660e-03, -2.2819e-02, -7.9778e-03, -1.1811e-02,\n",
       "          -3.0117e-03,  1.7286e-02, -6.8535e-03, -2.1178e-02,  8.5260e-03,\n",
       "           2.0226e-02, -5.5397e-03,  2.5323e-02, -1.4101e-02,  1.2912e-02,\n",
       "          -1.0134e-03],\n",
       "         [-7.0955e-04,  2.8190e-03,  4.6569e-03, -1.1984e-03, -1.1068e-03,\n",
       "           2.4212e-03, -6.8152e-04,  3.1960e-03,  5.8990e-03, -3.8048e-03,\n",
       "          -6.0471e-03, -1.6274e-03, -5.0796e-03,  3.9599e-03, -5.4377e-03,\n",
       "           4.7079e-03],\n",
       "         [ 6.9299e-03,  1.6336e-03, -9.4239e-03, -3.3475e-03, -1.3724e-02,\n",
       "           6.4866e-04,  1.4061e-02, -5.6022e-03, -6.7343e-03, -4.0859e-04,\n",
       "           6.5620e-03, -6.2610e-03,  9.8395e-03, -5.0630e-03,  5.0767e-03,\n",
       "           1.8663e-03],\n",
       "         [ 3.8541e-03, -2.7396e-03, -9.5359e-03,  1.0096e-03, -4.3517e-03,\n",
       "          -3.7829e-03,  5.7572e-03, -4.2947e-03, -8.9073e-03,  5.6448e-03,\n",
       "           8.3434e-03, -2.5066e-03,  1.0588e-02, -5.7543e-03,  8.1347e-03,\n",
       "          -3.5172e-03]], device='cuda:0'),\n",
       " 'node_network.9.bias': tensor([ 8.9954e-05,  7.3803e-03, -7.0924e-03, -8.1370e-03,  1.2596e-02,\n",
       "          1.2197e-02,  2.2975e-02,  7.7796e-03, -1.1036e-02, -4.4116e-03,\n",
       "          1.6163e-02, -5.4457e-03, -2.9614e-02,  7.3053e-03, -1.0998e-02,\n",
       "         -1.1795e-02], device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_not_checkpointed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_encoder.0.weight': tensor([[-1.5066e-03, -7.5187e-04, -2.2855e-03],\n",
       "         [ 4.5522e-04, -1.0468e-03, -2.0531e-03],\n",
       "         [-7.0288e-04, -4.6534e-04, -4.4859e-04],\n",
       "         [-4.4661e-04, -1.1200e-03,  8.9562e-04],\n",
       "         [ 4.0046e-04,  1.4957e-03,  4.7784e-04],\n",
       "         [ 1.7397e-03,  1.2750e-03, -6.1366e-04],\n",
       "         [ 1.1351e-03,  4.6967e-04, -2.6968e-04],\n",
       "         [ 4.4861e-04, -2.5037e-03,  3.3685e-03],\n",
       "         [-1.8265e-03,  2.8013e-04,  1.3742e-04],\n",
       "         [-4.4511e-05, -6.4947e-04,  2.5390e-03],\n",
       "         [ 1.0493e-04, -4.2207e-04, -4.8594e-03],\n",
       "         [-1.5353e-05, -1.5799e-04,  3.1580e-03],\n",
       "         [ 8.5638e-04,  2.9611e-03, -3.0343e-04],\n",
       "         [ 9.0987e-04,  3.5355e-04,  5.5008e-04],\n",
       "         [ 3.4069e-04, -8.3479e-05, -5.7403e-04],\n",
       "         [-1.8485e-03,  3.6549e-04,  2.8088e-04]], device='cuda:0'),\n",
       " 'node_encoder.0.bias': tensor([-1.0034e-03,  4.6029e-03,  5.1182e-04,  5.9158e-04,  1.5166e-03,\n",
       "          1.0189e-03,  4.4330e-03,  4.3456e-03, -6.1968e-03,  1.7150e-04,\n",
       "          5.3703e-03, -2.8836e-03, -9.4906e-03,  7.3239e-05, -5.7473e-04,\n",
       "         -2.4863e-03], device='cuda:0'),\n",
       " 'node_encoder.1.weight': tensor([ 9.9291e-04, -5.6682e-04,  5.6536e-05,  9.5336e-04, -2.2453e-04,\n",
       "          2.4505e-04,  1.3654e-03,  2.0172e-04, -6.7914e-04, -4.2318e-04,\n",
       "         -6.7828e-04, -6.0849e-04, -1.0910e-03,  5.7362e-04,  5.1472e-04,\n",
       "          8.4871e-05], device='cuda:0'),\n",
       " 'node_encoder.1.bias': tensor([-7.5766e-05,  2.1466e-03,  6.4645e-04,  5.4691e-04,  1.1254e-03,\n",
       "          6.8350e-04,  2.4741e-03,  2.3992e-03, -2.1007e-03,  3.6469e-04,\n",
       "          2.3877e-03, -9.0526e-04, -4.4660e-03,  2.7401e-04,  4.0418e-05,\n",
       "         -5.3065e-04], device='cuda:0'),\n",
       " 'edge_network.0.weight': tensor([[ 9.7919e-03,  5.0549e-03,  5.7377e-03, -7.3867e-04, -3.5066e-03,\n",
       "          -8.2730e-04,  1.9231e-03, -6.7834e-03, -3.4318e-03,  1.1731e-03,\n",
       "          -1.0288e-02,  4.6736e-03,  9.7351e-03, -1.9147e-03, -2.2950e-04,\n",
       "          -3.8127e-04, -1.1399e-02, -2.7692e-03,  2.4572e-02,  1.0100e-02,\n",
       "           5.8296e-03,  4.1189e-03, -3.5195e-04, -4.2721e-03, -1.3579e-03,\n",
       "           2.3782e-03, -5.9238e-03, -3.5626e-03,  2.2041e-03, -1.0646e-02,\n",
       "           4.7228e-03,  9.2995e-03, -1.6864e-03,  4.6627e-04, -1.5422e-04,\n",
       "          -1.1012e-02, -4.5645e-03,  2.3656e-02],\n",
       "         [-3.2675e-03, -2.3826e-03, -5.4209e-03, -2.1907e-03,  1.3170e-03,\n",
       "           1.5182e-03, -5.8282e-04,  7.5801e-03,  3.6135e-03, -6.8034e-04,\n",
       "           7.4044e-03, -6.1785e-03, -6.3952e-03, -2.4673e-03, -6.0523e-04,\n",
       "          -5.1098e-04, -5.1263e-04,  3.2633e-03,  2.3060e-03, -3.4334e-03,\n",
       "          -2.4195e-03, -5.7377e-03, -1.8368e-03,  8.3738e-04,  1.9369e-03,\n",
       "          -2.0983e-04,  6.9667e-03,  3.7154e-03, -6.5419e-04,  7.2760e-03,\n",
       "          -6.1154e-03, -6.0952e-03, -2.1405e-03, -7.0942e-04,  6.0309e-04,\n",
       "          -4.1951e-04,  5.0891e-03,  4.0713e-03],\n",
       "         [-3.8034e-03, -5.9968e-03, -1.1787e-03,  2.8496e-04,  2.5078e-03,\n",
       "           1.1057e-03, -1.1609e-03,  5.6695e-03,  4.8681e-03, -1.9273e-03,\n",
       "           1.2664e-02, -8.6842e-03, -5.4552e-03,  1.5235e-04,  1.3400e-03,\n",
       "          -1.3119e-03, -3.1105e-03, -4.8572e-04, -7.6307e-03, -4.7043e-03,\n",
       "          -6.4985e-03, -2.9651e-03,  9.5851e-04,  2.6359e-03,  1.1113e-03,\n",
       "          -6.9044e-04,  5.4406e-03,  5.2135e-03, -1.6224e-03,  1.2366e-02,\n",
       "          -8.8890e-03, -7.0411e-03,  3.6848e-04,  1.1529e-03, -3.5112e-04,\n",
       "          -3.3249e-03, -6.6500e-03, -3.6045e-03],\n",
       "         [-2.1633e-03, -4.1706e-03,  1.5089e-03, -1.4669e-03,  1.0295e-03,\n",
       "           5.3739e-03,  4.9598e-04,  6.3481e-03,  8.1264e-03, -3.3034e-03,\n",
       "           2.0550e-02, -1.3418e-02, -3.2779e-04,  1.7961e-03,  4.5040e-03,\n",
       "           6.2703e-04, -2.0337e-02, -6.7060e-03,  2.8648e-03, -2.3748e-03,\n",
       "          -3.5098e-03,  8.0572e-04, -1.0325e-03,  8.4697e-05,  5.3680e-03,\n",
       "           8.4901e-04,  6.5513e-03,  8.3078e-03, -3.1479e-03,  2.0896e-02,\n",
       "          -1.4059e-02, -4.5348e-04,  3.4780e-03,  4.9383e-03,  1.0139e-03,\n",
       "          -2.0770e-02, -8.7522e-03, -6.0883e-04],\n",
       "         [-4.9854e-03, -8.0867e-04, -6.3690e-03,  2.3122e-03,  2.1752e-03,\n",
       "          -4.9832e-03, -1.5231e-04,  2.2757e-03, -1.4308e-03,  3.1125e-03,\n",
       "          -7.7527e-03,  3.9475e-03, -6.2175e-03, -2.9900e-03,  3.9273e-05,\n",
       "          -2.7868e-04,  1.7824e-02, -1.8713e-03,  2.5377e-03, -4.9124e-03,\n",
       "          -1.3324e-03, -6.1523e-03,  1.3704e-03,  2.3420e-03, -5.1025e-03,\n",
       "          -5.0792e-04,  2.1776e-03, -1.8899e-03,  2.3650e-03, -8.6494e-03,\n",
       "           4.5633e-03, -7.1943e-03, -4.2906e-03, -1.7625e-03, -1.0729e-03,\n",
       "           1.7844e-02,  2.1184e-03,  4.5146e-03],\n",
       "         [-5.1168e-03, -1.8206e-03, -1.0266e-02,  9.3685e-04,  2.1550e-03,\n",
       "          -4.4561e-03,  1.3866e-04,  6.0944e-03, -6.1764e-04,  3.3926e-03,\n",
       "          -5.8793e-03,  2.6256e-03, -8.9824e-03, -5.3195e-03, -4.1600e-04,\n",
       "           7.0668e-04,  1.8713e-02,  8.2841e-03,  2.0947e-03, -5.1610e-03,\n",
       "          -2.4859e-03, -9.3720e-03, -3.4918e-04,  3.1175e-03, -4.5366e-03,\n",
       "          -9.6083e-04,  7.0657e-03, -9.5260e-04,  2.0272e-03, -6.0547e-03,\n",
       "           2.3719e-03, -1.0267e-02, -6.7024e-03, -3.1953e-03, -1.0864e-03,\n",
       "           1.8770e-02,  8.8426e-03,  7.0884e-03],\n",
       "         [ 1.7452e-03, -1.7916e-03,  6.3006e-03,  9.6864e-04, -8.3380e-04,\n",
       "           1.0500e-03, -8.9648e-04, -4.1143e-03,  4.9644e-04, -3.0978e-03,\n",
       "           4.7846e-03, -1.6782e-03,  3.0777e-03,  4.2070e-03, -1.2166e-04,\n",
       "          -8.3234e-04, -9.2005e-03, -2.0270e-03, -1.0576e-02,  2.0104e-03,\n",
       "          -2.2571e-03,  5.1353e-03,  1.6924e-03, -1.0519e-03,  8.2529e-04,\n",
       "          -3.5617e-04, -4.4602e-03,  9.8279e-04, -2.0454e-03,  4.7002e-03,\n",
       "          -1.9059e-03,  2.8638e-03,  3.9210e-03,  1.0958e-03,  2.1210e-04,\n",
       "          -9.5673e-03, -3.0843e-03, -7.2264e-03],\n",
       "         [ 1.2892e-03,  8.4618e-04, -2.0581e-03, -7.6990e-04, -9.9113e-04,\n",
       "          -7.0041e-04, -6.0216e-04,  1.1841e-03, -1.4025e-03,  2.7016e-04,\n",
       "          -3.2604e-03,  1.1270e-03, -1.7255e-03, -1.7281e-03, -3.1517e-03,\n",
       "          -1.0703e-03,  4.5466e-03,  4.8423e-03,  2.9591e-03,  1.1556e-03,\n",
       "           5.6747e-04, -1.2512e-03, -2.8681e-05, -4.5088e-04, -4.7801e-04,\n",
       "          -3.4651e-04,  2.9518e-04, -1.2690e-03,  4.9130e-04, -2.7609e-03,\n",
       "           1.4961e-03, -4.4683e-04, -1.6307e-03, -1.8324e-03, -4.1311e-04,\n",
       "           4.7161e-03,  4.3636e-03,  4.6721e-04],\n",
       "         [-1.3150e-02, -7.9854e-03, -1.1374e-02,  3.4712e-04,  5.2057e-03,\n",
       "           1.5470e-03, -8.7879e-04,  1.1319e-02,  7.0388e-03, -2.5379e-04,\n",
       "           1.6202e-02, -9.6787e-03, -1.5657e-02, -2.0088e-05,  2.7621e-03,\n",
       "           1.6396e-03,  7.4973e-03,  5.8049e-04, -2.0468e-02, -1.3106e-02,\n",
       "          -8.8144e-03, -1.0532e-02, -6.5086e-04,  5.7009e-03,  1.6000e-03,\n",
       "          -1.8100e-03,  1.1875e-02,  7.1334e-03, -1.6083e-03,  1.6294e-02,\n",
       "          -1.0329e-02, -1.6613e-02, -1.2051e-03,  9.2332e-05,  4.2706e-04,\n",
       "           7.4632e-03,  3.2165e-03, -1.4531e-02],\n",
       "         [ 1.6406e-02,  1.9264e-02,  1.2322e-02,  6.0434e-04, -6.1359e-03,\n",
       "          -7.1272e-03,  5.4160e-04, -2.1508e-02, -2.0506e-02,  5.8648e-03,\n",
       "          -4.5263e-02,  3.1002e-02,  2.1522e-02,  5.7330e-04, -6.5190e-03,\n",
       "          -1.9701e-03,  1.9299e-02, -9.6184e-04,  1.3160e-02,  1.6045e-02,\n",
       "           1.9916e-02,  1.5207e-02,  6.5320e-04, -4.3525e-03, -7.1021e-03,\n",
       "           6.4377e-04, -2.1487e-02, -2.0775e-02,  6.1826e-03, -4.4510e-02,\n",
       "           3.2271e-02,  2.4598e-02,  6.2540e-04, -3.2216e-03, -2.7020e-03,\n",
       "           2.0300e-02, -2.9369e-03,  5.1564e-03],\n",
       "         [-5.5571e-03,  1.9657e-03, -7.3590e-04, -1.2380e-03,  1.7050e-03,\n",
       "           2.1667e-03, -8.0640e-04, -6.8087e-05, -8.9156e-04, -9.4335e-04,\n",
       "           2.1316e-03,  1.7810e-03, -1.4416e-03,  4.3079e-03,  2.6655e-04,\n",
       "           1.1550e-03,  4.2838e-03,  1.2510e-04, -2.0427e-02, -5.2512e-03,\n",
       "           1.3610e-03,  7.3505e-05, -8.7215e-04,  1.8784e-03,  2.4018e-03,\n",
       "          -4.8450e-04, -1.2741e-03, -8.5392e-04, -8.4364e-04,  2.2836e-03,\n",
       "           2.2441e-03, -4.8753e-04,  4.0983e-03,  1.0475e-03,  1.8850e-03,\n",
       "           4.7269e-03,  1.3651e-03, -2.0784e-02],\n",
       "         [-1.8744e-03, -7.7950e-03,  1.0091e-03,  9.7709e-04,  1.3496e-03,\n",
       "           3.5094e-03,  2.2928e-03,  4.8946e-03,  9.9818e-03, -2.2923e-03,\n",
       "           1.8865e-02, -1.3680e-02, -1.1749e-03, -9.2271e-04,  6.2987e-03,\n",
       "           1.5247e-03, -2.1637e-02, -7.9971e-03,  1.1935e-02, -1.8500e-03,\n",
       "          -6.6525e-03, -1.5430e-03,  6.7225e-05, -4.0156e-04,  3.0840e-03,\n",
       "           1.8078e-03,  5.8092e-03,  9.3436e-03, -2.3448e-03,  1.8038e-02,\n",
       "          -1.4113e-02, -3.6337e-03,  3.5710e-04,  4.1760e-03,  1.3110e-03,\n",
       "          -2.2688e-02, -4.5602e-03,  8.0669e-03],\n",
       "         [ 1.3001e-02,  5.2688e-03,  1.9178e-02,  8.7307e-04, -3.3921e-03,\n",
       "          -1.5823e-03, -2.5605e-03, -1.4952e-02, -8.3253e-03, -3.6037e-03,\n",
       "          -1.0920e-02,  9.6378e-03,  1.7421e-02,  5.4294e-03, -3.9709e-03,\n",
       "          -4.1638e-03, -7.3404e-03, -2.2795e-03, -6.9856e-03,  1.2879e-02,\n",
       "           5.8053e-03,  1.8097e-02,  2.2442e-03, -3.4501e-03, -1.5906e-03,\n",
       "          -1.2760e-03, -1.5728e-02, -7.7830e-03, -1.4265e-03, -1.0616e-02,\n",
       "           9.9885e-03,  1.8650e-02,  6.4537e-03,  3.3546e-04, -2.1785e-03,\n",
       "          -7.2017e-03, -6.3476e-03, -9.0791e-03],\n",
       "         [-5.4681e-04,  1.9559e-04,  2.4319e-03,  6.2855e-04, -1.1673e-03,\n",
       "           1.3606e-03,  8.0322e-04, -3.6582e-03,  5.9769e-04, -1.2759e-03,\n",
       "           9.0141e-04,  9.5389e-04,  3.0584e-03,  2.9333e-03,  1.3129e-03,\n",
       "           2.3073e-03, -4.8919e-03,  2.3835e-03, -3.9068e-03,  4.2640e-04,\n",
       "           2.1400e-04,  2.4844e-03,  1.9411e-04, -1.5534e-03,  1.1363e-03,\n",
       "           5.4407e-04, -3.8571e-03,  3.1051e-04, -1.1806e-03,  6.2845e-04,\n",
       "           1.0138e-03,  2.8639e-03,  2.6550e-03,  1.0780e-03,  1.8393e-03,\n",
       "          -5.1235e-03,  4.2522e-03, -4.8698e-03],\n",
       "         [-4.6589e-03, -3.7163e-03, -5.2330e-03, -1.0317e-03,  1.3607e-03,\n",
       "           3.0917e-03,  2.3374e-04,  7.6321e-03,  5.7880e-03, -7.2915e-04,\n",
       "           1.2690e-02, -9.3479e-03, -6.5367e-03, -6.4850e-04,  1.5036e-03,\n",
       "           7.3904e-04, -5.1020e-03, -1.2172e-03, -6.2923e-04, -5.1981e-03,\n",
       "          -3.7411e-03, -5.0288e-03, -6.3583e-04,  1.4103e-03,  3.2875e-03,\n",
       "           3.7701e-04,  7.5555e-03,  6.0237e-03, -1.0787e-03,  1.2942e-02,\n",
       "          -9.6353e-03, -6.4091e-03, -1.3831e-04,  1.3388e-03,  8.1714e-04,\n",
       "          -5.2389e-03, -3.2752e-03, -3.2149e-04],\n",
       "         [ 2.8908e-03,  3.8720e-03, -5.8520e-03, -4.9700e-04, -2.7788e-03,\n",
       "          -1.0468e-03,  1.2111e-03, -1.9140e-03, -3.9053e-03,  4.2937e-03,\n",
       "          -1.2830e-02,  6.9165e-03, -8.9975e-04, -3.3884e-03, -3.0132e-03,\n",
       "           1.8200e-03,  1.1367e-02,  6.8363e-03,  8.1927e-03,  3.3744e-03,\n",
       "           4.0181e-03, -3.3391e-03, -1.4221e-03, -2.4747e-03, -5.8357e-04,\n",
       "           4.2254e-05, -1.0062e-03, -3.9450e-03,  2.6823e-03, -1.2187e-02,\n",
       "           6.3755e-03,  3.6681e-04, -4.1629e-03, -5.0000e-03, -1.5040e-04,\n",
       "           1.1525e-02,  1.0924e-02,  8.0042e-03]], device='cuda:0'),\n",
       " 'edge_network.0.bias': tensor([ 0.0019, -0.0015, -0.0018, -0.0029,  0.0004, -0.0002,  0.0001,  0.0005,\n",
       "         -0.0035,  0.0076, -0.0005, -0.0028,  0.0036,  0.0002, -0.0025,  0.0014],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.1.weight': tensor([ 0.0045, -0.0001, -0.0015,  0.0033,  0.0048, -0.0011,  0.0005,  0.0002,\n",
       "         -0.0023, -0.0011,  0.0004, -0.0013, -0.0009,  0.0003, -0.0025,  0.0044],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.1.bias': tensor([ 0.0048, -0.0028, -0.0036, -0.0045,  0.0008, -0.0017,  0.0005,  0.0005,\n",
       "         -0.0079,  0.0151, -0.0022, -0.0040,  0.0081,  0.0011, -0.0048,  0.0027],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.3.weight': tensor([[-5.0765e-03, -1.2208e-03, -1.8040e-03, -8.1050e-03,  3.0643e-03,\n",
       "           3.5662e-03,  3.3014e-03,  1.5058e-03, -8.2702e-04, -2.6053e-03,\n",
       "           2.8161e-03, -1.0286e-04,  1.8318e-04, -1.8550e-03,  7.6871e-04,\n",
       "           2.6074e-03],\n",
       "         [-3.7010e-03,  3.2638e-04, -1.1155e-03, -5.7439e-03,  1.8119e-03,\n",
       "           3.2923e-03,  5.2561e-04,  1.0536e-03, -1.1093e-03, -7.8354e-04,\n",
       "           2.7339e-03, -4.7540e-04,  6.9909e-04, -2.2730e-03, -9.3080e-04,\n",
       "           1.7846e-03],\n",
       "         [ 3.7436e-03, -2.6944e-03, -1.1849e-03,  6.5303e-03, -4.2139e-03,\n",
       "           9.1761e-04, -1.4038e-04, -2.5661e-04,  3.0126e-03, -8.2801e-04,\n",
       "          -3.0724e-03,  1.9846e-03,  3.8855e-03, -2.0575e-03, -3.7994e-03,\n",
       "          -2.1021e-03],\n",
       "         [-1.6635e-02,  2.4330e-03,  1.4884e-02,  7.0627e-03, -1.9798e-03,\n",
       "          -1.0309e-02,  3.6854e-03, -4.8470e-03,  4.2091e-03, -5.3485e-03,\n",
       "           1.2455e-03, -1.2708e-03, -8.3951e-03,  6.0887e-03,  8.1738e-03,\n",
       "           8.1745e-03],\n",
       "         [-9.5005e-03,  1.3340e-02,  5.0611e-03, -1.0586e-02,  1.5128e-02,\n",
       "          -1.0589e-02, -1.5826e-03, -8.9253e-03,  1.6303e-03,  3.2133e-03,\n",
       "           8.3372e-04, -9.4803e-03, -1.4563e-02,  8.8561e-03,  1.1149e-02,\n",
       "           1.0690e-02],\n",
       "         [ 9.0430e-03, -2.4072e-03, -1.3195e-02, -1.2928e-02,  4.9018e-03,\n",
       "           1.3469e-02, -5.2869e-04,  6.5069e-03, -7.8337e-03,  3.0910e-03,\n",
       "           3.3530e-03,  9.4041e-04,  7.1900e-03, -6.8588e-03, -7.1682e-03,\n",
       "          -6.6631e-03],\n",
       "         [-2.7503e-03,  1.7580e-03, -2.0278e-03, -9.2758e-03,  6.1019e-03,\n",
       "           4.4035e-04,  1.5399e-03, -8.2605e-04,  4.9771e-04, -6.5680e-04,\n",
       "           1.5611e-03, -1.3658e-03, -2.6207e-03, -3.8819e-05,  2.4908e-03,\n",
       "           4.2943e-03],\n",
       "         [-3.2959e-03,  7.8468e-03,  5.4162e-03,  4.0354e-03,  5.5122e-03,\n",
       "          -1.2424e-02, -2.9847e-03, -1.0346e-02,  6.0526e-03,  3.4322e-03,\n",
       "          -3.8494e-03, -5.4936e-03, -1.0165e-02,  7.2732e-03,  6.6768e-03,\n",
       "           7.5819e-03],\n",
       "         [-4.6430e-04, -5.9935e-03,  5.2723e-03,  1.3035e-02, -8.2647e-03,\n",
       "          -1.3467e-03,  3.5723e-03,  3.2489e-03,  1.4740e-03, -6.1610e-03,\n",
       "          -3.4259e-03,  7.0280e-03,  4.2284e-03,  1.2079e-03, -2.9572e-03,\n",
       "          -4.8477e-03],\n",
       "         [-1.5791e-03,  2.2147e-03,  3.7286e-03,  1.5140e-03,  1.9076e-03,\n",
       "          -3.3459e-03,  2.8038e-04, -2.1980e-03,  3.4176e-05, -4.0573e-04,\n",
       "           2.1042e-03, -2.7092e-03, -3.4441e-03,  1.5561e-03,  2.8328e-03,\n",
       "           2.8870e-03],\n",
       "         [-1.3786e-03, -7.5092e-03,  2.1926e-03,  6.4392e-03, -6.5945e-03,\n",
       "           3.7625e-03,  4.9316e-03,  5.7985e-03, -3.6946e-04, -6.1414e-03,\n",
       "          -5.7357e-05,  6.3414e-03,  5.7403e-03, -1.6990e-03, -5.2710e-03,\n",
       "          -5.5847e-03],\n",
       "         [ 3.4134e-03,  8.0808e-03,  1.8103e-03,  2.9295e-03,  5.7382e-03,\n",
       "          -1.4165e-02, -5.3231e-03, -6.6842e-03,  7.6352e-03,  5.0027e-03,\n",
       "          -7.7295e-03, -3.6297e-03, -1.0113e-02,  8.9011e-03,  7.3701e-03,\n",
       "           2.7132e-03],\n",
       "         [ 1.1297e-02, -1.0162e-03, -3.6784e-03,  5.7247e-03, -5.0371e-03,\n",
       "           1.6647e-03, -3.8814e-03, -6.5876e-04, -5.2846e-04,  3.7455e-03,\n",
       "          -3.7865e-04, -1.3577e-03,  3.2381e-03, -2.9791e-03, -2.9595e-03,\n",
       "          -3.2265e-03],\n",
       "         [-2.5694e-02,  1.0944e-02,  1.0573e-02, -1.5244e-02,  1.4739e-02,\n",
       "          -1.0109e-02,  3.3729e-03, -8.5050e-03,  6.7458e-03, -3.5699e-03,\n",
       "           2.9209e-03, -5.5857e-03, -1.7219e-02,  6.9415e-03,  1.2197e-02,\n",
       "           1.6600e-02],\n",
       "         [ 4.2127e-02, -2.2818e-02, -2.6291e-02,  1.1318e-02, -3.0034e-02,\n",
       "           3.4187e-02, -8.3099e-03,  2.1589e-02, -2.0491e-02,  1.0433e-02,\n",
       "           2.8886e-03,  1.0922e-02,  3.8788e-02, -2.3248e-02, -2.6870e-02,\n",
       "          -3.1994e-02],\n",
       "         [ 4.5052e-04, -3.2836e-03,  3.5886e-04,  3.2944e-03, -2.7816e-03,\n",
       "           9.8914e-04,  1.5414e-03,  3.5438e-03, -1.3241e-04, -2.4176e-03,\n",
       "          -1.9439e-03,  4.2547e-03,  2.5676e-03,  1.8490e-04, -1.7028e-03,\n",
       "          -2.9147e-03]], device='cuda:0'),\n",
       " 'edge_network.3.bias': tensor([-0.0056, -0.0060,  0.0032,  0.0153,  0.0068, -0.0232, -0.0027,  0.0163,\n",
       "          0.0057,  0.0036, -0.0027,  0.0175,  0.0014,  0.0086, -0.0372, -0.0010],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.4.weight': tensor([ 8.2120e-04, -3.0040e-05,  1.1651e-03, -3.8065e-03, -8.1002e-04,\n",
       "         -2.6934e-03,  1.9857e-03, -2.4507e-03, -2.8926e-03, -2.7099e-04,\n",
       "         -9.7525e-04,  1.7267e-03,  6.1893e-04,  4.9329e-03, -2.0993e-03,\n",
       "          7.1367e-05], device='cuda:0'),\n",
       " 'edge_network.4.bias': tensor([-0.0019, -0.0023,  0.0005,  0.0059,  0.0027, -0.0080, -0.0011,  0.0057,\n",
       "          0.0023,  0.0017, -0.0012,  0.0057, -0.0002,  0.0026, -0.0146, -0.0006],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.6.weight': tensor([[-4.5433e-03, -2.6884e-03,  7.1066e-03,  5.8977e-03,  3.9526e-03,\n",
       "          -1.0119e-02, -2.4606e-03,  3.1832e-03, -8.6884e-03,  7.0238e-03,\n",
       "          -5.8872e-03,  8.8217e-03,  1.9374e-03, -6.6740e-03,  1.7866e-03,\n",
       "           5.2213e-03],\n",
       "         [-3.2390e-03, -1.2374e-03,  4.1853e-03,  2.8662e-03,  3.8929e-05,\n",
       "          -5.0569e-03, -1.0374e-03,  7.4943e-04, -5.0745e-03,  1.3967e-03,\n",
       "          -2.9983e-03,  2.8414e-03,  1.6330e-03, -8.8865e-04,  3.7607e-03,\n",
       "           3.0621e-03],\n",
       "         [ 7.9460e-03,  7.8731e-03, -8.6397e-03, -2.6224e-03, -1.0796e-02,\n",
       "           1.5700e-02,  8.1773e-03, -7.8243e-05,  1.1811e-02, -1.1575e-02,\n",
       "           2.1710e-03, -1.7551e-02, -9.8531e-03,  1.4594e-02, -4.9682e-03,\n",
       "          -2.3577e-03],\n",
       "         [ 3.0294e-03, -8.5539e-04, -1.0356e-02, -9.3885e-03,  1.1198e-02,\n",
       "           7.4556e-03, -2.6353e-03, -4.1968e-03,  8.3599e-03,  2.3133e-03,\n",
       "           1.2362e-02,  8.4528e-04,  1.1025e-03, -8.9379e-03, -7.6861e-03,\n",
       "          -8.6775e-03],\n",
       "         [ 5.7155e-03,  3.7693e-03, -2.7712e-03, -1.7578e-03, -1.0611e-02,\n",
       "           8.9796e-03,  4.7176e-03, -1.5452e-04,  7.1882e-03, -7.8671e-03,\n",
       "          -7.8299e-04, -9.7417e-03, -3.9289e-03,  1.1208e-02, -7.0152e-04,\n",
       "          -1.7966e-03],\n",
       "         [ 4.8372e-03,  2.4400e-03, -2.0357e-03,  1.6953e-03, -7.0973e-03,\n",
       "           5.8480e-03,  5.3497e-03,  1.3892e-03,  5.8141e-03, -4.6158e-03,\n",
       "          -2.9409e-03, -7.1547e-03, -5.8858e-03,  7.7139e-03, -3.0401e-03,\n",
       "           1.2851e-04],\n",
       "         [ 1.0949e-02,  4.9255e-03, -1.1308e-02, -2.0196e-03,  1.3138e-04,\n",
       "           1.6345e-02,  9.0468e-03,  1.1916e-03,  1.5826e-02, -4.4486e-03,\n",
       "           3.2017e-03, -1.2796e-02, -9.9701e-03,  5.6706e-03, -1.5106e-02,\n",
       "          -6.3415e-03],\n",
       "         [-3.9295e-03, -1.3024e-02,  1.1294e-02,  3.8105e-03,  7.9964e-03,\n",
       "          -1.3590e-02, -8.0676e-04,  1.9322e-03, -1.4051e-02,  1.0423e-02,\n",
       "          -5.8991e-03,  1.6157e-02,  8.0992e-03, -1.0967e-02,  6.6062e-03,\n",
       "           6.0286e-03],\n",
       "         [-1.3512e-02, -3.4312e-03,  1.1505e-02,  6.2904e-03,  5.7593e-03,\n",
       "          -1.9567e-02, -9.0788e-03,  1.5988e-03, -1.5844e-02,  9.8452e-03,\n",
       "          -5.6736e-03,  1.5679e-02,  8.3995e-03, -1.0476e-02,  8.8851e-03,\n",
       "           6.6491e-03],\n",
       "         [ 8.3012e-03, -1.3370e-03, -1.8543e-03,  3.2680e-03, -3.5896e-03,\n",
       "           5.8374e-03,  9.7149e-03,  3.4863e-03,  8.1104e-03, -4.5490e-04,\n",
       "          -6.3480e-03, -4.6522e-03, -9.4480e-03,  4.7222e-03, -6.8898e-03,\n",
       "           2.9559e-04],\n",
       "         [ 6.3999e-03,  6.5133e-03, -7.5281e-03, -2.6047e-03, -6.5808e-03,\n",
       "           1.2773e-02,  5.1385e-03,  9.1536e-05,  1.0134e-02, -7.8670e-03,\n",
       "           2.2999e-03, -1.3258e-02, -7.9049e-03,  9.8203e-03, -4.5545e-03,\n",
       "          -2.5925e-03],\n",
       "         [-2.7836e-03, -1.3691e-03,  8.5170e-03,  8.6061e-03,  4.9483e-04,\n",
       "          -1.1224e-02, -1.2185e-03,  4.9763e-03, -7.3223e-03,  6.0173e-03,\n",
       "          -1.0107e-02,  8.0926e-03, -2.9067e-04, -3.6216e-03,  7.3861e-04,\n",
       "           5.9041e-03],\n",
       "         [ 8.0989e-04, -6.9276e-03, -3.0038e-03, -6.8920e-03,  9.9012e-03,\n",
       "          -4.1713e-04, -8.5890e-04, -5.1677e-03,  2.1697e-03,  3.2572e-03,\n",
       "           6.9944e-03,  4.1825e-03,  7.9369e-03, -5.2490e-03, -1.7677e-04,\n",
       "          -8.3064e-03],\n",
       "         [-9.2276e-03,  3.3408e-03, -1.5278e-03, -6.9102e-03,  9.3304e-04,\n",
       "          -2.1921e-03, -1.0355e-02, -6.7004e-03, -5.9484e-03, -5.2392e-03,\n",
       "           1.0892e-02, -1.1835e-03,  1.0102e-02,  1.2522e-04,  1.1106e-02,\n",
       "          -2.4215e-03],\n",
       "         [-6.5430e-03,  4.6749e-03,  4.1572e-04, -2.5530e-03, -1.3283e-03,\n",
       "          -2.8110e-03, -9.7950e-03, -2.0845e-03, -4.8722e-03, -5.7958e-04,\n",
       "           5.2570e-03,  3.0507e-03,  2.5669e-03, -4.4592e-03,  4.9055e-03,\n",
       "           2.0633e-03],\n",
       "         [-4.2102e-03, -2.6668e-03,  6.0003e-03,  2.3141e-03, -4.0203e-04,\n",
       "          -7.9611e-03, -3.8982e-03, -2.1648e-04, -7.6131e-03,  2.3704e-03,\n",
       "          -2.5415e-03,  6.6672e-03,  5.5040e-03, -2.5808e-03,  5.3343e-03,\n",
       "           3.1411e-03]], device='cuda:0'),\n",
       " 'edge_network.6.bias': tensor([-0.0075, -0.0036,  0.0042,  0.0133,  0.0005, -0.0026,  0.0034, -0.0063,\n",
       "         -0.0079, -0.0061,  0.0037, -0.0125,  0.0075,  0.0118,  0.0052, -0.0031],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.7.weight': tensor([ 8.1443e-05, -4.8358e-04,  2.7098e-03,  4.8567e-03,  1.6428e-03,\n",
       "         -2.8053e-04,  4.4395e-03, -6.1429e-04,  3.1465e-04,  7.3715e-04,\n",
       "          4.5183e-04,  2.0222e-04,  3.9616e-03,  3.2288e-04,  3.8758e-03,\n",
       "          9.2059e-04], device='cuda:0'),\n",
       " 'edge_network.7.bias': tensor([-6.5642e-04, -4.6778e-04,  1.5609e-03,  4.3609e-03,  1.3484e-03,\n",
       "         -1.0709e-05,  1.7880e-03, -1.5346e-04, -1.2183e-03,  1.7734e-04,\n",
       "          1.4563e-03, -3.2885e-04,  3.1995e-03,  4.0786e-03,  2.8582e-03,\n",
       "          1.0678e-03], device='cuda:0'),\n",
       " 'edge_network.9.weight': tensor([[-0.0048,  0.0218,  0.0292, -0.0422,  0.0332, -0.0024,  0.0554,  0.0173,\n",
       "           0.0013,  0.0027,  0.0115, -0.0032, -0.0561, -0.0136, -0.0512, -0.0029]],\n",
       "        device='cuda:0'),\n",
       " 'edge_network.9.bias': tensor([-0.0019], device='cuda:0'),\n",
       " 'node_network.0.weight': tensor([[ 3.6475e-03,  1.5523e-03,  7.3013e-03,  3.9050e-03,  3.5898e-03,\n",
       "          -2.5764e-03, -7.5666e-04, -7.0515e-03, -5.3828e-03, -2.4456e-03,\n",
       "          -5.5435e-03,  3.8273e-03,  4.4545e-03,  5.3798e-03, -6.8196e-04,\n",
       "          -3.5125e-03,  1.3510e-03, -6.6318e-03, -7.7556e-03,  9.4945e-03,\n",
       "           8.1018e-03,  1.2600e-02,  5.6615e-03,  4.0985e-03, -2.1961e-03,\n",
       "          -1.0105e-03, -1.5192e-02, -1.1702e-02, -2.4541e-03, -1.7681e-02,\n",
       "           1.0019e-02,  1.3270e-02,  8.1337e-03, -4.5094e-03, -3.5887e-03,\n",
       "           3.6703e-03, -1.0088e-02, -7.8292e-03,  9.8619e-03,  7.4940e-03,\n",
       "           1.2303e-02,  5.7991e-03,  3.0629e-03, -3.1252e-03, -8.0549e-04,\n",
       "          -1.6063e-02, -1.1063e-02, -3.4535e-05, -1.6161e-02,  1.1016e-02,\n",
       "           1.3878e-02,  7.9697e-03, -2.6466e-04, -3.2182e-03,  3.0028e-03,\n",
       "          -1.1597e-02, -7.5886e-03],\n",
       "         [-1.7322e-04, -4.5071e-04,  1.3165e-04,  4.8802e-04,  3.9492e-04,\n",
       "          -7.7600e-04, -1.9113e-04, -1.2148e-04, -1.1783e-05, -8.1522e-05,\n",
       "           1.5148e-03, -2.7350e-04, -2.9385e-04,  4.6535e-04,  5.7504e-04,\n",
       "          -3.4393e-04, -4.3443e-04, -1.0137e-03, -2.0672e-03, -2.3643e-03,\n",
       "          -5.8882e-03, -2.0946e-03,  1.3696e-03,  1.8877e-03, -3.6217e-03,\n",
       "          -1.3784e-03,  5.5842e-03,  3.8221e-03, -6.0046e-04,  1.1779e-02,\n",
       "          -4.8357e-03, -5.2201e-03, -2.5133e-03,  3.1563e-03, -2.2346e-03,\n",
       "          -1.5925e-03, -2.5581e-03, -3.5866e-03, -9.5863e-04, -4.3087e-03,\n",
       "          -1.1502e-03,  8.2035e-04,  6.9885e-04, -2.9852e-03, -1.4384e-03,\n",
       "           2.6269e-03,  1.5503e-03, -8.6270e-04,  8.1956e-03, -2.3920e-03,\n",
       "          -3.7839e-03, -8.7744e-04,  1.6277e-03, -1.6699e-03, -1.0147e-03,\n",
       "           1.2861e-03, -5.7760e-03],\n",
       "         [-1.2332e-03,  2.9200e-04,  9.3593e-04,  1.3517e-03,  9.2799e-04,\n",
       "           3.1245e-04,  8.5201e-04, -3.9170e-04,  4.3820e-04, -1.0587e-03,\n",
       "          -1.8458e-03, -3.3454e-04,  5.0943e-04,  6.8351e-04, -4.6893e-04,\n",
       "           1.7329e-04,  8.8723e-04, -1.4909e-03,  4.4526e-04,  2.0449e-03,\n",
       "           4.8859e-03,  5.1645e-03,  3.7980e-03,  1.5037e-03,  3.2425e-04,\n",
       "           1.8774e-03, -6.1669e-03, -3.9170e-03, -1.2278e-03, -1.5533e-02,\n",
       "           4.5203e-03,  6.5549e-03,  2.4351e-03, -3.6350e-03,  8.6856e-05,\n",
       "           7.4713e-03, -2.4917e-03,  2.5689e-03,  7.4072e-05,  3.1960e-03,\n",
       "           3.2631e-03,  3.6799e-03,  2.4522e-03, -6.6549e-04,  1.8400e-03,\n",
       "          -4.0341e-03, -2.2701e-03, -5.4900e-04, -1.0375e-02,  2.9511e-03,\n",
       "           3.8273e-03,  1.8002e-03, -9.7356e-04, -3.0985e-04,  5.2482e-03,\n",
       "          -6.4227e-03,  1.6466e-03],\n",
       "         [-9.5542e-04, -1.3053e-03, -6.5204e-04, -8.0452e-04, -3.4140e-04,\n",
       "          -3.0601e-04, -2.7583e-04,  5.8369e-04,  5.6618e-04, -1.5237e-04,\n",
       "           2.8518e-03, -4.4202e-04, -1.6710e-03, -2.0001e-04,  6.5207e-04,\n",
       "          -6.7533e-05, -1.1324e-03,  1.8603e-03, -2.7010e-03, -1.1047e-02,\n",
       "          -1.2154e-02, -1.0166e-02, -4.8705e-03,  1.1201e-03, -2.0026e-03,\n",
       "          -1.5468e-03,  1.3139e-02,  1.0749e-02,  6.1950e-04,  3.2460e-02,\n",
       "          -1.1524e-02, -1.6646e-02, -4.3869e-03,  8.4897e-03, -5.9807e-04,\n",
       "          -1.5075e-02,  1.0956e-04, -9.2205e-03, -1.0852e-02, -9.7965e-03,\n",
       "          -1.0393e-02, -3.6232e-03,  1.6951e-03, -1.7048e-03, -3.3609e-04,\n",
       "           1.2045e-02,  9.8299e-03,  2.9712e-05,  2.6611e-02, -1.0941e-02,\n",
       "          -1.6223e-02, -3.8404e-03,  5.8162e-03, -3.1983e-04, -1.4265e-02,\n",
       "          -4.0789e-03, -4.4578e-03],\n",
       "         [-2.1908e-03, -1.4142e-03, -3.7632e-03, -2.1414e-03, -1.7311e-03,\n",
       "           7.1464e-04, -1.3240e-04,  3.3372e-03,  2.3656e-03,  9.9402e-04,\n",
       "           4.4580e-03, -1.8079e-03, -2.8388e-03, -1.9425e-03,  6.5865e-04,\n",
       "           1.6515e-03, -6.8063e-04,  4.2988e-03, -9.3180e-06, -1.4891e-03,\n",
       "          -2.5709e-04, -4.9381e-04, -1.7855e-03, -1.6773e-03, -5.7535e-04,\n",
       "          -4.8271e-04,  7.3866e-04, -3.7705e-04,  2.1245e-04, -5.8583e-04,\n",
       "           1.8619e-03, -1.1358e-03,  1.9162e-04,  8.5093e-04, -4.9740e-05,\n",
       "           4.3936e-03,  5.2487e-03, -4.7821e-03, -3.8309e-04, -3.1387e-04,\n",
       "          -3.2325e-04, -3.6452e-03, -2.4132e-03,  5.2000e-04, -1.6154e-03,\n",
       "           5.4243e-04, -1.2846e-03, -3.3384e-04, -1.4227e-04,  2.2787e-03,\n",
       "          -8.7684e-04,  2.0392e-05, -9.6746e-04,  3.4106e-04,  4.9469e-03,\n",
       "           1.2043e-02, -8.3538e-03],\n",
       "         [-4.3410e-03, -1.1364e-03, -7.9533e-03, -3.0456e-03, -2.4221e-03,\n",
       "           2.5981e-03,  8.3676e-04,  7.2978e-03,  5.8796e-03,  3.2384e-03,\n",
       "           5.5098e-03, -4.5541e-03, -4.5638e-03, -5.0005e-03,  1.5683e-03,\n",
       "           3.5288e-03, -1.3657e-03, -1.7959e-04,  9.9362e-03, -5.0742e-03,\n",
       "          -2.6796e-03, -7.4430e-03, -2.3658e-03, -2.1440e-03,  2.7939e-03,\n",
       "           1.0283e-03,  8.8022e-03,  7.4561e-03,  2.4274e-03,  5.3602e-03,\n",
       "          -6.7212e-03, -5.3077e-03, -5.4396e-03,  1.3035e-03,  3.0858e-03,\n",
       "           1.3665e-03, -4.1822e-04,  1.2285e-02, -5.5435e-03, -3.5458e-03,\n",
       "          -8.8618e-03, -3.5081e-03, -2.2616e-03,  2.8892e-03,  4.7819e-04,\n",
       "           1.0711e-02,  7.5938e-03,  1.7018e-03,  7.3444e-03, -7.9359e-03,\n",
       "          -7.4946e-03, -6.2185e-03, -5.7573e-04,  2.6591e-03,  8.7809e-04,\n",
       "           2.7355e-03,  1.2015e-02],\n",
       "         [ 1.0254e-03,  2.5894e-03,  3.9532e-03,  3.8006e-03,  1.4451e-03,\n",
       "           9.3901e-04,  2.2540e-03, -3.4970e-03, -8.5369e-04, -1.9970e-03,\n",
       "          -7.4965e-03,  2.2886e-04,  4.4811e-03,  1.8301e-03, -2.3876e-03,\n",
       "           1.7959e-04,  4.1873e-04, -4.2899e-03,  5.2476e-03, -2.2124e-03,\n",
       "           5.5767e-03,  1.1671e-03,  2.4392e-03,  1.8822e-03,  6.0013e-03,\n",
       "           4.2876e-03, -2.7013e-03,  2.9316e-03, -2.1884e-03, -2.2586e-03,\n",
       "          -4.4297e-03,  5.1475e-03,  1.5617e-03, -3.8718e-03,  4.3709e-03,\n",
       "          -1.1509e-02, -9.2677e-03,  8.4799e-03, -2.3442e-03,  4.8839e-03,\n",
       "           1.3518e-03,  2.9169e-03,  2.2559e-03,  4.1763e-03,  4.0392e-03,\n",
       "          -1.7779e-03,  3.0020e-03, -1.6659e-03, -1.2457e-03, -4.1953e-03,\n",
       "           4.4369e-03,  1.8592e-03, -1.5549e-03,  2.7385e-03, -1.1430e-02,\n",
       "          -1.3654e-02,  1.0230e-02],\n",
       "         [-1.3122e-03, -1.2669e-03, -1.0968e-03, -1.4784e-04,  5.0229e-04,\n",
       "          -2.5311e-05,  4.0829e-05,  1.6427e-03,  1.2197e-03, -3.1844e-04,\n",
       "           2.4161e-03, -1.5063e-03, -2.0558e-03, -3.5964e-04,  5.0891e-04,\n",
       "          -2.0124e-04, -4.5195e-05,  3.9544e-04, -1.2958e-03, -2.1575e-03,\n",
       "          -2.2979e-03, -2.3017e-03,  6.9008e-04, -7.1196e-05,  5.3631e-04,\n",
       "           8.7405e-04,  2.8517e-03,  2.6799e-03, -1.2180e-03,  2.8663e-03,\n",
       "          -2.5930e-03, -2.8912e-03, -9.4196e-04, -1.5403e-05,  7.5845e-04,\n",
       "           1.8790e-03,  5.4231e-03, -2.4167e-03, -2.1529e-03, -1.3909e-03,\n",
       "          -2.7317e-03,  6.4424e-04,  4.5813e-05,  9.9278e-04,  1.0844e-03,\n",
       "           2.7803e-03,  2.4209e-03, -1.1299e-03,  8.4388e-04, -2.5453e-03,\n",
       "          -3.1704e-03, -1.0336e-03, -1.0992e-03,  5.8877e-04,  2.3047e-03,\n",
       "           4.5423e-03, -7.4374e-05],\n",
       "         [ 2.4050e-03,  3.7396e-03,  4.6705e-03,  3.0821e-03, -2.0085e-04,\n",
       "           1.2661e-03,  1.7610e-03, -4.3338e-03, -1.7812e-03, -1.6028e-03,\n",
       "          -9.7216e-03,  1.4297e-03,  6.6953e-03,  1.5501e-03, -3.6570e-03,\n",
       "           1.2472e-03,  8.8896e-04, -1.7934e-03,  6.7920e-03,  2.3658e-03,\n",
       "           5.6516e-03,  4.9869e-03,  1.9908e-03,  9.0625e-04,  2.4209e-03,\n",
       "           1.9425e-03, -4.8593e-03, -1.2454e-03, -1.2741e-03, -8.6279e-03,\n",
       "           3.5242e-04,  8.0472e-03,  5.6422e-04, -4.4169e-03,  1.7029e-03,\n",
       "          -5.7005e-03, -5.4349e-03,  1.0740e-02,  5.1599e-03,  6.0478e-03,\n",
       "           6.5487e-03,  1.1105e-03, -4.4818e-04,  1.8758e-03,  9.0385e-04,\n",
       "          -6.5811e-03, -2.7937e-03,  8.1249e-05, -8.9103e-03,  2.3489e-03,\n",
       "           1.0600e-02,  7.8748e-04, -2.9851e-03,  1.3441e-03, -6.4085e-03,\n",
       "          -4.7285e-03,  1.1653e-02],\n",
       "         [-4.0359e-03, -1.4744e-03, -7.5598e-03, -3.9642e-03, -2.1528e-03,\n",
       "           1.9216e-03, -2.0632e-04,  6.8077e-03,  4.6010e-03,  2.1621e-03,\n",
       "           7.8096e-03, -3.8986e-03, -5.1551e-03, -4.0121e-03,  6.8008e-04,\n",
       "           2.7796e-03, -1.8358e-03,  5.0596e-03,  2.0320e-03, -6.3302e-03,\n",
       "          -4.6137e-03, -8.4915e-03, -3.1485e-03, -2.6110e-03,  2.5200e-03,\n",
       "           2.9366e-05,  1.0340e-02,  8.0215e-03,  8.0029e-04,  1.1140e-02,\n",
       "          -7.2756e-03, -6.7839e-03, -5.0188e-03,  8.3210e-04,  4.0577e-03,\n",
       "           1.6409e-03,  9.1131e-03,  3.2346e-04, -6.9636e-03, -4.7679e-03,\n",
       "          -7.3624e-03, -2.9441e-03, -1.2776e-03,  2.7704e-03, -3.2970e-05,\n",
       "           9.3605e-03,  6.8235e-03, -1.6016e-03,  9.9629e-03, -6.9811e-03,\n",
       "          -7.6833e-03, -3.6743e-03, -1.8122e-03,  3.5533e-03,  1.8720e-03,\n",
       "           1.2500e-02, -4.4227e-03],\n",
       "         [ 1.7790e-04, -9.6063e-04, -2.1301e-03, -3.2092e-03, -9.9403e-04,\n",
       "          -8.4556e-04, -1.9197e-03,  1.4385e-03, -7.6426e-04,  2.1525e-03,\n",
       "           2.4478e-03,  2.1493e-03, -2.6542e-03, -1.0654e-03,  2.3519e-03,\n",
       "          -9.7950e-04,  1.3250e-03,  2.7946e-03, -3.8700e-03,  1.0331e-02,\n",
       "           7.6415e-03,  9.7839e-03,  1.5775e-03, -2.0010e-03, -5.0098e-03,\n",
       "          -1.8909e-03, -1.2673e-02, -1.5306e-02,  2.5386e-03, -3.2763e-02,\n",
       "           1.9001e-02,  9.7694e-03,  5.3602e-03, -1.6100e-03, -5.3274e-03,\n",
       "           2.5133e-02,  2.3024e-03, -7.5655e-04,  8.3274e-03,  4.3019e-03,\n",
       "           8.7391e-03,  2.5485e-03, -8.4557e-04, -4.7179e-03, -1.4352e-03,\n",
       "          -9.0923e-03, -1.1709e-02,  1.9879e-03, -2.6133e-02,  1.5035e-02,\n",
       "           8.3264e-03,  3.0261e-03,  8.5446e-05, -4.4080e-03,  2.4130e-02,\n",
       "           3.6284e-03, -1.3095e-03],\n",
       "         [ 5.7806e-03,  2.0089e-03,  3.7519e-03, -2.6785e-04, -1.0076e-03,\n",
       "          -2.5431e-04, -1.0461e-03, -4.0482e-03, -3.9291e-03,  5.2185e-04,\n",
       "          -5.5013e-03,  3.5338e-03,  4.5708e-03,  5.4309e-04, -1.4513e-03,\n",
       "          -1.0983e-03,  1.5368e-04,  1.0974e-03,  3.0809e-03,  5.0471e-03,\n",
       "          -2.0404e-03, -2.1656e-03, -4.4305e-03, -2.6000e-03,  5.4122e-04,\n",
       "          -2.0778e-03,  9.2252e-05, -9.6918e-04,  2.9839e-03,  7.6929e-03,\n",
       "          -7.7362e-04, -1.6944e-03, -1.9078e-03,  1.2333e-03, -1.3826e-04,\n",
       "          -1.1292e-02,  2.7877e-03,  7.4925e-03,  6.2730e-03, -6.3301e-04,\n",
       "          -9.0080e-04, -3.5693e-03, -2.8388e-03,  6.0019e-04, -1.7509e-03,\n",
       "           5.7716e-04, -5.4039e-04,  3.8209e-03,  5.4044e-03, -1.0611e-03,\n",
       "           1.0623e-03, -2.7890e-03,  1.5914e-03, -5.0267e-04, -1.0223e-02,\n",
       "          -2.5254e-03,  1.4047e-02],\n",
       "         [ 1.8300e-03, -6.5745e-04,  1.4361e-03, -7.8429e-04,  6.8374e-05,\n",
       "          -1.5521e-03, -1.4561e-03, -1.2724e-03, -2.0520e-03, -2.4998e-04,\n",
       "           1.2779e-03,  1.8072e-03, -1.8838e-04,  8.2888e-04,  3.2248e-04,\n",
       "          -1.8580e-03, -9.3258e-05,  2.2435e-03, -5.7367e-03,  7.0994e-05,\n",
       "          -3.9857e-03, -8.3900e-04, -1.2383e-03, -1.0383e-03, -5.5546e-04,\n",
       "          -1.4992e-03,  2.7478e-03,  1.2000e-03, -5.9734e-04,  7.6005e-03,\n",
       "          -2.3504e-03, -3.1144e-03, -6.2873e-05,  1.0400e-03, -7.0673e-04,\n",
       "          -2.6674e-03,  4.3247e-03, -2.0188e-03,  5.2711e-04, -2.5165e-03,\n",
       "           1.3434e-03,  3.7707e-04, -2.8527e-04, -3.0485e-04, -6.5743e-04,\n",
       "           3.3040e-04,  1.0203e-04, -1.6262e-03,  3.0194e-03, -8.7461e-04,\n",
       "          -7.6141e-04,  9.1842e-04, -2.1776e-04, -4.5447e-04, -7.5201e-04,\n",
       "           3.6667e-03, -3.3358e-03],\n",
       "         [ 1.0707e-03,  7.8375e-04,  2.0073e-03,  1.2895e-03,  5.1734e-04,\n",
       "          -4.3258e-05,  3.7500e-04, -1.6043e-03, -1.1134e-03, -7.3375e-04,\n",
       "          -3.4732e-03,  1.0454e-03,  1.6821e-03,  7.7692e-04, -7.3973e-04,\n",
       "          -4.4418e-04,  1.1391e-03, -6.2939e-04,  1.3057e-03,  3.9299e-03,\n",
       "           4.4321e-03,  2.8568e-03,  1.8330e-03, -2.3305e-04, -4.4703e-04,\n",
       "           9.7516e-04, -4.8200e-03, -4.5751e-03,  4.8383e-04, -1.2710e-02,\n",
       "           5.9274e-03,  4.5739e-03,  6.3911e-04, -2.0667e-03, -4.2417e-04,\n",
       "           4.9542e-03,  8.3935e-04,  3.9793e-03,  4.0032e-03,  3.9337e-03,\n",
       "           2.6529e-03,  1.1924e-03, -5.4838e-04, -6.3172e-04,  3.9217e-04,\n",
       "          -3.8637e-03, -3.9411e-03,  1.1459e-03, -1.0398e-02,  5.4863e-03,\n",
       "           4.5521e-03,  1.8174e-05, -8.6276e-04, -6.6996e-04,  4.4357e-03,\n",
       "           3.3747e-04,  4.1370e-03],\n",
       "         [-2.9320e-03, -1.6312e-03, -2.0258e-03, -2.0761e-04,  7.6321e-04,\n",
       "          -2.5655e-04,  9.1678e-05,  2.2248e-03,  2.0217e-03,  2.9339e-04,\n",
       "           4.9512e-03, -1.8244e-03, -2.9298e-03, -9.5779e-05,  2.1407e-03,\n",
       "           5.9850e-05, -6.1256e-04, -2.8004e-03, -3.2959e-03, -3.9256e-03,\n",
       "          -1.8735e-03, -1.1784e-03, -1.3163e-05,  1.5530e-03, -5.5865e-04,\n",
       "          -4.1172e-04,  2.9047e-03,  2.2503e-03, -7.2065e-04,  7.1367e-03,\n",
       "          -1.9505e-03, -3.3911e-03,  9.2332e-04,  2.6983e-03, -1.1752e-03,\n",
       "           7.2999e-04, -3.6791e-03, -1.0907e-02, -5.1965e-03, -2.1997e-03,\n",
       "          -3.1892e-03, -7.2070e-04,  5.5574e-04,  8.0214e-04, -1.9073e-04,\n",
       "           3.0999e-03,  3.0811e-03, -7.4714e-04,  8.2570e-03, -3.0160e-03,\n",
       "          -4.9439e-03,  1.2761e-03,  1.7014e-03,  5.7561e-04, -1.2625e-04,\n",
       "          -6.4107e-04, -1.1636e-02],\n",
       "         [ 1.2367e-03, -6.6879e-04,  9.9316e-04,  6.5566e-04,  6.4079e-04,\n",
       "          -1.1164e-03, -2.2717e-04, -1.0121e-03, -1.2035e-03, -7.2225e-04,\n",
       "           3.4490e-04,  6.1981e-04, -4.2522e-05,  6.1805e-04, -7.1587e-05,\n",
       "          -1.1146e-03,  3.6217e-05,  1.0795e-03, -2.1082e-03,  1.3162e-03,\n",
       "          -4.9997e-04, -1.3856e-03, -1.5074e-03, -5.7534e-04, -1.7124e-04,\n",
       "          -7.1646e-04, -7.8831e-04, -1.0191e-03,  2.1485e-04,  4.1254e-03,\n",
       "           7.7146e-04, -1.1776e-03,  4.6213e-04,  5.2118e-04,  1.8015e-04,\n",
       "          -3.4025e-03,  3.7889e-03, -4.3515e-03,  1.6746e-04, -3.8450e-04,\n",
       "          -1.2891e-03, -1.0783e-03,  1.5206e-04, -4.9161e-04, -4.7527e-04,\n",
       "          -6.6086e-04, -8.0178e-04, -2.1677e-04,  3.7272e-03,  8.2598e-04,\n",
       "          -1.7461e-03,  7.5763e-04,  4.9129e-04, -2.4756e-04, -2.5997e-03,\n",
       "           2.9089e-03, -6.7743e-03]], device='cuda:0'),\n",
       " 'node_network.0.bias': tensor([ 0.0053, -0.0003,  0.0017, -0.0014, -0.0032, -0.0053,  0.0057, -0.0006,\n",
       "          0.0056, -0.0060, -0.0037,  0.0018, -0.0006,  0.0024, -0.0022,  0.0008],\n",
       "        device='cuda:0'),\n",
       " 'node_network.1.weight': tensor([ 9.8672e-04,  9.2226e-04,  8.3651e-04, -1.1705e-03,  1.7172e-05,\n",
       "         -9.1584e-04,  8.7258e-05,  7.1084e-04,  1.6701e-04, -2.5302e-03,\n",
       "          1.9923e-03,  6.8224e-04, -3.7284e-05, -1.2182e-03,  1.9553e-03,\n",
       "         -3.6045e-04], device='cuda:0'),\n",
       " 'node_network.1.bias': tensor([ 0.0058, -0.0018,  0.0024, -0.0067, -0.0016, -0.0043,  0.0013, -0.0015,\n",
       "          0.0033, -0.0050,  0.0042, -0.0007, -0.0006,  0.0024, -0.0030, -0.0009],\n",
       "        device='cuda:0'),\n",
       " 'node_network.3.weight': tensor([[ 3.6743e-03,  2.1622e-03,  1.4985e-04, -4.1238e-03, -2.9466e-03,\n",
       "           4.3805e-03,  1.3188e-03, -1.8000e-03, -7.9538e-04,  4.3338e-03,\n",
       "           1.9772e-03, -5.2784e-03, -3.0620e-03, -1.0103e-05, -1.1691e-03,\n",
       "           3.3132e-04],\n",
       "         [-7.4324e-05,  1.0292e-03, -1.1225e-04, -1.2613e-03, -1.3261e-03,\n",
       "           3.0672e-04,  1.4974e-03, -2.4236e-04, -2.0411e-04, -3.1092e-04,\n",
       "           1.2069e-03, -1.3444e-03, -1.9869e-03,  1.0561e-03, -3.2126e-04,\n",
       "           3.1062e-04],\n",
       "         [ 2.7810e-03, -1.8301e-04, -1.4029e-03, -2.5484e-03, -1.7583e-03,\n",
       "           2.7707e-03,  1.6947e-03, -5.8867e-04, -1.0373e-03,  2.1036e-03,\n",
       "           4.5657e-04, -2.4111e-03, -1.2497e-03,  1.0803e-03, -1.8549e-03,\n",
       "           7.8964e-04],\n",
       "         [-1.7002e-03,  3.4594e-03,  4.6410e-04, -2.0939e-03,  5.9387e-04,\n",
       "          -7.4877e-03,  1.1469e-03,  6.6326e-03,  5.2348e-03, -4.8310e-03,\n",
       "          -4.9062e-03,  3.4266e-04, -3.2736e-03,  1.0884e-03,  6.8267e-03,\n",
       "          -6.6988e-04],\n",
       "         [ 5.2150e-03,  9.2613e-03, -6.7796e-03, -4.2045e-03, -1.4866e-03,\n",
       "           3.8093e-03, -2.6072e-03,  1.8699e-03, -4.7217e-03,  2.9739e-03,\n",
       "           1.2061e-02, -6.3000e-03, -7.1670e-03,  3.5657e-03,  1.6896e-03,\n",
       "          -6.8789e-03],\n",
       "         [-1.1644e-03, -1.9455e-04, -5.4302e-03, -2.2070e-03, -2.8224e-04,\n",
       "          -8.9671e-05,  5.6762e-03, -7.2162e-04,  2.3412e-03, -3.7766e-03,\n",
       "           1.1878e-03, -3.4136e-03, -1.2523e-03,  1.1197e-03,  2.2904e-03,\n",
       "           4.6905e-03],\n",
       "         [-4.5404e-03, -1.0362e-03,  1.8257e-03,  4.2365e-03,  3.7185e-03,\n",
       "          -5.9313e-03, -8.5122e-04,  2.4013e-03,  2.4077e-03, -3.0379e-03,\n",
       "          -3.6018e-03,  2.3414e-03, -6.2800e-04,  1.3256e-03,  3.4947e-03,\n",
       "           1.2773e-03],\n",
       "         [-5.1427e-03,  1.1855e-02, -1.1031e-02, -8.5181e-03,  6.0379e-03,\n",
       "          -1.4684e-02,  2.9043e-03,  1.3821e-02,  6.8320e-03, -1.4068e-02,\n",
       "           5.7639e-03, -3.0934e-03, -1.2923e-02,  1.0854e-02,  1.7413e-02,\n",
       "          -8.7009e-04],\n",
       "         [-4.4166e-03, -1.0943e-02,  4.6833e-03,  1.0532e-03,  6.3086e-04,\n",
       "           4.1048e-03,  2.4701e-03, -1.1309e-02,  3.3277e-03,  3.3888e-03,\n",
       "          -5.3070e-03,  1.0329e-03,  1.2046e-02, -2.6706e-03, -4.1796e-03,\n",
       "           9.6546e-03],\n",
       "         [ 1.0015e-02,  2.7275e-03,  2.0494e-03,  2.8812e-03, -4.4010e-03,\n",
       "           1.1346e-02, -7.0151e-03, -1.6359e-03, -1.1132e-02,  8.9064e-03,\n",
       "           5.6023e-03, -4.4637e-04, -6.4456e-04, -6.8793e-03, -1.1020e-02,\n",
       "          -8.6991e-03],\n",
       "         [-2.4516e-03, -6.1230e-03,  2.5912e-03,  6.1470e-03,  1.8555e-03,\n",
       "          -4.7477e-04, -1.5096e-03, -3.5582e-03, -5.4945e-04, -7.7330e-05,\n",
       "          -3.5606e-03,  5.4773e-03,  6.0444e-03, -3.3535e-03, -2.6278e-03,\n",
       "           1.2832e-03],\n",
       "         [-2.8554e-03,  2.8332e-03, -3.3185e-03, -1.9516e-03,  4.4711e-03,\n",
       "          -7.1034e-03,  9.0646e-04,  7.7590e-03,  3.4710e-03, -7.9666e-03,\n",
       "           1.9753e-03, -1.9960e-03, -5.9136e-03,  3.7665e-03,  7.7624e-03,\n",
       "           1.3819e-05],\n",
       "         [-3.3578e-03, -5.5978e-04,  7.2306e-04,  3.3619e-03,  8.9950e-04,\n",
       "          -5.2509e-03,  5.7991e-04,  2.9989e-03,  2.0825e-03, -3.1400e-03,\n",
       "          -2.6430e-03,  3.5678e-03, -1.5840e-05,  1.1757e-03,  2.6154e-03,\n",
       "           6.5758e-06],\n",
       "         [-1.8047e-03,  2.5412e-04, -2.6744e-04,  1.5576e-04, -1.8097e-04,\n",
       "          -3.3341e-03,  1.6679e-03, -2.4689e-03,  2.1465e-03, -1.8944e-03,\n",
       "          -1.9344e-03,  1.6275e-03,  3.1171e-03,  7.0468e-04,  1.7282e-03,\n",
       "           2.1185e-03],\n",
       "         [ 1.0305e-02, -2.5620e-03,  6.2501e-03,  2.4072e-03, -6.3156e-03,\n",
       "           1.4943e-02, -5.3367e-03, -4.9108e-03, -7.8641e-03,  1.4012e-02,\n",
       "           1.5433e-03, -1.5244e-03,  3.2635e-03, -8.8197e-03, -1.3122e-02,\n",
       "          -5.5595e-03],\n",
       "         [-4.4826e-03, -1.1980e-02,  9.6048e-03,  6.6660e-03,  4.9026e-04,\n",
       "           2.6948e-03, -2.5427e-03, -8.2468e-03, -1.5389e-03,  3.3850e-03,\n",
       "          -9.8211e-03,  1.1418e-02,  1.3645e-02, -4.0031e-03, -9.5254e-03,\n",
       "           2.2014e-03]], device='cuda:0'),\n",
       " 'node_network.3.bias': tensor([ 0.0020,  0.0001,  0.0012, -0.0070, -0.0037, -0.0041, -0.0052, -0.0250,\n",
       "          0.0088,  0.0125,  0.0035, -0.0086, -0.0052, -0.0015,  0.0178,  0.0144],\n",
       "        device='cuda:0'),\n",
       " 'node_network.4.weight': tensor([ 3.1948e-04, -2.6858e-04, -4.2460e-04,  1.1070e-03, -3.6392e-04,\n",
       "          3.1167e-04, -1.5125e-03, -5.4937e-04,  3.0528e-03, -8.5846e-04,\n",
       "         -2.3143e-03, -2.0569e-05, -1.2004e-03,  2.1001e-03,  1.0367e-03,\n",
       "         -1.8564e-03], device='cuda:0'),\n",
       " 'node_network.4.bias': tensor([ 0.0007,  0.0003,  0.0003, -0.0030, -0.0010, -0.0016, -0.0019, -0.0086,\n",
       "          0.0024,  0.0056,  0.0014, -0.0029, -0.0022, -0.0008,  0.0067,  0.0047],\n",
       "        device='cuda:0'),\n",
       " 'node_network.6.weight': tensor([[ 8.1942e-03,  8.6568e-03,  8.3785e-03, -6.2643e-03, -9.0764e-04,\n",
       "          -9.7120e-03, -5.7708e-03, -6.6371e-03,  7.3435e-03, -1.5736e-02,\n",
       "          -1.1671e-02,  3.9052e-03,  1.8134e-02,  1.2063e-02, -3.8847e-03,\n",
       "          -9.6602e-03],\n",
       "         [ 1.1158e-03,  6.6882e-03, -1.7273e-03, -7.6199e-03, -7.7303e-03,\n",
       "           1.2767e-02,  1.3269e-02,  7.5559e-03, -8.7571e-03,  1.4066e-02,\n",
       "           1.2887e-04, -1.0741e-02, -3.8176e-03, -1.5632e-02,  4.1775e-03,\n",
       "          -5.6340e-03],\n",
       "         [-7.9776e-04, -8.4297e-03,  5.9570e-04,  4.4320e-03,  7.6958e-03,\n",
       "          -8.2951e-03, -7.8030e-03, -3.7014e-03,  3.7461e-03, -1.0943e-02,\n",
       "           2.4761e-03,  7.0233e-03, -1.6439e-04,  8.8647e-03, -2.4057e-03,\n",
       "           7.1552e-03],\n",
       "         [ 7.4458e-03,  1.1148e-02,  2.2894e-03, -7.0994e-03, -4.9871e-03,\n",
       "           5.2621e-04,  2.9884e-03,  2.8268e-03,  5.4486e-04, -3.7902e-03,\n",
       "          -9.2744e-03, -4.2087e-03,  9.9047e-03, -1.8407e-03,  2.5573e-03,\n",
       "          -1.0602e-02],\n",
       "         [-5.0254e-03, -8.3552e-03, -1.6010e-03,  7.4146e-03,  4.7860e-03,\n",
       "          -3.8197e-03, -5.3119e-03, -3.8177e-03,  1.7024e-03, -4.4105e-04,\n",
       "           6.3501e-03,  5.1957e-03, -5.4528e-03,  6.7680e-03, -2.4166e-03,\n",
       "           9.5074e-03],\n",
       "         [ 4.2516e-03,  7.6921e-03,  8.2052e-04, -6.1368e-03, -4.3139e-03,\n",
       "           2.5087e-03,  2.1122e-03,  5.0153e-04,  1.4223e-03,  6.4638e-04,\n",
       "          -5.2509e-03, -1.1486e-03,  6.4023e-03, -2.3283e-03,  2.5277e-04,\n",
       "          -8.8964e-03],\n",
       "         [ 1.5657e-03,  4.5132e-04,  9.5768e-04,  1.7021e-03,  1.4619e-04,\n",
       "          -9.2926e-04,  1.2448e-04,  3.7808e-04, -1.2476e-06, -4.3545e-03,\n",
       "          -9.5927e-04, -1.2421e-03,  1.9012e-03, -9.7571e-04,  8.6664e-04,\n",
       "           6.2786e-05],\n",
       "         [-1.4535e-03,  1.6745e-03,  1.5893e-03,  2.2305e-03, -1.0307e-03,\n",
       "           3.5481e-03,  3.2212e-03,  3.0177e-03, -4.6823e-03, -8.5942e-06,\n",
       "          -1.5542e-03, -4.0744e-03, -1.5537e-03, -4.4395e-03,  3.4116e-03,\n",
       "           2.6255e-03],\n",
       "         [-1.8551e-03, -2.1165e-03, -3.4516e-03, -4.1440e-03, -7.0813e-04,\n",
       "           1.7823e-03, -3.5043e-04, -1.1223e-03,  1.4133e-03,  7.8433e-03,\n",
       "           4.4616e-03,  4.2034e-03, -4.5160e-03,  1.8349e-03, -3.1663e-03,\n",
       "          -1.6265e-03],\n",
       "         [-1.2396e-02, -1.4612e-02, -6.3753e-03,  2.6042e-03,  2.4177e-03,\n",
       "           3.1998e-03,  3.4848e-03,  3.1284e-04, -4.9854e-03,  1.8218e-02,\n",
       "           1.8311e-02,  3.3323e-03, -2.1501e-02,  2.5560e-04, -5.3055e-03,\n",
       "           1.4491e-02],\n",
       "         [ 3.1800e-03,  6.0156e-03, -4.4475e-04, -3.2659e-03, -2.1236e-03,\n",
       "           1.4940e-03,  9.3152e-04,  7.8863e-04, -7.4916e-04,  3.5564e-04,\n",
       "          -4.4065e-03, -2.6788e-03,  4.4886e-03, -1.3995e-03,  1.6169e-03,\n",
       "          -4.9055e-03],\n",
       "         [ 2.2699e-03,  1.4795e-03,  3.2097e-03,  2.6279e-03, -1.3545e-03,\n",
       "          -7.1423e-03,  3.5642e-05,  1.1007e-03,  3.4969e-03, -7.5699e-03,\n",
       "          -1.4076e-03, -1.7679e-03,  2.9866e-03,  1.9362e-03, -1.5955e-03,\n",
       "           1.5792e-03],\n",
       "         [ 8.9401e-03,  6.1154e-03, -7.9795e-04, -1.7331e-03,  2.1949e-03,\n",
       "          -6.4471e-03, -6.1513e-03, -2.1421e-03,  5.2315e-03, -9.7011e-03,\n",
       "          -8.8943e-03,  1.6583e-03,  1.1732e-02,  6.2704e-03,  1.2068e-03,\n",
       "          -8.0918e-03],\n",
       "         [ 2.6644e-03,  4.7453e-03, -8.0908e-04, -1.5487e-03,  8.3856e-04,\n",
       "           1.7412e-03, -4.6423e-03, -2.9299e-03,  2.5547e-03, -1.3696e-03,\n",
       "          -4.5784e-03,  1.4306e-03,  6.1958e-03,  1.5663e-03,  1.4066e-03,\n",
       "          -6.3305e-03],\n",
       "         [-3.6857e-03, -4.4414e-03,  6.4704e-04,  5.0485e-03,  1.4354e-03,\n",
       "          -1.0257e-03, -3.1275e-03,  2.1193e-04,  1.1045e-03, -1.0313e-03,\n",
       "           2.9071e-03,  2.7382e-03, -3.1935e-03,  1.2537e-03, -9.1253e-04,\n",
       "           6.3217e-03],\n",
       "         [-1.4414e-02, -1.6712e-02, -3.2809e-03,  1.1752e-02,  3.6413e-03,\n",
       "           9.8034e-03,  6.9905e-03,  3.6563e-03, -9.3850e-03,  1.3817e-02,\n",
       "           1.3362e-02, -3.6252e-03, -2.1546e-02, -1.4197e-02,  4.1907e-03,\n",
       "           1.4004e-02]], device='cuda:0'),\n",
       " 'node_network.6.bias': tensor([ 0.0196,  0.0038, -0.0083,  0.0138, -0.0088,  0.0094,  0.0005,  0.0009,\n",
       "         -0.0038, -0.0238,  0.0069,  0.0015,  0.0100,  0.0069, -0.0056, -0.0230],\n",
       "        device='cuda:0'),\n",
       " 'node_network.7.weight': tensor([-0.0010,  0.0018, -0.0014,  0.0011,  0.0019,  0.0007, -0.0012, -0.0007,\n",
       "         -0.0007,  0.0022, -0.0014,  0.0008, -0.0033,  0.0011,  0.0011, -0.0027],\n",
       "        device='cuda:0'),\n",
       " 'node_network.7.bias': tensor([ 0.0067, -0.0002, -0.0030,  0.0048, -0.0038,  0.0030, -0.0004, -0.0007,\n",
       "         -0.0024, -0.0093,  0.0017, -0.0002,  0.0033,  0.0023, -0.0019, -0.0104],\n",
       "        device='cuda:0'),\n",
       " 'node_network.9.weight': tensor([[ 8.9388e-04, -1.0105e-02, -1.7736e-04,  3.6366e-03,  1.7039e-02,\n",
       "          -1.1548e-02, -1.0004e-02,  5.0462e-04, -3.2814e-03,  7.2303e-03,\n",
       "           4.3243e-03,  8.9732e-03,  2.2431e-04, -1.6037e-04,  9.3190e-04,\n",
       "          -7.5687e-03],\n",
       "         [-4.0037e-03, -5.5275e-04,  5.7970e-03,  2.8310e-03,  6.5612e-03,\n",
       "           1.9834e-04, -8.2850e-03,  2.2520e-03,  4.5639e-03,  1.8186e-03,\n",
       "          -5.0114e-03,  2.4004e-03, -6.0802e-03,  4.2071e-03, -2.8071e-03,\n",
       "          -2.3301e-03],\n",
       "         [ 3.4406e-04, -7.8809e-03, -4.1172e-03,  3.0177e-03,  9.8089e-03,\n",
       "          -8.2449e-03, -6.3799e-03, -1.5798e-03, -8.2949e-03,  1.0118e-02,\n",
       "           7.1587e-03,  4.5237e-03,  5.5993e-03, -2.7578e-03,  6.0022e-03,\n",
       "          -8.4271e-03],\n",
       "         [ 1.4249e-03,  8.5158e-04, -5.5035e-03, -1.3290e-04,  1.4844e-03,\n",
       "          -3.4017e-03, -1.0527e-03, -2.9373e-03, -5.8302e-03,  4.0628e-03,\n",
       "           6.9574e-03,  4.1545e-04,  6.0468e-03, -3.7513e-03,  2.4709e-03,\n",
       "          -4.5075e-03],\n",
       "         [-2.9675e-03,  1.5451e-03,  9.2173e-03, -1.7152e-03,  1.3723e-02,\n",
       "          -6.5097e-04, -1.2492e-02,  4.9238e-03,  9.1055e-03, -2.5265e-03,\n",
       "          -7.1286e-03,  8.0740e-03, -9.7250e-03,  5.2780e-03, -9.3632e-03,\n",
       "          -1.3177e-03],\n",
       "         [ 3.5162e-03,  4.4629e-03,  5.9468e-03, -7.5760e-03,  4.2989e-03,\n",
       "           1.4166e-03, -2.3344e-03,  3.2902e-03,  1.0710e-02, -1.0375e-02,\n",
       "          -5.8063e-03,  4.9514e-03, -8.0888e-03,  3.6814e-03, -1.4046e-02,\n",
       "           6.6159e-03],\n",
       "         [-7.5538e-03,  2.2856e-03,  1.8026e-02,  5.9026e-03,  5.1886e-03,\n",
       "           7.3845e-03, -9.4668e-03,  4.4329e-03,  1.5120e-02, -6.6662e-03,\n",
       "          -1.8026e-02,  2.8928e-03, -1.9356e-02,  1.0651e-02, -6.0897e-03,\n",
       "           5.6495e-04],\n",
       "         [-4.2953e-03,  5.0014e-03,  6.6417e-03,  2.5114e-03, -3.7729e-03,\n",
       "           4.1628e-03, -1.4252e-03,  1.9839e-03,  6.2691e-03, -5.1046e-03,\n",
       "          -6.4562e-03, -2.3382e-03, -7.3959e-03,  3.3538e-03, -2.4179e-03,\n",
       "           2.8391e-03],\n",
       "         [ 5.7024e-04, -4.1169e-03, -7.9805e-03,  8.6116e-04, -4.7550e-04,\n",
       "          -4.0316e-03,  1.9630e-03, -1.4207e-03, -7.8618e-03,  5.8299e-03,\n",
       "           7.6347e-03, -1.0723e-03,  8.8097e-03, -4.8639e-03,  6.1966e-03,\n",
       "          -2.1649e-03],\n",
       "         [ 2.0046e-03,  5.7449e-03, -5.1608e-03, -2.3664e-03, -1.0469e-02,\n",
       "           3.6982e-03,  7.5012e-03, -2.5237e-03, -7.9731e-04, -3.0327e-03,\n",
       "           2.8144e-03, -4.1627e-03,  4.0661e-03, -3.1043e-03,  1.1656e-04,\n",
       "           2.2233e-03],\n",
       "         [-4.9516e-03, -4.4423e-03,  1.1707e-02,  6.2295e-03,  1.9807e-02,\n",
       "          -3.7394e-03, -1.9623e-02,  4.6019e-03,  6.8551e-03,  3.8852e-03,\n",
       "          -7.3635e-03,  9.7722e-03, -1.3071e-02,  6.6487e-03, -6.4026e-03,\n",
       "          -6.0038e-03],\n",
       "         [-7.5144e-05,  9.4294e-04, -3.7832e-03, -4.0939e-04, -6.0823e-03,\n",
       "           1.4720e-03,  5.2091e-03, -1.2266e-03, -2.8917e-03,  4.6189e-04,\n",
       "           3.0702e-03, -3.9325e-03,  4.2231e-03, -2.0971e-03,  3.6459e-03,\n",
       "           9.4434e-04],\n",
       "         [ 1.0122e-02, -4.7660e-03, -2.2819e-02, -7.9778e-03, -1.1811e-02,\n",
       "          -3.0117e-03,  1.7286e-02, -6.8535e-03, -2.1178e-02,  8.5260e-03,\n",
       "           2.0226e-02, -5.5397e-03,  2.5323e-02, -1.4101e-02,  1.2912e-02,\n",
       "          -1.0134e-03],\n",
       "         [-7.0955e-04,  2.8190e-03,  4.6569e-03, -1.1984e-03, -1.1068e-03,\n",
       "           2.4212e-03, -6.8152e-04,  3.1960e-03,  5.8990e-03, -3.8048e-03,\n",
       "          -6.0471e-03, -1.6274e-03, -5.0796e-03,  3.9599e-03, -5.4377e-03,\n",
       "           4.7079e-03],\n",
       "         [ 6.9299e-03,  1.6336e-03, -9.4239e-03, -3.3475e-03, -1.3724e-02,\n",
       "           6.4866e-04,  1.4061e-02, -5.6022e-03, -6.7343e-03, -4.0859e-04,\n",
       "           6.5620e-03, -6.2610e-03,  9.8395e-03, -5.0630e-03,  5.0767e-03,\n",
       "           1.8663e-03],\n",
       "         [ 3.8541e-03, -2.7396e-03, -9.5359e-03,  1.0096e-03, -4.3517e-03,\n",
       "          -3.7829e-03,  5.7572e-03, -4.2947e-03, -8.9073e-03,  5.6448e-03,\n",
       "           8.3434e-03, -2.5066e-03,  1.0588e-02, -5.7543e-03,  8.1347e-03,\n",
       "          -3.5172e-03]], device='cuda:0'),\n",
       " 'node_network.9.bias': tensor([ 8.9954e-05,  7.3803e-03, -7.0924e-03, -8.1370e-03,  1.2596e-02,\n",
       "          1.2197e-02,  2.2975e-02,  7.7796e-03, -1.1036e-02, -4.4116e-03,\n",
       "          1.6163e-02, -5.4457e-03, -2.9614e-02,  7.3053e-03, -1.0998e-02,\n",
       "         -1.1795e-02], device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_checkpointed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory max (GB): 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory max (GB):\", torch.cuda.max_memory_allocated() / 1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointed Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68278\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201015_125031-2sj71rwe/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201015_125031-2sj71rwe/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33methereal-paper-192\u001b[0m: \u001b[34mhttps://wandb.ai/murnanedaniel/CheckpointExploration/runs/2sj71rwe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201015_125451-lebw9fio\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpleasant-jazz-193\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/murnanedaniel/CheckpointExploration\" target=\"_blank\">https://wandb.ai/murnanedaniel/CheckpointExploration</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/murnanedaniel/CheckpointExploration/runs/lebw9fio\" target=\"_blank\">https://wandb.ai/murnanedaniel/CheckpointExploration/runs/lebw9fio</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "m_configs = {\"in_channels\": 12, \"hidden_dim\": 32, \"n_graph_iters\": 8}\n",
    "model = CheckResAGNN_Stripped(**m_configs).to(device)\n",
    "\n",
    "# m_configs = {\"input_dim\": 3, \"hidden_node_dim\": 64, \"hidden_edge_dim\": 64, \"in_layers\": 1, \"node_layers\": 3, \"edge_layers\": 3, \"n_graph_iters\": 8, \"layer_norm\": True}\n",
    "# model = CheckMPNN_Network(**m_configs).to(device)\n",
    "\n",
    "model_name = wandb.init(project=\"CheckpointExploration\", config=m_configs)\n",
    "wandb.watch(model, log='all')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=5)\n",
    "\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.3642, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "1 tensor(1.3648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "2 tensor(1.3236, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "3 tensor(1.3407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "4 tensor(1.3073, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "5 tensor(1.2915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "6 tensor(1.2839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "7 tensor(1.2782, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "8 tensor(1.2860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "9 tensor(1.2808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "10 tensor(1.2678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "11 tensor(1.2949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "12 tensor(1.2845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "13 tensor(1.2720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "14 tensor(1.2831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "15 tensor(1.2770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "16 tensor(1.2314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "17 tensor(1.2508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "18 tensor(1.2508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "19 tensor(1.2698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "20 tensor(1.2764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "21 tensor(1.2682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "22 tensor(1.2565, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "23 tensor(1.2507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "24 tensor(1.2500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "25 tensor(1.2472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "26 tensor(1.2518, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "27 tensor(1.2256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "28 tensor(1.1999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "29 tensor(1.2546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "30 tensor(1.2061, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "31 tensor(1.2254, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "32 tensor(1.2402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "33 tensor(1.1783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "34 tensor(1.2141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "35 tensor(1.2347, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "36 tensor(1.1942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "37 tensor(1.2310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "38 tensor(1.2088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "39 tensor(1.1946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "40 tensor(1.2018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "41 tensor(1.2367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "42 tensor(1.2210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "43 tensor(1.2321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "44 tensor(1.2071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "45 tensor(1.2246, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "46 tensor(1.2042, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "47 tensor(1.2159, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "48 tensor(1.1848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "49 tensor(1.2131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "50 tensor(1.2026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "51 tensor(1.2176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "52 tensor(1.1808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "53 tensor(1.1264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "54 tensor(1.2040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "55 tensor(1.2109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "56 tensor(1.2027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "57 tensor(1.2205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "58 tensor(1.1936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "59 tensor(1.2064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "60 tensor(1.2023, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "61 tensor(1.2033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "62 tensor(1.1947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "63 tensor(1.2004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "64 tensor(1.2137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "65 tensor(1.2050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "66 tensor(1.1859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "67 tensor(1.1902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "68 tensor(1.1746, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "69 tensor(1.2355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "70 tensor(1.2003, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "71 tensor(1.1642, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "72 tensor(1.2027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "73 tensor(1.2219, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "74 tensor(1.2189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "75 tensor(1.2207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "76 tensor(1.2435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "77 tensor(1.2104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "78 tensor(1.1797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "79 tensor(1.1818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "80 tensor(1.2326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "81 tensor(1.2196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "82 tensor(1.2007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "83 tensor(1.1854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "84 tensor(1.2170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "85 tensor(1.1841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "86 tensor(1.2124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "87 tensor(1.1973, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "88 tensor(1.2100, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "89 tensor(1.2119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "90 tensor(1.1929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "91 tensor(1.1930, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "92 tensor(1.1654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "93 tensor(1.1903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "94 tensor(1.1995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "95 tensor(1.2153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "96 tensor(1.2191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "97 tensor(1.1815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "98 tensor(1.2092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(1.2097, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Memory max (GB): 9.426188468933105 - time: 1.2816722393035889\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, e, cell = batch.x.to(device), batch.edge_index.to(device), batch.cell_data.to(device)\n",
    "    weight = torch.tensor((~batch.y.bool()).sum() / batch.y.sum())\n",
    "    combined_y = batch.y_pid.float()         \n",
    "#     x.requires_grad_(True)\n",
    "    \n",
    "    output = model(torch.cat([cell, x], axis=-1), e)\n",
    "#     output = model(x, e)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(output, combined_y.to(device), pos_weight = weight)\n",
    "    total_loss += loss.item()\n",
    "    print(i, loss)\n",
    "    tic = tt()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     break\n",
    "    \n",
    "print(\"Memory max (GB):\", torch.cuda.max_memory_allocated() / 1024**3, \"- time:\", tt()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_checkpointed = output.data.clone()\n",
    "grad_checkpointed = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_checkpointed[name] = param.grad.data.clone()\n",
    "    else:\n",
    "        print(name, \"is NONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Memory Grid Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Want to scan over MP iterations, network size (width), and MLP layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer_set = 1, 7, 1\n",
    "iteration_set = 1, 9, 1\n",
    "channel_set = 8, 128, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_step(input_data, model, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, e = input_data.x.to(device), input_data.e_radius.to(device)\n",
    "    weight = torch.tensor((~input_data.y.bool()).sum() / input_data.y.sum())\n",
    "    combined_y = input_data.y         \n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    output = model(x, e)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(output, combined_y.to(device), pos_weight = weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_data = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "memory_uncheck = np.empty(((layer_set[1] - layer_set[0])//layer_set[2], (iteration_set[1] - iteration_set[0])//iteration_set[2], (channel_set[1] - channel_set[0])//channel_set[2]))\n",
    "memory_check = np.empty(((layer_set[1] - layer_set[0])//layer_set[2], (iteration_set[1] - iteration_set[0])//iteration_set[2], (channel_set[1] - channel_set[0])//channel_set[2]))\n",
    "time_uncheck = np.empty(((layer_set[1] - layer_set[0])//layer_set[2], (iteration_set[1] - iteration_set[0])//iteration_set[2], (channel_set[1] - channel_set[0])//channel_set[2]))\n",
    "time_check = np.empty(((layer_set[1] - layer_set[0])//layer_set[2], (iteration_set[1] - iteration_set[0])//iteration_set[2], (channel_set[1] - channel_set[0])//channel_set[2]))\n",
    "\n",
    "\n",
    "for i, layers in enumerate(np.arange(*layer_set)):\n",
    "    for j, iterations in enumerate(np.arange(*iteration_set)):\n",
    "        for k, channels in enumerate(np.arange(*channel_set)):\n",
    "            \n",
    "            torch.cuda.reset_max_memory_allocated()\n",
    "            torch.manual_seed(0)\n",
    "\n",
    "            m_configs = {\"input_dim\": 3, \"hidden_node_dim\": channels, \"hidden_edge_dim\": channels, \"in_layers\": layers, \"node_layers\": layers, \"edge_layers\": layers, \"n_graph_iters\": iterations, \"layer_norm\": True}\n",
    "            model = MPNN_Network(**m_configs).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "            \n",
    "            tic = tt()\n",
    "            train_step(input_data, model, optimizer)\n",
    "            tic_uncheck = tt() - tic\n",
    "            time_uncheck[i, j, k] = tic_uncheck\n",
    "            max_mem_uncheck = torch.cuda.max_memory_allocated() / 1024**3\n",
    "            memory_uncheck[i, j, k] = max_mem_uncheck\n",
    "            \n",
    "                        \n",
    "            torch.cuda.reset_max_memory_allocated()\n",
    "            torch.manual_seed(0)\n",
    "            \n",
    "            model = CheckMPNN_Network(**m_configs).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "            \n",
    "            tic = tt()\n",
    "            train_step(input_data, model, optimizer)\n",
    "            tic_check = tt() - tic\n",
    "            time_check[i, j, k] = tic_check\n",
    "            max_mem_check = torch.cuda.max_memory_allocated() / 1024**3\n",
    "            memory_check[i, j, k] = max_mem_check\n",
    "            print(\"Step:\", (i, j, k), \"- Memory (unchecked):\", max_mem_uncheck, \"(checked):\", max_mem_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Compare Checked and Unchecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mem_ratio = memory_uncheck / memory_check\n",
    "time_ratio = time_check / time_uncheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer_list = np.arange(*layer_set)\n",
    "iteration_list = np.arange(*iteration_set)\n",
    "channel_list = np.arange(*channel_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean_mem_ratio = np.mean(mem_ratio, axis=2)\n",
    "max_mem_ratio = np.max(mem_ratio, axis=2)\n",
    "mean_time_ratio = np.mean(time_ratio, axis=2)\n",
    "max_time_ratio = np.max(time_ratio, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "im = ax.imshow(max_mem_ratio)\n",
    "\n",
    "ax.set_xticklabels(np.arange(0, 9))\n",
    "ax.set_yticklabels(np.arange(0, 8))\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(layer_list)):\n",
    "    for j in range(len(iteration_list)):\n",
    "        text = ax.text(j, i, str('{:.1f}').format(max_mem_ratio[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Memory Improvement (Maxed over Channels)\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Layers\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "im = ax.imshow(max_time_ratio)\n",
    "\n",
    "ax.set_xticklabels(np.arange(0, 9))\n",
    "ax.set_yticklabels(np.arange(0, 8))\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(layer_list)):\n",
    "    for j in range(len(iteration_list)):\n",
    "        text = ax.text(j, i, str('{:.1f}').format(max_time_ratio[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Speed Penalty (Maxed over Channels)\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Layers\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    tic = tt() \n",
    "    model.train()\n",
    "    train_loss = train_gnn(model, train_loader, optimizer, m_configs)\n",
    "    print('Training loss: {:.4f} in time {}'.format(train_loss, tt() - tic))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        edge_pur, edge_eff, val_loss = evaluate_gnn(model, test_loader, m_configs)\n",
    "    wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss, \"edge_pur\": edge_pur, \"edge_eff\": edge_eff, \"lr\": optimizer.param_groups[0]['lr']})\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    save_model(epoch, model, optimizer, scheduler, val_loss, m_configs, 'ResAGNN/'+model_name._name+'.tar')\n",
    "\n",
    "    print('Epoch: {}, Eff: {:.4f}, Pur: {:.4f}, Loss: {:.4f}, LR: {} in time {}'.format(epoch, edge_eff, edge_pur, val_loss, optimizer.param_groups[0]['lr'], tt()-tic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExatrkxTest",
   "language": "python",
   "name": "exatrkx-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
