{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Architecture Performance Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "from time import time as tt\n",
    "import importlib\n",
    "\n",
    "# External imports\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from itertools import chain\n",
    "from random import shuffle, sample\n",
    "from scipy.optimize import root_scalar as root\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_cluster import knn_graph, radius_graph\n",
    "import trackml.dataset\n",
    "import torch_geometric\n",
    "from itertools import permutations\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from argparse import Namespace\n",
    "from trackml.score import score_event\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Limit CPU usage on Jupyter\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Pick up local packages\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../Tracking-ML-Exa.TrkX/src/Pipelines/Examples')\n",
    "from LightningModules.GNN.Models.checkpoint_agnn import CheckpointedResAGNN\n",
    "# from LightningModules.Embedding.utils import get_best_run, build_edges, res, graph_intersection\n",
    "\n",
    "# Local imports\n",
    "from prepare_utils import *\n",
    "from performance_utils import *\n",
    "from toy_utils import *\n",
    "from models import *\n",
    "from trainers import *\n",
    "from lightning_modules.filter_scanner import Filter_Model\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Get rid of RuntimeWarnings, gross\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import wandb\n",
    "import faiss\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "torch_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Lightning Load Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_label = \"akfxzlsc\"\n",
    "\n",
    "best_run_path = get_best_run(run_label, wandb_save_dir=\"/global/cscratch1/sd/danieltm/test_20200930/wandb_data\")\n",
    "\n",
    "chkpnt = torch.load(best_run_path)\n",
    "hparams = chkpnt['hyper_parameters']\n",
    "model = Filter_Model.load_from_checkpoint(best_run_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Prepare GNN Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pt_cut = 0\n",
    "train_number = 1000\n",
    "test_number = 100\n",
    "load_dir = \"/global/cscratch1/sd/danieltm/ExaTrkX/trackml_processed/filter_processed/\"\n",
    "save_dir = \"/global/cscratch1/sd/danieltm/test_20200930/trackml_processed/graphs_processed/0_pt_cut/all_events\"\n",
    "\n",
    "basename = os.path.join(load_dir, str(pt_cut) + \"_pt_cut\")\n",
    "train_path = os.path.join(basename, str(train_number) + \"_events_train_cell_info.pkl\")\n",
    "test_path = os.path.join(basename, str(test_number) + \"_events_test_cell_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 43 s, total: 44.6 s\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = torch.load(train_path)\n",
    "test_dataset = torch.load(test_path)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sections = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 saved in time 0.001600503921508789\n",
      "1 saved in time 0.00089263916015625\n",
      "2 saved in time 0.0006067752838134766\n",
      "3 saved in time 0.0007889270782470703\n",
      "4 saved in time 0.0006837844848632812\n",
      "5 saved in time 0.0007073879241943359\n",
      "6 saved in time 0.0008025169372558594\n",
      "7 saved in time 0.0006792545318603516\n",
      "8 saved in time 0.0005385875701904297\n",
      "9 saved in time 0.0006489753723144531\n",
      "10 saved in time 0.0008313655853271484\n",
      "11 saved in time 0.0006642341613769531\n",
      "12 saved in time 0.0007109642028808594\n",
      "13 saved in time 0.0005555152893066406\n",
      "14 saved in time 0.0008051395416259766\n",
      "15 saved in time 0.0006551742553710938\n",
      "16 saved in time 0.0006959438323974609\n",
      "17 saved in time 0.0005574226379394531\n",
      "18 saved in time 0.0007450580596923828\n",
      "19 saved in time 0.0008175373077392578\n",
      "20 saved in time 0.0004887580871582031\n",
      "21 saved in time 0.0006873607635498047\n",
      "22 saved in time 0.0006415843963623047\n",
      "23 saved in time 0.0005433559417724609\n",
      "24 saved in time 0.0006670951843261719\n",
      "25 saved in time 0.0007145404815673828\n",
      "26 saved in time 0.0008070468902587891\n",
      "27 saved in time 0.0007083415985107422\n",
      "28 saved in time 0.0006878376007080078\n",
      "29 saved in time 0.0007190704345703125\n",
      "30 saved in time 0.00060272216796875\n",
      "31 saved in time 0.0005164146423339844\n",
      "32 saved in time 0.0006525516510009766\n",
      "33 saved in time 0.0007107257843017578\n",
      "34 saved in time 0.0005404949188232422\n",
      "35 saved in time 0.0006780624389648438\n",
      "36 saved in time 0.0006432533264160156\n",
      "37 saved in time 0.0007412433624267578\n",
      "38 saved in time 0.0005354881286621094\n",
      "39 saved in time 0.0004978179931640625\n",
      "40 saved in time 0.000522613525390625\n",
      "41 saved in time 0.0005192756652832031\n",
      "42 saved in time 0.0005931854248046875\n",
      "43 saved in time 0.0007491111755371094\n",
      "44 saved in time 0.0007429122924804688\n",
      "45 saved in time 0.0008492469787597656\n",
      "46 saved in time 0.000484466552734375\n",
      "47 saved in time 0.0006699562072753906\n",
      "48 saved in time 0.0006184577941894531\n",
      "49 saved in time 0.0006928443908691406\n",
      "50 saved in time 0.0007076263427734375\n",
      "51 saved in time 0.0005662441253662109\n",
      "52 saved in time 0.0006289482116699219\n",
      "53 saved in time 0.0007078647613525391\n",
      "54 saved in time 0.000514984130859375\n",
      "55 saved in time 0.0007760524749755859\n",
      "56 saved in time 0.0006053447723388672\n",
      "57 saved in time 0.0006082057952880859\n",
      "58 saved in time 0.0005507469177246094\n",
      "59 saved in time 0.0006670951843261719\n",
      "60 saved in time 0.0007011890411376953\n",
      "61 saved in time 0.0007736682891845703\n",
      "62 saved in time 0.0005841255187988281\n",
      "63 saved in time 0.0006625652313232422\n",
      "64 saved in time 0.0006744861602783203\n",
      "65 saved in time 0.0007083415985107422\n",
      "66 saved in time 0.0007278919219970703\n",
      "67 saved in time 0.0007245540618896484\n",
      "68 saved in time 0.00047278404235839844\n",
      "69 saved in time 0.00054168701171875\n",
      "70 saved in time 0.0006468296051025391\n",
      "71 saved in time 0.0007369518280029297\n",
      "72 saved in time 0.00039386749267578125\n",
      "73 saved in time 0.0006918907165527344\n",
      "74 saved in time 0.0007121562957763672\n",
      "75 saved in time 0.0007872581481933594\n",
      "76 saved in time 0.0005776882171630859\n",
      "77 saved in time 0.0005593299865722656\n",
      "78 saved in time 0.0006113052368164062\n",
      "79 saved in time 0.0007722377777099609\n",
      "80 saved in time 0.0005645751953125\n",
      "81 saved in time 0.0005922317504882812\n",
      "82 saved in time 0.0005738735198974609\n",
      "83 saved in time 0.0007340908050537109\n",
      "84 saved in time 0.0005462169647216797\n",
      "85 saved in time 0.0007741451263427734\n",
      "86 saved in time 0.0006248950958251953\n",
      "87 saved in time 0.0006470680236816406\n",
      "88 saved in time 0.0005385875701904297\n",
      "89 saved in time 0.0007927417755126953\n",
      "90 saved in time 0.0006151199340820312\n",
      "91 saved in time 0.0007617473602294922\n",
      "92 saved in time 0.0006198883056640625\n",
      "93 saved in time 0.000732421875\n",
      "94 saved in time 0.0006296634674072266\n",
      "95 saved in time 0.0008144378662109375\n",
      "96 saved in time 0.0007169246673583984\n",
      "97 saved in time 0.000518798828125\n",
      "98 saved in time 0.000766754150390625\n",
      "99 saved in time 0.0006372928619384766\n",
      "100 saved in time 0.0006101131439208984\n",
      "101 saved in time 0.0007228851318359375\n",
      "102 saved in time 0.0006384849548339844\n",
      "103 saved in time 0.0005528926849365234\n",
      "104 saved in time 0.0006580352783203125\n",
      "105 saved in time 0.0006563663482666016\n",
      "106 saved in time 0.0007574558258056641\n",
      "107 saved in time 0.0007207393646240234\n",
      "108 saved in time 0.0004620552062988281\n",
      "109 saved in time 0.0005543231964111328\n",
      "110 saved in time 0.0007510185241699219\n",
      "111 saved in time 0.0005650520324707031\n",
      "112 saved in time 0.000782012939453125\n",
      "113 saved in time 0.0005197525024414062\n",
      "114 saved in time 0.0006518363952636719\n",
      "115 saved in time 0.0006186962127685547\n",
      "116 saved in time 0.0005626678466796875\n",
      "117 saved in time 0.0008184909820556641\n",
      "118 saved in time 0.0005936622619628906\n",
      "119 saved in time 0.0006036758422851562\n",
      "120 saved in time 0.0007331371307373047\n",
      "121 saved in time 0.0008182525634765625\n",
      "122 saved in time 0.0004792213439941406\n",
      "123 saved in time 0.0005791187286376953\n",
      "124 saved in time 0.00055694580078125\n",
      "125 saved in time 0.0005450248718261719\n",
      "126 saved in time 0.0005390644073486328\n",
      "127 saved in time 0.0006825923919677734\n",
      "128 saved in time 0.0006570816040039062\n",
      "129 saved in time 0.0006616115570068359\n",
      "130 saved in time 0.0005934238433837891\n",
      "131 saved in time 0.0005631446838378906\n",
      "132 saved in time 0.0005364418029785156\n",
      "133 saved in time 0.0007009506225585938\n",
      "134 saved in time 0.0005114078521728516\n",
      "135 saved in time 0.0006480216979980469\n",
      "136 saved in time 0.0004885196685791016\n",
      "137 saved in time 0.0006303787231445312\n",
      "138 saved in time 0.0006165504455566406\n",
      "139 saved in time 0.0005624294281005859\n",
      "140 saved in time 0.0005793571472167969\n",
      "141 saved in time 0.0007200241088867188\n",
      "142 saved in time 0.0005524158477783203\n",
      "143 saved in time 0.0006537437438964844\n",
      "144 saved in time 0.0006268024444580078\n",
      "145 saved in time 0.0006563663482666016\n",
      "146 saved in time 0.0005292892456054688\n",
      "147 saved in time 0.0006427764892578125\n",
      "148 saved in time 0.0005707740783691406\n",
      "149 saved in time 0.0006096363067626953\n",
      "150 saved in time 0.0006475448608398438\n",
      "151 saved in time 0.0005519390106201172\n",
      "152 saved in time 0.0005824565887451172\n",
      "153 saved in time 0.0007429122924804688\n",
      "154 saved in time 0.0007557868957519531\n",
      "155 saved in time 0.0007054805755615234\n",
      "156 saved in time 0.0005631446838378906\n",
      "157 saved in time 0.000492095947265625\n",
      "158 saved in time 0.0006449222564697266\n",
      "159 saved in time 0.0006022453308105469\n",
      "160 saved in time 0.00045299530029296875\n",
      "161 saved in time 0.0007083415985107422\n",
      "162 saved in time 0.000720977783203125\n",
      "163 saved in time 0.0005295276641845703\n",
      "164 saved in time 0.0005931854248046875\n",
      "165 saved in time 0.0007092952728271484\n",
      "166 saved in time 0.0006849765777587891\n",
      "167 saved in time 0.0005199909210205078\n",
      "168 saved in time 0.0006151199340820312\n",
      "169 saved in time 0.0005762577056884766\n",
      "170 saved in time 0.0006113052368164062\n",
      "171 saved in time 0.0006740093231201172\n",
      "172 saved in time 0.000507354736328125\n",
      "173 saved in time 0.0007462501525878906\n",
      "174 saved in time 0.0006229877471923828\n",
      "175 saved in time 0.0005030632019042969\n",
      "176 saved in time 0.0005571842193603516\n",
      "177 saved in time 0.0005924701690673828\n",
      "178 saved in time 0.0005710124969482422\n",
      "179 saved in time 0.0006282329559326172\n",
      "180 saved in time 0.000644683837890625\n",
      "181 saved in time 0.0006039142608642578\n",
      "182 saved in time 0.0004930496215820312\n",
      "183 saved in time 0.0006277561187744141\n",
      "184 saved in time 0.0006122589111328125\n",
      "185 saved in time 0.0005774497985839844\n",
      "186 saved in time 0.00043845176696777344\n",
      "187 saved in time 0.0006539821624755859\n",
      "188 saved in time 0.0005598068237304688\n",
      "189 saved in time 0.0006687641143798828\n",
      "190 saved in time 0.0006177425384521484\n",
      "191 saved in time 0.0006387233734130859\n",
      "192 saved in time 0.0006394386291503906\n",
      "193 saved in time 0.0005023479461669922\n",
      "194 saved in time 0.000461578369140625\n",
      "195 saved in time 0.0005953311920166016\n",
      "196 saved in time 0.0006279945373535156\n",
      "197 saved in time 0.0007653236389160156\n",
      "198 saved in time 0.0006158351898193359\n",
      "199 saved in time 0.0006892681121826172\n",
      "200 saved in time 0.0005469322204589844\n",
      "201 saved in time 0.0005862712860107422\n",
      "202 saved in time 0.0007154941558837891\n",
      "203 saved in time 0.0007650852203369141\n",
      "204 saved in time 0.0006554126739501953\n",
      "205 saved in time 0.0005714893341064453\n",
      "206 saved in time 0.0005118846893310547\n",
      "207 saved in time 0.0006172657012939453\n",
      "208 saved in time 0.0006053447723388672\n",
      "209 saved in time 0.0005805492401123047\n",
      "210 saved in time 0.0004622936248779297\n",
      "211 saved in time 0.000621795654296875\n",
      "212 saved in time 0.0005855560302734375\n",
      "213 saved in time 0.0007762908935546875\n",
      "214 saved in time 0.0004909038543701172\n",
      "215 saved in time 0.0005965232849121094\n",
      "216 saved in time 0.0007929801940917969\n",
      "217 saved in time 0.0006272792816162109\n",
      "218 saved in time 0.0004980564117431641\n",
      "219 saved in time 0.0007786750793457031\n",
      "220 saved in time 0.00070953369140625\n",
      "221 saved in time 0.0006494522094726562\n",
      "222 saved in time 0.0005018711090087891\n",
      "223 saved in time 0.0006730556488037109\n",
      "224 saved in time 0.0005941390991210938\n",
      "225 saved in time 0.0006697177886962891\n",
      "226 saved in time 0.0006401538848876953\n",
      "227 saved in time 0.0005576610565185547\n",
      "228 saved in time 0.000576019287109375\n",
      "229 saved in time 0.0006134510040283203\n",
      "230 saved in time 0.0005438327789306641\n",
      "231 saved in time 0.0005905628204345703\n",
      "232 saved in time 0.0005784034729003906\n",
      "233 saved in time 0.0005974769592285156\n",
      "234 saved in time 0.0006465911865234375\n",
      "235 saved in time 0.00048351287841796875\n",
      "236 saved in time 0.0006928443908691406\n",
      "237 saved in time 0.00075531005859375\n",
      "238 saved in time 0.0006749629974365234\n",
      "239 saved in time 0.0006525516510009766\n",
      "240 saved in time 0.0005819797515869141\n",
      "241 saved in time 0.0006246566772460938\n",
      "242 saved in time 0.00048279762268066406\n",
      "243 saved in time 0.0006422996520996094\n",
      "244 saved in time 0.0006780624389648438\n",
      "245 saved in time 0.0006148815155029297\n",
      "246 saved in time 0.0005905628204345703\n",
      "247 saved in time 0.0005388259887695312\n",
      "248 saved in time 0.0006506443023681641\n",
      "249 saved in time 0.0005652904510498047\n",
      "250 saved in time 0.0005414485931396484\n",
      "251 saved in time 0.000591278076171875\n",
      "252 saved in time 0.0005500316619873047\n",
      "253 saved in time 0.0007386207580566406\n",
      "254 saved in time 0.0005826950073242188\n",
      "255 saved in time 0.0007717609405517578\n",
      "256 saved in time 0.0004477500915527344\n",
      "257 saved in time 0.0006721019744873047\n",
      "258 saved in time 0.0005893707275390625\n",
      "259 saved in time 0.0007238388061523438\n",
      "260 saved in time 0.0005085468292236328\n",
      "261 saved in time 0.0007381439208984375\n",
      "262 saved in time 0.0005807876586914062\n",
      "263 saved in time 0.0005791187286376953\n",
      "264 saved in time 0.0007047653198242188\n",
      "265 saved in time 0.0005638599395751953\n",
      "266 saved in time 0.0006389617919921875\n",
      "267 saved in time 0.0007328987121582031\n",
      "268 saved in time 0.00048232078552246094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269 saved in time 0.0006635189056396484\n",
      "270 saved in time 0.0006654262542724609\n",
      "271 saved in time 0.0005879402160644531\n",
      "272 saved in time 0.0006017684936523438\n",
      "273 saved in time 0.0007472038269042969\n",
      "274 saved in time 0.0005445480346679688\n",
      "275 saved in time 0.0006933212280273438\n",
      "276 saved in time 0.0007627010345458984\n",
      "277 saved in time 0.0006377696990966797\n",
      "278 saved in time 0.0005397796630859375\n",
      "279 saved in time 0.0005831718444824219\n",
      "280 saved in time 0.0005257129669189453\n",
      "281 saved in time 0.0007479190826416016\n",
      "282 saved in time 0.0005679130554199219\n",
      "283 saved in time 0.0006432533264160156\n",
      "284 saved in time 0.0005216598510742188\n",
      "285 saved in time 0.0005011558532714844\n",
      "286 saved in time 0.0006444454193115234\n",
      "287 saved in time 0.0007166862487792969\n",
      "288 saved in time 0.0005717277526855469\n",
      "289 saved in time 0.0006115436553955078\n",
      "290 saved in time 0.0007181167602539062\n",
      "291 saved in time 0.0006628036499023438\n",
      "292 saved in time 0.0004987716674804688\n",
      "293 saved in time 0.0007331371307373047\n",
      "294 saved in time 0.0005850791931152344\n",
      "295 saved in time 0.0006730556488037109\n",
      "296 saved in time 0.0006411075592041016\n",
      "297 saved in time 0.0003631114959716797\n",
      "298 saved in time 0.0005400180816650391\n",
      "299 saved in time 0.0004601478576660156\n",
      "300 saved in time 0.000720977783203125\n",
      "301 saved in time 0.0007367134094238281\n",
      "302 saved in time 0.0005645751953125\n",
      "303 saved in time 0.0007948875427246094\n",
      "304 saved in time 0.0004303455352783203\n",
      "305 saved in time 0.0006771087646484375\n",
      "306 saved in time 0.0005815029144287109\n",
      "307 saved in time 0.0005736351013183594\n",
      "308 saved in time 0.0006084442138671875\n",
      "309 saved in time 0.0005998611450195312\n",
      "310 saved in time 0.0007557868957519531\n",
      "311 saved in time 0.00044465065002441406\n",
      "312 saved in time 0.0006725788116455078\n",
      "313 saved in time 0.0007123947143554688\n",
      "314 saved in time 0.0006859302520751953\n",
      "315 saved in time 0.0006592273712158203\n",
      "316 saved in time 0.0005331039428710938\n",
      "317 saved in time 0.0007317066192626953\n",
      "318 saved in time 0.0006232261657714844\n",
      "319 saved in time 0.0005717277526855469\n",
      "320 saved in time 0.0006608963012695312\n",
      "321 saved in time 0.0005664825439453125\n",
      "322 saved in time 0.0006380081176757812\n",
      "323 saved in time 0.0007407665252685547\n",
      "324 saved in time 0.00046181678771972656\n",
      "325 saved in time 0.0006861686706542969\n",
      "326 saved in time 0.0005984306335449219\n",
      "327 saved in time 0.0006773471832275391\n",
      "328 saved in time 0.0006089210510253906\n",
      "329 saved in time 0.0006651878356933594\n",
      "330 saved in time 0.0005552768707275391\n",
      "331 saved in time 0.0006015300750732422\n",
      "332 saved in time 0.0005919933319091797\n",
      "333 saved in time 0.0006520748138427734\n",
      "334 saved in time 0.0006167888641357422\n",
      "335 saved in time 0.0006520748138427734\n",
      "336 saved in time 0.0006420612335205078\n",
      "337 saved in time 0.0006268024444580078\n",
      "338 saved in time 0.0006656646728515625\n",
      "339 saved in time 0.0006589889526367188\n",
      "340 saved in time 0.0007383823394775391\n",
      "341 saved in time 0.0005686283111572266\n",
      "342 saved in time 0.0008137226104736328\n",
      "343 saved in time 0.0006158351898193359\n",
      "344 saved in time 0.0005872249603271484\n",
      "345 saved in time 0.0005838871002197266\n",
      "346 saved in time 0.0005366802215576172\n",
      "347 saved in time 0.0006692409515380859\n",
      "348 saved in time 0.0006968975067138672\n",
      "349 saved in time 0.0005018711090087891\n",
      "350 saved in time 0.0007517337799072266\n",
      "351 saved in time 0.0007295608520507812\n",
      "352 saved in time 0.0004286766052246094\n",
      "353 saved in time 0.0006840229034423828\n",
      "354 saved in time 0.0005893707275390625\n",
      "355 saved in time 0.0004975795745849609\n",
      "356 saved in time 0.000640869140625\n",
      "357 saved in time 0.0008199214935302734\n",
      "358 saved in time 0.0008254051208496094\n",
      "359 saved in time 0.0006239414215087891\n",
      "360 saved in time 0.0006535053253173828\n",
      "361 saved in time 0.0007319450378417969\n",
      "362 saved in time 0.0005309581756591797\n",
      "363 saved in time 0.0006730556488037109\n",
      "364 saved in time 0.0005903244018554688\n",
      "365 saved in time 0.0005943775177001953\n",
      "366 saved in time 0.00067901611328125\n",
      "367 saved in time 0.0007336139678955078\n",
      "368 saved in time 0.0004875659942626953\n",
      "369 saved in time 0.0006656646728515625\n",
      "370 saved in time 0.0006284713745117188\n",
      "371 saved in time 0.0006835460662841797\n",
      "372 saved in time 0.0006272792816162109\n",
      "373 saved in time 0.0005691051483154297\n",
      "374 saved in time 0.0005657672882080078\n",
      "375 saved in time 0.0006372928619384766\n",
      "376 saved in time 0.0005626678466796875\n",
      "377 saved in time 0.0005357265472412109\n",
      "378 saved in time 0.0005919933319091797\n",
      "379 saved in time 0.0007076263427734375\n",
      "380 saved in time 0.0004949569702148438\n",
      "381 saved in time 0.0007171630859375\n",
      "382 saved in time 0.0006747245788574219\n",
      "383 saved in time 0.0005412101745605469\n",
      "384 saved in time 0.0006384849548339844\n",
      "385 saved in time 0.0006628036499023438\n",
      "386 saved in time 0.0003745555877685547\n",
      "387 saved in time 0.00063323974609375\n",
      "388 saved in time 0.0006115436553955078\n",
      "389 saved in time 0.0006437301635742188\n",
      "390 saved in time 0.0004565715789794922\n",
      "391 saved in time 0.0006175041198730469\n",
      "392 saved in time 0.0006117820739746094\n",
      "393 saved in time 0.0006020069122314453\n",
      "394 saved in time 0.0005519390106201172\n",
      "395 saved in time 0.000652313232421875\n",
      "396 saved in time 0.0006449222564697266\n",
      "397 saved in time 0.0005347728729248047\n",
      "398 saved in time 0.0005788803100585938\n",
      "399 saved in time 0.000637054443359375\n",
      "400 saved in time 0.00048041343688964844\n",
      "401 saved in time 0.0006506443023681641\n",
      "402 saved in time 0.0005905628204345703\n",
      "403 saved in time 0.0005156993865966797\n",
      "404 saved in time 0.0006487369537353516\n",
      "405 saved in time 0.0007228851318359375\n",
      "406 saved in time 0.0009264945983886719\n",
      "407 saved in time 0.0007472038269042969\n",
      "408 saved in time 0.0007028579711914062\n",
      "409 saved in time 0.0005831718444824219\n",
      "410 saved in time 0.0005114078521728516\n",
      "411 saved in time 0.000804901123046875\n",
      "412 saved in time 0.0005977153778076172\n",
      "413 saved in time 0.0007014274597167969\n",
      "414 saved in time 0.0004985332489013672\n",
      "415 saved in time 0.0005533695220947266\n",
      "416 saved in time 0.0006115436553955078\n",
      "417 saved in time 0.0005183219909667969\n",
      "418 saved in time 0.0005147457122802734\n",
      "419 saved in time 0.0005939006805419922\n",
      "420 saved in time 0.0005283355712890625\n",
      "421 saved in time 0.0003795623779296875\n",
      "422 saved in time 0.0005857944488525391\n",
      "423 saved in time 0.0006420612335205078\n",
      "424 saved in time 0.0006146430969238281\n",
      "425 saved in time 0.0005452632904052734\n",
      "426 saved in time 0.0005214214324951172\n",
      "427 saved in time 0.0007488727569580078\n",
      "428 saved in time 0.0006425380706787109\n",
      "429 saved in time 0.0005657672882080078\n",
      "430 saved in time 0.0006184577941894531\n",
      "431 saved in time 0.0005624294281005859\n",
      "432 saved in time 0.0005733966827392578\n",
      "433 saved in time 0.0005006790161132812\n",
      "434 saved in time 0.0005564689636230469\n",
      "435 saved in time 0.0006794929504394531\n",
      "436 saved in time 0.0004665851593017578\n",
      "437 saved in time 0.0005986690521240234\n",
      "438 saved in time 0.0005600452423095703\n",
      "439 saved in time 0.0005705356597900391\n",
      "440 saved in time 0.0005526542663574219\n",
      "441 saved in time 0.000518798828125\n",
      "442 saved in time 0.000667572021484375\n",
      "443 saved in time 0.0006213188171386719\n",
      "444 saved in time 0.0006315708160400391\n",
      "445 saved in time 0.0006952285766601562\n",
      "446 saved in time 0.0004875659942626953\n",
      "447 saved in time 0.0005745887756347656\n",
      "448 saved in time 0.00047707557678222656\n",
      "449 saved in time 0.0007121562957763672\n",
      "450 saved in time 0.0004730224609375\n",
      "451 saved in time 0.0006008148193359375\n",
      "452 saved in time 0.0004253387451171875\n",
      "453 saved in time 0.0004506111145019531\n",
      "454 saved in time 0.0004949569702148438\n",
      "455 saved in time 0.0005393028259277344\n",
      "456 saved in time 0.00057220458984375\n",
      "457 saved in time 0.0006062984466552734\n",
      "458 saved in time 0.0006783008575439453\n",
      "459 saved in time 0.0006191730499267578\n",
      "460 saved in time 0.000659942626953125\n",
      "461 saved in time 0.0006740093231201172\n",
      "462 saved in time 0.0007750988006591797\n",
      "463 saved in time 0.0006189346313476562\n",
      "464 saved in time 0.0005376338958740234\n",
      "465 saved in time 0.0005862712860107422\n",
      "466 saved in time 0.00040531158447265625\n",
      "467 saved in time 0.0004999637603759766\n",
      "468 saved in time 0.0005550384521484375\n",
      "469 saved in time 0.0005507469177246094\n",
      "470 saved in time 0.00048613548278808594\n",
      "471 saved in time 0.0004596710205078125\n",
      "472 saved in time 0.0006680488586425781\n",
      "473 saved in time 0.0005593299865722656\n",
      "474 saved in time 0.0005526542663574219\n",
      "475 saved in time 0.0005974769592285156\n",
      "476 saved in time 0.0006279945373535156\n",
      "477 saved in time 0.0005075931549072266\n",
      "478 saved in time 0.0005390644073486328\n",
      "479 saved in time 0.0005850791931152344\n",
      "480 saved in time 0.0005354881286621094\n",
      "481 saved in time 0.0005140304565429688\n",
      "482 saved in time 0.0004513263702392578\n",
      "483 saved in time 0.0005297660827636719\n",
      "484 saved in time 0.0006394386291503906\n",
      "485 saved in time 0.0005488395690917969\n",
      "486 saved in time 0.00047206878662109375\n",
      "487 saved in time 0.0006148815155029297\n",
      "488 saved in time 0.0005211830139160156\n",
      "489 saved in time 0.0005860328674316406\n",
      "490 saved in time 0.0005376338958740234\n",
      "491 saved in time 0.0005924701690673828\n",
      "492 saved in time 0.00048279762268066406\n",
      "493 saved in time 0.0005636215209960938\n",
      "494 saved in time 0.00041103363037109375\n",
      "495 saved in time 0.0005848407745361328\n",
      "496 saved in time 0.0005326271057128906\n",
      "497 saved in time 0.000530242919921875\n",
      "498 saved in time 0.0006928443908691406\n",
      "499 saved in time 0.0005495548248291016\n",
      "500 saved in time 0.0005822181701660156\n",
      "501 saved in time 0.0004649162292480469\n",
      "502 saved in time 0.0005629062652587891\n",
      "503 saved in time 0.0005085468292236328\n",
      "504 saved in time 0.0005698204040527344\n",
      "505 saved in time 0.0005600452423095703\n",
      "506 saved in time 0.0005824565887451172\n",
      "507 saved in time 0.0005645751953125\n",
      "508 saved in time 0.0005443096160888672\n",
      "509 saved in time 0.0005342960357666016\n",
      "510 saved in time 0.0005674362182617188\n",
      "511 saved in time 0.0006036758422851562\n",
      "512 saved in time 0.0006456375122070312\n",
      "513 saved in time 0.000461578369140625\n",
      "514 saved in time 0.0006814002990722656\n",
      "515 saved in time 0.0006754398345947266\n",
      "516 saved in time 0.0005350112915039062\n",
      "517 saved in time 0.0005068778991699219\n",
      "518 saved in time 0.0006663799285888672\n",
      "519 saved in time 0.0006215572357177734\n",
      "520 saved in time 0.0005991458892822266\n",
      "521 saved in time 0.0006072521209716797\n",
      "522 saved in time 0.0006685256958007812\n",
      "523 saved in time 0.0005776882171630859\n",
      "524 saved in time 0.00048232078552246094\n",
      "525 saved in time 0.00063323974609375\n",
      "526 saved in time 0.0005846023559570312\n",
      "527 saved in time 0.0006594657897949219\n",
      "528 saved in time 0.0006382465362548828\n",
      "529 saved in time 0.0006532669067382812\n",
      "530 saved in time 0.00046825408935546875\n",
      "531 saved in time 0.0005400180816650391\n",
      "532 saved in time 0.0006670951843261719\n",
      "533 saved in time 0.0006673336029052734\n",
      "534 saved in time 0.0005471706390380859\n",
      "535 saved in time 0.0006148815155029297\n",
      "536 saved in time 0.0005674362182617188\n",
      "537 saved in time 0.000545501708984375\n",
      "538 saved in time 0.0005898475646972656\n",
      "539 saved in time 0.0007646083831787109\n",
      "540 saved in time 0.000568389892578125\n",
      "541 saved in time 0.000736236572265625\n",
      "542 saved in time 0.0006091594696044922\n",
      "543 saved in time 0.0005185604095458984\n",
      "544 saved in time 0.0006821155548095703\n",
      "545 saved in time 0.0006875991821289062\n",
      "546 saved in time 0.0005991458892822266\n",
      "547 saved in time 0.0006630420684814453\n",
      "548 saved in time 0.0005004405975341797\n",
      "549 saved in time 0.0006737709045410156\n",
      "550 saved in time 0.0005764961242675781\n",
      "551 saved in time 0.00041937828063964844\n",
      "552 saved in time 0.0005283355712890625\n",
      "553 saved in time 0.0007605552673339844\n",
      "554 saved in time 0.0005803108215332031\n",
      "555 saved in time 0.0005028247833251953\n",
      "556 saved in time 0.00043654441833496094\n",
      "557 saved in time 0.0006711483001708984\n",
      "558 saved in time 0.0007228851318359375\n",
      "559 saved in time 0.0006344318389892578\n",
      "560 saved in time 0.0005350112915039062\n",
      "561 saved in time 0.0005793571472167969\n",
      "562 saved in time 0.00051116943359375\n",
      "563 saved in time 0.0008139610290527344\n",
      "564 saved in time 0.0004210472106933594\n",
      "565 saved in time 0.0005018711090087891\n",
      "566 saved in time 0.0006496906280517578\n",
      "567 saved in time 0.0004756450653076172\n",
      "568 saved in time 0.0004265308380126953\n",
      "569 saved in time 0.0005552768707275391\n",
      "570 saved in time 0.0006794929504394531\n",
      "571 saved in time 0.0006299018859863281\n",
      "572 saved in time 0.0006444454193115234\n",
      "573 saved in time 0.0005583763122558594\n",
      "574 saved in time 0.0006775856018066406\n",
      "575 saved in time 0.0006182193756103516\n",
      "576 saved in time 0.0006220340728759766\n",
      "577 saved in time 0.0007214546203613281\n",
      "578 saved in time 0.0006000995635986328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579 saved in time 0.0007283687591552734\n",
      "580 saved in time 0.00061798095703125\n",
      "581 saved in time 0.0007302761077880859\n",
      "582 saved in time 0.0005774497985839844\n",
      "583 saved in time 0.000484466552734375\n",
      "584 saved in time 0.0005459785461425781\n",
      "585 saved in time 0.0004937648773193359\n",
      "586 saved in time 0.000568389892578125\n",
      "587 saved in time 0.0006349086761474609\n",
      "588 saved in time 0.0004875659942626953\n",
      "589 saved in time 0.0006122589111328125\n",
      "590 saved in time 0.0005452632904052734\n",
      "591 saved in time 0.0005803108215332031\n",
      "592 saved in time 0.0005762577056884766\n",
      "593 saved in time 0.0006225109100341797\n",
      "594 saved in time 0.0005397796630859375\n",
      "595 saved in time 0.0005300045013427734\n",
      "596 saved in time 0.0005548000335693359\n",
      "597 saved in time 0.0005755424499511719\n",
      "598 saved in time 0.0005104541778564453\n",
      "599 saved in time 0.0004775524139404297\n",
      "600 saved in time 0.0005898475646972656\n",
      "601 saved in time 0.0005791187286376953\n",
      "602 saved in time 0.0005290508270263672\n",
      "603 saved in time 0.0005464553833007812\n",
      "604 saved in time 0.0005855560302734375\n",
      "605 saved in time 0.0007753372192382812\n",
      "606 saved in time 0.0006146430969238281\n",
      "607 saved in time 0.0005877017974853516\n",
      "608 saved in time 0.0006392002105712891\n",
      "609 saved in time 0.0007174015045166016\n",
      "610 saved in time 0.0006814002990722656\n",
      "611 saved in time 0.000690460205078125\n",
      "612 saved in time 0.0006277561187744141\n",
      "613 saved in time 0.0006067752838134766\n",
      "614 saved in time 0.0006568431854248047\n",
      "615 saved in time 0.0006349086761474609\n",
      "616 saved in time 0.0003788471221923828\n",
      "617 saved in time 0.000720977783203125\n",
      "618 saved in time 0.00045561790466308594\n",
      "619 saved in time 0.0006415843963623047\n",
      "620 saved in time 0.0006630420684814453\n",
      "621 saved in time 0.0007061958312988281\n",
      "622 saved in time 0.0006175041198730469\n",
      "623 saved in time 0.0005695819854736328\n",
      "624 saved in time 0.0006008148193359375\n",
      "625 saved in time 0.0007417201995849609\n",
      "626 saved in time 0.0005478858947753906\n",
      "627 saved in time 0.0004975795745849609\n",
      "628 saved in time 0.0006611347198486328\n",
      "629 saved in time 0.0006692409515380859\n",
      "630 saved in time 0.0007534027099609375\n",
      "631 saved in time 0.0006654262542724609\n",
      "632 saved in time 0.0007810592651367188\n",
      "633 saved in time 0.0006170272827148438\n",
      "634 saved in time 0.0004279613494873047\n",
      "635 saved in time 0.0005705356597900391\n",
      "636 saved in time 0.000560760498046875\n",
      "637 saved in time 0.0006909370422363281\n",
      "638 saved in time 0.00067138671875\n",
      "639 saved in time 0.0005764961242675781\n",
      "640 saved in time 0.0005483627319335938\n",
      "641 saved in time 0.0005643367767333984\n",
      "642 saved in time 0.0006463527679443359\n",
      "643 saved in time 0.0006008148193359375\n",
      "644 saved in time 0.0005257129669189453\n",
      "645 saved in time 0.0005402565002441406\n",
      "646 saved in time 0.0004730224609375\n",
      "647 saved in time 0.0005967617034912109\n",
      "648 saved in time 0.0005791187286376953\n",
      "649 saved in time 0.0006976127624511719\n",
      "650 saved in time 0.0005016326904296875\n",
      "651 saved in time 0.0006630420684814453\n",
      "652 saved in time 0.00047659873962402344\n",
      "653 saved in time 0.0006442070007324219\n",
      "654 saved in time 0.0006020069122314453\n",
      "655 saved in time 0.0007576942443847656\n",
      "656 saved in time 0.0007443428039550781\n",
      "657 saved in time 0.0005900859832763672\n",
      "658 saved in time 0.0005922317504882812\n",
      "659 saved in time 0.0006353855133056641\n",
      "660 saved in time 0.0005037784576416016\n",
      "661 saved in time 0.0004906654357910156\n",
      "662 saved in time 0.0007052421569824219\n",
      "663 saved in time 0.0007579326629638672\n",
      "664 saved in time 0.0005049705505371094\n",
      "665 saved in time 0.0006480216979980469\n",
      "666 saved in time 0.0005588531494140625\n",
      "667 saved in time 0.0008087158203125\n",
      "668 saved in time 0.0005548000335693359\n",
      "669 saved in time 0.0004792213439941406\n",
      "670 saved in time 0.000701904296875\n",
      "671 saved in time 0.0005419254302978516\n",
      "672 saved in time 0.0006120204925537109\n",
      "673 saved in time 0.0006020069122314453\n",
      "674 saved in time 0.00045609474182128906\n",
      "675 saved in time 0.0006520748138427734\n",
      "676 saved in time 0.0004928112030029297\n",
      "677 saved in time 0.0005810260772705078\n",
      "678 saved in time 0.0006134510040283203\n",
      "679 saved in time 0.0005376338958740234\n",
      "680 saved in time 0.0004458427429199219\n",
      "681 saved in time 0.0006263256072998047\n",
      "682 saved in time 0.0005269050598144531\n",
      "683 saved in time 0.0005786418914794922\n",
      "684 saved in time 0.0004999637603759766\n",
      "685 saved in time 0.0005178451538085938\n",
      "686 saved in time 0.0006377696990966797\n",
      "687 saved in time 0.0005478858947753906\n",
      "688 saved in time 0.0006656646728515625\n",
      "689 saved in time 0.0006427764892578125\n",
      "690 saved in time 0.0005962848663330078\n",
      "691 saved in time 0.0006268024444580078\n",
      "692 saved in time 0.00035881996154785156\n",
      "693 saved in time 0.0005583763122558594\n",
      "694 saved in time 0.0005066394805908203\n",
      "695 saved in time 0.0005950927734375\n",
      "696 saved in time 0.0006389617919921875\n",
      "697 saved in time 0.0005443096160888672\n",
      "698 saved in time 0.0004355907440185547\n",
      "699 saved in time 0.0006036758422851562\n",
      "700 saved in time 0.0004830360412597656\n",
      "701 saved in time 0.0005786418914794922\n",
      "702 saved in time 0.0005745887756347656\n",
      "703 saved in time 0.0005030632019042969\n",
      "704 saved in time 0.0004885196685791016\n",
      "705 saved in time 0.0005931854248046875\n",
      "706 saved in time 0.0006837844848632812\n",
      "707 saved in time 0.0006556510925292969\n",
      "708 saved in time 0.0007393360137939453\n",
      "709 saved in time 0.0005776882171630859\n",
      "710 saved in time 0.0005254745483398438\n",
      "711 saved in time 0.0007066726684570312\n",
      "712 saved in time 0.0005803108215332031\n",
      "713 saved in time 0.0006084442138671875\n",
      "714 saved in time 0.0006718635559082031\n",
      "715 saved in time 0.0006356239318847656\n",
      "716 saved in time 0.0006124973297119141\n",
      "717 saved in time 0.0005476474761962891\n",
      "718 saved in time 0.0005049705505371094\n",
      "719 saved in time 0.0005528926849365234\n",
      "720 saved in time 0.0005342960357666016\n",
      "721 saved in time 0.0004987716674804688\n",
      "722 saved in time 0.0005738735198974609\n",
      "723 saved in time 0.0006008148193359375\n",
      "724 saved in time 0.0005738735198974609\n",
      "725 saved in time 0.0006923675537109375\n",
      "726 saved in time 0.0005395412445068359\n",
      "727 saved in time 0.0006718635559082031\n",
      "728 saved in time 0.000530242919921875\n",
      "729 saved in time 0.0006465911865234375\n",
      "730 saved in time 0.0006110668182373047\n",
      "731 saved in time 0.0005786418914794922\n",
      "732 saved in time 0.0007138252258300781\n",
      "733 saved in time 0.0005712509155273438\n",
      "734 saved in time 0.0005145072937011719\n",
      "735 saved in time 0.00041103363037109375\n",
      "736 saved in time 0.0004220008850097656\n",
      "737 saved in time 0.0005893707275390625\n",
      "738 saved in time 0.00048732757568359375\n",
      "739 saved in time 0.0006029605865478516\n",
      "740 saved in time 0.0006697177886962891\n",
      "741 saved in time 0.0006005764007568359\n",
      "742 saved in time 0.0006318092346191406\n",
      "743 saved in time 0.0005831718444824219\n",
      "744 saved in time 0.0006768703460693359\n",
      "745 saved in time 0.0006721019744873047\n",
      "746 saved in time 0.0005161762237548828\n",
      "747 saved in time 0.0006055831909179688\n",
      "748 saved in time 0.0005059242248535156\n",
      "749 saved in time 0.0005345344543457031\n",
      "750 saved in time 0.000518798828125\n",
      "751 saved in time 0.0004432201385498047\n",
      "752 saved in time 0.0004191398620605469\n",
      "753 saved in time 0.0005993843078613281\n",
      "754 saved in time 0.0004582405090332031\n",
      "755 saved in time 0.0005145072937011719\n",
      "756 saved in time 0.0006372928619384766\n",
      "757 saved in time 0.0005397796630859375\n",
      "758 saved in time 0.0004265308380126953\n",
      "759 saved in time 0.0005657672882080078\n",
      "760 saved in time 0.0005753040313720703\n",
      "761 saved in time 0.0006377696990966797\n",
      "762 saved in time 0.0005176067352294922\n",
      "763 saved in time 0.0007114410400390625\n",
      "764 saved in time 0.0004374980926513672\n",
      "765 saved in time 0.0005786418914794922\n",
      "766 saved in time 0.0005009174346923828\n",
      "767 saved in time 0.0006811618804931641\n",
      "768 saved in time 0.0006542205810546875\n",
      "769 saved in time 0.0005905628204345703\n",
      "770 saved in time 0.0005123615264892578\n",
      "771 saved in time 0.0005066394805908203\n",
      "772 saved in time 0.0005881786346435547\n",
      "773 saved in time 0.0006458759307861328\n",
      "774 saved in time 0.0006475448608398438\n",
      "775 saved in time 0.0006964206695556641\n",
      "776 saved in time 0.00054168701171875\n",
      "777 saved in time 0.0007231235504150391\n",
      "778 saved in time 0.0007040500640869141\n",
      "779 saved in time 0.0006041526794433594\n",
      "780 saved in time 0.0005807876586914062\n",
      "781 saved in time 0.0005064010620117188\n",
      "782 saved in time 0.0006053447723388672\n",
      "783 saved in time 0.0005588531494140625\n",
      "784 saved in time 0.0005791187286376953\n",
      "785 saved in time 0.0005004405975341797\n",
      "786 saved in time 0.0005700588226318359\n",
      "787 saved in time 0.0005826950073242188\n",
      "788 saved in time 0.0005333423614501953\n",
      "789 saved in time 0.0005080699920654297\n",
      "790 saved in time 0.0005509853363037109\n",
      "791 saved in time 0.0005805492401123047\n",
      "792 saved in time 0.00064849853515625\n",
      "793 saved in time 0.0006794929504394531\n",
      "794 saved in time 0.0005180835723876953\n",
      "795 saved in time 0.0005910396575927734\n",
      "796 saved in time 0.0005664825439453125\n",
      "797 saved in time 0.0005986690521240234\n",
      "798 saved in time 0.0005748271942138672\n",
      "799 saved in time 0.0007116794586181641\n",
      "800 saved in time 0.0005733966827392578\n",
      "801 saved in time 0.0005412101745605469\n",
      "802 saved in time 0.0006592273712158203\n",
      "803 saved in time 0.0005345344543457031\n",
      "804 saved in time 0.0004730224609375\n",
      "805 saved in time 0.0005512237548828125\n",
      "806 saved in time 0.0005316734313964844\n",
      "807 saved in time 0.0006518363952636719\n",
      "808 saved in time 0.0006978511810302734\n",
      "809 saved in time 0.0005755424499511719\n",
      "810 saved in time 0.0005738735198974609\n",
      "811 saved in time 0.0007054805755615234\n",
      "812 saved in time 0.0004115104675292969\n",
      "813 saved in time 0.0006952285766601562\n",
      "814 saved in time 0.00048351287841796875\n",
      "815 saved in time 0.0006041526794433594\n",
      "816 saved in time 0.0005004405975341797\n",
      "817 saved in time 0.0004875659942626953\n",
      "818 saved in time 0.00048732757568359375\n",
      "819 saved in time 0.0006299018859863281\n",
      "820 saved in time 0.0004973411560058594\n",
      "821 saved in time 0.0006346702575683594\n",
      "822 saved in time 0.0005309581756591797\n",
      "823 saved in time 0.0006089210510253906\n",
      "824 saved in time 0.0005705356597900391\n",
      "825 saved in time 0.0006709098815917969\n",
      "826 saved in time 0.0004901885986328125\n",
      "827 saved in time 0.00046944618225097656\n",
      "828 saved in time 0.0006272792816162109\n",
      "829 saved in time 0.0005099773406982422\n",
      "830 saved in time 0.0004787445068359375\n",
      "831 saved in time 0.0006923675537109375\n",
      "832 saved in time 0.0004038810729980469\n",
      "833 saved in time 0.0005939006805419922\n",
      "834 saved in time 0.0005600452423095703\n",
      "835 saved in time 0.0006754398345947266\n",
      "836 saved in time 0.0004906654357910156\n",
      "837 saved in time 0.0005688667297363281\n",
      "838 saved in time 0.0006160736083984375\n",
      "839 saved in time 0.0006334781646728516\n",
      "840 saved in time 0.0005984306335449219\n",
      "841 saved in time 0.0004930496215820312\n",
      "842 saved in time 0.0006318092346191406\n",
      "843 saved in time 0.0005080699920654297\n",
      "844 saved in time 0.0004813671112060547\n",
      "845 saved in time 0.0006365776062011719\n",
      "846 saved in time 0.0005064010620117188\n",
      "847 saved in time 0.0003418922424316406\n",
      "848 saved in time 0.00060272216796875\n",
      "849 saved in time 0.0005700588226318359\n",
      "850 saved in time 0.0005192756652832031\n",
      "851 saved in time 0.0005674362182617188\n",
      "852 saved in time 0.0004878044128417969\n",
      "853 saved in time 0.0005660057067871094\n",
      "854 saved in time 0.0006072521209716797\n",
      "855 saved in time 0.0006175041198730469\n",
      "856 saved in time 0.00045228004455566406\n",
      "857 saved in time 0.0005826950073242188\n",
      "858 saved in time 0.0004794597625732422\n",
      "859 saved in time 0.0006198883056640625\n",
      "860 saved in time 0.0005183219909667969\n",
      "861 saved in time 0.0006656646728515625\n",
      "862 saved in time 0.0005650520324707031\n",
      "863 saved in time 0.0006339550018310547\n",
      "864 saved in time 0.0005457401275634766\n",
      "865 saved in time 0.0006732940673828125\n",
      "866 saved in time 0.0005626678466796875\n",
      "867 saved in time 0.0004999637603759766\n",
      "868 saved in time 0.0006153583526611328\n",
      "869 saved in time 0.0006732940673828125\n",
      "870 saved in time 0.00047087669372558594\n",
      "871 saved in time 0.0005829334259033203\n",
      "872 saved in time 0.0006844997406005859\n",
      "873 saved in time 0.0004825592041015625\n",
      "874 saved in time 0.0006031990051269531\n",
      "875 saved in time 0.0005314350128173828\n",
      "876 saved in time 0.00047969818115234375\n",
      "877 saved in time 0.0005514621734619141\n",
      "878 saved in time 0.0005877017974853516\n",
      "879 saved in time 0.0005586147308349609\n",
      "880 saved in time 0.0004413127899169922\n",
      "881 saved in time 0.0005762577056884766\n",
      "882 saved in time 0.0004138946533203125\n",
      "883 saved in time 0.0004184246063232422\n",
      "884 saved in time 0.00035953521728515625\n",
      "885 saved in time 0.0006976127624511719\n",
      "886 saved in time 0.0006000995635986328\n",
      "887 saved in time 0.0005133152008056641\n",
      "888 saved in time 0.0005004405975341797\n",
      "889 saved in time 0.0005486011505126953\n",
      "890 saved in time 0.0006296634674072266\n",
      "891 saved in time 0.0006580352783203125\n",
      "892 saved in time 0.0003807544708251953\n",
      "893 saved in time 0.0006480216979980469\n",
      "894 saved in time 0.0003829002380371094\n",
      "895 saved in time 0.0005333423614501953\n",
      "896 saved in time 0.00040459632873535156\n",
      "897 saved in time 0.00046753883361816406\n",
      "898 saved in time 0.0005726814270019531\n",
      "899 saved in time 0.0007352828979492188\n",
      "900 saved in time 0.0003924369812011719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901 saved in time 0.0005047321319580078\n",
      "902 saved in time 0.00043845176696777344\n",
      "903 saved in time 0.0006113052368164062\n",
      "904 saved in time 0.0004611015319824219\n",
      "905 saved in time 0.0005676746368408203\n",
      "906 saved in time 0.0005767345428466797\n",
      "907 saved in time 0.0006110668182373047\n",
      "908 saved in time 0.0004892349243164062\n",
      "909 saved in time 0.0006556510925292969\n",
      "910 saved in time 0.0004911422729492188\n",
      "911 saved in time 0.0004258155822753906\n",
      "912 saved in time 0.0006051063537597656\n",
      "913 saved in time 0.0004315376281738281\n",
      "914 saved in time 0.0004937648773193359\n",
      "915 saved in time 0.0005159378051757812\n",
      "916 saved in time 0.000453948974609375\n",
      "917 saved in time 0.0005321502685546875\n",
      "918 saved in time 0.0005867481231689453\n",
      "919 saved in time 0.0006082057952880859\n",
      "920 saved in time 0.0007765293121337891\n",
      "921 saved in time 0.0004868507385253906\n",
      "922 saved in time 0.0016207695007324219\n",
      "923 saved in time 0.0007171630859375\n",
      "924 saved in time 0.0005331039428710938\n",
      "925 saved in time 0.0006148815155029297\n",
      "926 saved in time 0.0006275177001953125\n",
      "927 saved in time 0.0004887580871582031\n",
      "928 saved in time 0.0004487037658691406\n",
      "929 saved in time 0.0005393028259277344\n",
      "930 saved in time 0.0017788410186767578\n",
      "931 saved in time 0.0009558200836181641\n",
      "932 saved in time 0.0005855560302734375\n",
      "933 saved in time 0.0006167888641357422\n",
      "934 saved in time 0.0005567073822021484\n",
      "935 saved in time 0.0022132396697998047\n",
      "936 saved in time 0.0007321834564208984\n",
      "937 saved in time 0.0021402835845947266\n",
      "938 saved in time 0.0008251667022705078\n",
      "939 saved in time 0.0005774497985839844\n",
      "940 saved in time 0.0007221698760986328\n",
      "941 saved in time 0.0006377696990966797\n",
      "942 saved in time 0.0004811286926269531\n",
      "943 saved in time 0.0005540847778320312\n",
      "944 saved in time 0.0005545616149902344\n",
      "945 saved in time 0.0009474754333496094\n",
      "946 saved in time 0.0005245208740234375\n",
      "947 saved in time 0.0005102157592773438\n",
      "948 saved in time 0.0005588531494140625\n",
      "949 saved in time 0.0005815029144287109\n",
      "950 saved in time 0.0005314350128173828\n",
      "951 saved in time 0.0005846023559570312\n",
      "952 saved in time 0.0005807876586914062\n",
      "953 saved in time 0.0004439353942871094\n",
      "954 saved in time 0.00046372413635253906\n",
      "955 saved in time 0.0005543231964111328\n",
      "956 saved in time 0.0005640983581542969\n",
      "957 saved in time 0.0006413459777832031\n",
      "958 saved in time 0.0006089210510253906\n",
      "959 saved in time 0.0005695819854736328\n",
      "960 saved in time 0.0006129741668701172\n",
      "961 saved in time 0.0005958080291748047\n",
      "962 saved in time 0.0006723403930664062\n",
      "963 saved in time 0.0006017684936523438\n",
      "964 saved in time 0.0005190372467041016\n",
      "965 saved in time 0.0006580352783203125\n",
      "966 saved in time 0.0006649494171142578\n",
      "967 saved in time 0.0007443428039550781\n",
      "968 saved in time 0.0004374980926513672\n",
      "969 saved in time 0.0004756450653076172\n",
      "970 saved in time 0.0006511211395263672\n",
      "971 saved in time 0.0028467178344726562\n",
      "972 saved in time 0.0005292892456054688\n",
      "973 saved in time 0.0005774497985839844\n",
      "974 saved in time 1.0968997478485107\n",
      "975 saved in time 1.169546365737915\n",
      "976 saved in time 0.7391095161437988\n",
      "977 saved in time 0.7897295951843262\n",
      "978 saved in time 0.9681828022003174\n",
      "979 saved in time 0.7414846420288086\n",
      "980 saved in time 1.1249868869781494\n",
      "981 saved in time 1.2876060009002686\n",
      "982 saved in time 0.618781566619873\n",
      "983 saved in time 0.911632776260376\n",
      "984 saved in time 1.109588861465454\n",
      "985 saved in time 0.9032087326049805\n",
      "986 saved in time 1.256899356842041\n",
      "987 saved in time 1.0266191959381104\n",
      "988 saved in time 1.1290557384490967\n",
      "989 saved in time 1.460960865020752\n",
      "990 saved in time 1.1202938556671143\n",
      "991 saved in time 1.0374188423156738\n",
      "992 saved in time 0.9566404819488525\n",
      "993 saved in time 1.3823676109313965\n",
      "994 saved in time 1.393603801727295\n",
      "995 saved in time 1.0352799892425537\n",
      "996 saved in time 0.8871860504150391\n",
      "997 saved in time 1.1080093383789062\n",
      "998 saved in time 0.9964513778686523\n",
      "999 saved in time 0.9819855690002441\n",
      "CPU times: user 1min 12s, sys: 7.16 s, total: 1min 19s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(train_dataset):\n",
    "        tic = tt()\n",
    "        if not os.path.exists(os.path.join(save_dir, batch.event_file[-4:])):\n",
    "            data = batch.to(device)\n",
    "            emb = (None if (hparams[\"emb_channels\"] == 0) \n",
    "                       else data.embedding) \n",
    "\n",
    "            cut_list = []\n",
    "            for j in range(sections):\n",
    "                subset_ind = torch.chunk(torch.arange(data.e_radius.shape[1]), sections)[j]\n",
    "#                 print(subset_ind)\n",
    "                output = model(torch.cat([data.cell_data, data.x], axis=-1), data.e_radius[:, subset_ind], emb).squeeze() if ('ci' in hparams[\"regime\"]) else model(data.x, data.e_radius[:, subset_ind], emb).squeeze()\n",
    "                cut = F.sigmoid(output) > 0.35\n",
    "                cut_list.append(cut)\n",
    "\n",
    "            cut_list = torch.cat(cut_list)\n",
    "            batch.edge_index = batch.e_radius[:, cut_list]\n",
    "            batch.e_radius = None\n",
    "            batch.embedding = None\n",
    "    #         batch.x = batch.x.cpu()\n",
    "    #         batch.y = torch.from_numpy(y[combined_indices]).float()\n",
    "            batch.y = batch.y[cut_list]\n",
    "\n",
    "            with open(os.path.join(save_dir, batch.event_file[-4:]), 'wb') as pickle_file:\n",
    "                torch.save(batch, pickle_file)\n",
    "\n",
    "        print(i, \"saved in time\", tt()-tic)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load & Save Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pt_cut = 0\n",
    "train_number = 1000\n",
    "test_number = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_dir = \"/global/cscratch1/sd/danieltm/test_20200930/trackml_processed/graphs_processed/\"\n",
    "# basename = os.path.join(save_dir, str(pt_cut) + \"_pt_cut_endcaps\")\n",
    "basename = os.path.join(save_dir, str(pt_cut) + \"_pt_cut\")\n",
    "load_path = os.path.join(basename, \"all_events\")\n",
    "all_events = os.listdir(load_path)\n",
    "all_events = sorted([os.path.join(load_path, event) for event in all_events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.47 s, sys: 5.14 s, total: 7.6 s\n",
      "Wall time: 8.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = [torch.load(event, map_location=\"cpu\") for event in all_events[:train_number]]\n",
    "test_dataset = [torch.load(event, map_location=\"cpu\") for event in all_events[-test_number:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 221 ms, sys: 7.52 s, total: 7.74 s\n",
      "Wall time: 7.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(os.path.join(basename, str(train_number) + \"_events_train.pkl\"), 'wb') as pickle_file:\n",
    "#     pickle.dump(train_dataset, pickle_file)\n",
    "    torch.save(train_dataset, pickle_file)\n",
    "with open(os.path.join(basename, str(test_number) + \"_events_test.pkl\"), 'wb') as pickle_file:\n",
    "#     pickle.dump(train_dataset, pickle_file)\n",
    "    torch.save(test_dataset, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load Scrubbed Filter-ready Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "No performance gain from prebuilt torch pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"../configs/dev_gnn.yaml\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(input_dir, num):\n",
    "    all_events = os.listdir(input_dir)\n",
    "    all_events = sorted([os.path.join(input_dir, event) for event in all_events])\n",
    "    loaded_events = [torch.load(event, map_location=torch.device('cpu')) for event in all_events[:num]]\n",
    "\n",
    "    return loaded_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class GraphDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Assign hyperparameters\n",
    "        self.hparams = hparams\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        # called only on 1 GPU\n",
    "        datatypes = [\"train\", \"val\", \"test\"]\n",
    "        input_dirs = [os.path.join(self.hparams[\"input_dir\"], datatype) for datatype in datatypes]\n",
    "        self.trainset, self.valset, self.testset = [load_dataset(input_dir, self.hparams[\"train_split\"][i]) for i, input_dir in enumerate(input_dirs)]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if len(self.trainset) > 0:\n",
    "            return DataLoader(self.trainset, batch_size=1, num_workers=1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if len(self.valset) > 0:\n",
    "            return DataLoader(self.valset, batch_size=1, num_workers=1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if len(self.testset) > 0:\n",
    "            return DataLoader(self.testset, batch_size=1, num_workers=1)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "class SplitGraphDataModule(GraphDataModule):\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        # called only on 1 GPU\n",
    "        datatypes = [\"train_split\", \"val\", \"test\"]\n",
    "        input_dirs = [os.path.join(self.hparams[\"input_dir\"], datatype) for datatype in datatypes]\n",
    "        self.trainset, self.valset, self.testset = [load_dataset(input_dir, self.hparams[\"train_split\"][i]) for i, input_dir in enumerate(input_dirs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.2 ms, sys: 221 ms, total: 274 ms\n",
      "Wall time: 493 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph_dm = GraphDataModule(config)\n",
    "# graph_dm = SplitGraphDataModule(config)\n",
    "graph_dm.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = graph_dm.train_dataloader().dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sampling Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def random_edge_slice(delta_phi, batch):\n",
    "    # 1. Select random phi\n",
    "    random_phi = np.random.rand()*2 - 1\n",
    "    e = batch.edge_index.to('cpu')\n",
    "    x = batch.x.to('cpu')\n",
    "\n",
    "    # 2. Find hits within delta_phi of random_phi\n",
    "    dif = abs(x[:,1] - random_phi)\n",
    "    subset_hits = np.where((dif < delta_phi) | ((2-dif) < delta_phi))[0]\n",
    "\n",
    "    # 3. Filter edges with subset_hits\n",
    "    subset_edges_ind = (np.isin(e[0], subset_hits) | np.isin(e[1], subset_hits))\n",
    "\n",
    "    subset_hits = np.unique(e[:, subset_edges_ind])\n",
    "    subset_edges_extended = (np.isin(e[0], subset_hits) | np.isin(e[1], subset_hits))\n",
    "    nested_ind = np.isin(np.where(subset_edges_extended)[0], np.where(subset_edges_ind)[0])\n",
    "    \n",
    "    return subset_edges_ind, subset_edges_extended, nested_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def random_edge_slice_v2(delta_phi, batch):\n",
    "    # 1. Select random phi\n",
    "    random_phi = np.random.rand()*2 - 1\n",
    "    e = batch.edge_index.to('cpu').numpy()\n",
    "    x = batch.x.to('cpu')\n",
    "\n",
    "    # 2. Find edges within delta_phi of random_phi\n",
    "    e_average = (x[e[0], 1] + x[e[1], 1])/2\n",
    "    dif = abs(e_average - random_phi)\n",
    "    subset_edges = ((dif < delta_phi) | ((2-dif) < delta_phi)).numpy()\n",
    "\n",
    "    # 3. Find connected edges to this subset   \n",
    "    e_ones = cp.array([1]*e_length).astype('Float32')\n",
    "    subset_ones = cp.array([1]*subset_edges.sum()).astype('Float32')\n",
    "\n",
    "    e_csr_in = cp.sparse.coo_matrix((e_ones, (cp.array(e[0]).astype('Float32'), cp.arange(e_length).astype('Float32'))), shape=(e.max()+1,e_length)).tocsr()\n",
    "    e_csr_out = cp.sparse.coo_matrix((e_ones, (cp.array(e[0]).astype('Float32'), cp.arange(e_length).astype('Float32'))), shape=(e.max()+1,e_length)).tocsr()\n",
    "    e_csr = e_csr_in + e_csr_out\n",
    "\n",
    "    subset_csr_in = cp.sparse.coo_matrix((subset_ones, (cp.array(e[0, subset_edges]).astype('Float32'), cp.arange(e_length)[subset_edges].astype('Float32'))), shape=(e.max()+1,e_length)).tocsr()\n",
    "    subset_csr_out = cp.sparse.coo_matrix((subset_ones, (cp.array(e[0, subset_edges]).astype('Float32'), cp.arange(e_length)[subset_edges].astype('Float32'))), shape=(e.max()+1,e_length)).tocsr()\n",
    "    subset_csr = subset_csr_in + subset_csr_out\n",
    "\n",
    "    summed = (subset_csr.T * e_csr).sum(axis=0)\n",
    "    subset_edges_extended = (summed>0)[0].get()\n",
    "    \n",
    "    return subset_edges, subset_edges_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_edges(phi_sections, batch):\n",
    "    # 1. Loop over 0 -> dPhi -> 1\n",
    "    delta_phi =  1/phi_sections\n",
    "    for phi in np.arange(0, 1, delta_phi):\n",
    "        random_phi = np.random.rand()*2 - 1\n",
    "        e = batch.edge_index.to('cpu')\n",
    "        x = batch.x.to('cpu')\n",
    "\n",
    "        # 2. Find hits within delta_phi of random_phi\n",
    "        dif = abs(x[:,1] - random_phi)\n",
    "        subset_hits = np.where((dif < delta_phi) | ((2-dif) < delta_phi))[0]\n",
    "\n",
    "        # 3. Filter edges with subset_hits\n",
    "        subset_edges_ind = (np.isin(e[0], subset_hits) | np.isin(e[1], subset_hits))\n",
    "\n",
    "        subset_hits = np.unique(e[:, subset_edges_ind])\n",
    "        subset_edges_extended = (np.isin(e[0], subset_hits) | np.isin(e[1], subset_hits))\n",
    "        nested_ind = np.isin(np.where(subset_edges_extended)[0], np.where(subset_edges_ind)[0])\n",
    "    \n",
    "    return subset_edges_ind, subset_edges_extended, nested_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch = graph_dm.val_dataloader().dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 453 ms, sys: 441 s, total: 453 ms\n",
      "Wall time: 424 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "subsets = random_edge_slice(0.1, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 234 ms, sys: 28.8 ms, total: 263 ms\n",
      "Wall time: 235 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.local/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "  del sys.path[0]\n",
      "/global/homes/d/danieltm/.local/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "  \n",
      "/global/homes/d/danieltm/.local/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "  app.launch_new_instance()\n",
      "/global/homes/d/danieltm/.local/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "/global/homes/d/danieltm/.local/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "/global/homes/d/danieltm/.local/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "subsets = random_edge_slice_v2(0.1, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.19 s, sys: 281 s, total: 5.19 s\n",
      "Wall time: 4.87 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([False,  True, False, ..., False, False, False]),\n",
       " array([ True,  True,  True, ..., False, False,  True]),\n",
       " array([False,  True, False, ..., False, False, False]))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "split_edges(12, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [
     0,
     10
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class GNNBase(LightningModule):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Initialise the Lightning Module that can scan over different GNN training regimes\n",
    "        '''\n",
    "        # Assign hyperparameters\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = [torch.optim.AdamW(self.parameters(), lr=(self.hparams[\"lr\"]), betas=(0.9, 0.999), eps=1e-08, amsgrad=True)]\n",
    "        scheduler = [\n",
    "            {\n",
    "                'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer[0], factor=self.hparams[\"factor\"], patience=self.hparams[\"patience\"]),\n",
    "                'monitor': 'checkpoint_on',\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        ]\n",
    "#         scheduler = [torch.optim.lr_scheduler.StepLR(optimizer[0], step_size=1, gamma=0.3)]\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        weight = (torch.tensor(self.hparams[\"weight\"]) if (\"weight\" in self.hparams)\n",
    "                      else torch.tensor((~batch.y_pid.bool()).sum() / batch.y_pid.sum()))\n",
    "\n",
    "        output = (self(torch.cat([batch.cell_data, batch.x], axis=-1), \n",
    "                       batch.edge_index).squeeze()\n",
    "                  if ('ci' in self.hparams[\"regime\"])\n",
    "                  else self(batch.x, batch.edge_index).squeeze())\n",
    "\n",
    "        \n",
    "        if ('pid' in self.hparams[\"regime\"]):\n",
    "            y_pid = (batch.pid[batch.edge_index[0, batch.nested_ind[0]]] == batch.pid[batch.edge_index[1, batch.nested_ind[0]]]).float()\n",
    "            loss = F.binary_cross_entropy_with_logits(output[batch.nested_ind[0]], y_pid.float(), pos_weight = weight)\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy_with_logits(output[batch.nested_ind[0]], batch.y[batch.nested_ind[0]], pos_weight = weight)\n",
    "            \n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        weight = (torch.tensor(self.hparams[\"weight\"]) if (\"weight\" in self.hparams)\n",
    "                      else torch.tensor((~batch.y_pid.bool()).sum() / batch.y_pid.sum()))\n",
    "\n",
    "        output = (self(torch.cat([batch.cell_data, batch.x], axis=-1), batch.edge_index).squeeze()\n",
    "                  if ('ci' in self.hparams[\"regime\"])\n",
    "                  else self(batch.x, batch.edge_index).squeeze())\n",
    "\n",
    "        if ('pid' in self.hparams[\"regime\"]):\n",
    "            y_pid = (batch.pid[batch.edge_index[0]] == batch.pid[batch.edge_index[1]]).float()\n",
    "            val_loss = F.binary_cross_entropy_with_logits(output, y_pid.float(), pos_weight = weight)\n",
    "        else:\n",
    "            val_loss = F.binary_cross_entropy_with_logits(output, batch.y, pos_weight = weight)\n",
    "\n",
    "        result = pl.EvalResult(checkpoint_on=val_loss)\n",
    "        result.log('val_loss', val_loss)\n",
    "\n",
    "        #Edge filter performance\n",
    "        preds = F.sigmoid(output) > 0.5 #Maybe send to CPU??\n",
    "        edge_positive = preds.sum().float()\n",
    "\n",
    "        if ('pid' in self.hparams[\"regime\"]):\n",
    "            y_pid = batch.pid[batch.edge_index[0]] == batch.pid[batch.edge_index[1]]\n",
    "            edge_true = y_pid.sum().float()\n",
    "            edge_true_positive = (y_pid & preds).sum().float()\n",
    "        else:\n",
    "            edge_true = batch.y.sum()\n",
    "            edge_true_positive = (batch.y.bool() & preds).sum().float()\n",
    "\n",
    "        result.log_dict({'eff': torch.tensor(edge_true_positive/edge_true), 'pur': torch.tensor(edge_true_positive/edge_positive)})\n",
    "\n",
    "        return result\n",
    "\n",
    "    def optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_idx, second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n",
    "        # warm up lr\n",
    "        if (self.hparams[\"warmup\"] is not None) and (self.trainer.global_step < self.hparams[\"warmup\"]):\n",
    "            lr_scale = min(1., float(self.trainer.global_step + 1) / self.hparams[\"warmup\"])\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr_scale * self.hparams[\"lr\"]\n",
    "\n",
    "        # update params\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "class InteractionGNN(GNNBase):\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        '''\n",
    "        Initialise the Lightning Module that can scan over different GNN training regimes\n",
    "        '''\n",
    "\n",
    "        # Setup input network\n",
    "        self.node_encoder = make_mlp(hparams[\"in_channels\"], [hparams[\"hidden\"]],\n",
    "                                      output_activation=hparams[\"hidden_activation\"],\n",
    "                                      layer_norm=hparams[\"layernorm\"])\n",
    "   \n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_encoder = make_mlp(2*(hparams[\"hidden\"]),\n",
    "                                     [hparams[\"hidden\"]]*hparams[\"nb_edge_layer\"],\n",
    "                                     layer_norm=hparams[\"layernorm\"],\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hparams[\"hidden_activation\"])\n",
    "\n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_network = make_mlp(4*hparams[\"hidden\"],\n",
    "                                     [hparams[\"hidden\"]]*hparams[\"nb_edge_layer\"],\n",
    "                                     layer_norm=hparams[\"layernorm\"],\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hparams[\"hidden_activation\"])\n",
    "\n",
    "        # The node network computes new node features\n",
    "        self.node_network = make_mlp(4*hparams[\"hidden\"],\n",
    "                                     [hparams[\"hidden\"]]*hparams[\"nb_node_layer\"],\n",
    "                                     layer_norm=hparams[\"layernorm\"],\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hparams[\"hidden_activation\"])\n",
    "        \n",
    "        # Final edge output classification network\n",
    "        self.output_edge_classifier = make_mlp(3*hparams[\"hidden\"],\n",
    "                                     [hparams[\"hidden\"], 1],\n",
    "                                     layer_norm=hparams[\"layernorm\"],\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hparams[\"hidden_activation\"])\n",
    "        \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        start, end = edge_index\n",
    "        \n",
    "        # Encode the graph features into the hidden space\n",
    "        x = self.node_encoder(x)\n",
    "        e = self.edge_encoder(torch.cat([x[start], x[end]], dim=1))\n",
    "        input_x = x\n",
    "        input_e = e      \n",
    "\n",
    "        edge_outputs = []\n",
    "        # Loop over iterations of edge and node networks\n",
    "        for i in range(self.hparams[\"n_graph_iters\"]):\n",
    "            \n",
    "            # Cocnatenate with initial latent space\n",
    "            x = torch.cat([x, input_x], dim=-1)\n",
    "            e = torch.cat([e, input_e], dim=-1)\n",
    "\n",
    "            # Compute new node features\n",
    "            edge_messages = scatter_add(e, end, dim=0, dim_size=x.shape[0]) + scatter_add(e, start, dim=0, dim_size=x.shape[0])\n",
    "            node_inputs = torch.cat([x, edge_messages], dim=-1)\n",
    "            x = self.node_network(node_inputs)\n",
    "          \n",
    "            # Compute new edge features            \n",
    "            edge_inputs = torch.cat([x[start], x[end], e], dim=-1)\n",
    "            e = self.edge_network(edge_inputs)\n",
    "            e = torch.sigmoid(e)\n",
    "            \n",
    "            classifier_inputs = torch.cat([x[start], x[end], e], dim=1)\n",
    "            edge_outputs.append(self.output_edge_classifier(classifier_inputs).squeeze(-1))\n",
    "    \n",
    "        # Compute final edge scores; use original edge directions only        \n",
    "        \n",
    "        return torch.cat(edge_outputs)\n",
    "#         classifier_inputs = torch.cat([x[start], x[end], e], dim=1)\n",
    "#         return self.output_edge_classifier(classifier_inputs).squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        weight = (torch.tensor(self.hparams[\"weight\"]) if (\"weight\" in self.hparams)\n",
    "                      else torch.tensor((~batch.y_pid.bool()).sum() / batch.y_pid.sum()))\n",
    "        \n",
    "        output = (self(torch.cat([batch.cell_data, batch.x], axis=-1), \n",
    "                       batch.edge_index).squeeze()\n",
    "                  if ('ci' in self.hparams[\"regime\"])\n",
    "                  else self(batch.x, batch.edge_index).squeeze())\n",
    "\n",
    "        if ('pid' in self.hparams[\"regime\"]):\n",
    "            y_pid = (batch.pid[batch.edge_index[0]] == batch.pid[batch.edge_index[1]]).float()\n",
    "            y_pid = y_pid.repeat((self.hparams[\"n_graph_iters\"]))\n",
    "            loss = F.binary_cross_entropy_with_logits(output, y_pid.float(), pos_weight = weight)\n",
    "        else:\n",
    "            y = batch.y.repeat((self.hparams[\"n_graph_iters\"]))\n",
    "            loss = F.binary_cross_entropy_with_logits(output, y, pos_weight = weight)\n",
    "            \n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        weight = (torch.tensor(self.hparams[\"weight\"]) if (\"weight\" in self.hparams)\n",
    "                      else torch.tensor((~batch.y_pid.bool()).sum() / batch.y_pid.sum()))\n",
    "\n",
    "        output = (self(torch.cat([batch.cell_data, batch.x], axis=-1), batch.edge_index).squeeze()\n",
    "                  if ('ci' in self.hparams[\"regime\"])\n",
    "                  else self(batch.x, batch.edge_index).squeeze())\n",
    "\n",
    "        if ('pid' in self.hparams[\"regime\"]):\n",
    "            y_pid = (batch.pid[batch.edge_index[0]] == batch.pid[batch.edge_index[1]]).float()\n",
    "            y_pid = y_pid.repeat((self.hparams[\"n_graph_iters\"]))\n",
    "            val_loss = F.binary_cross_entropy_with_logits(output, y_pid.float(), pos_weight = weight)\n",
    "        else:\n",
    "            y = batch.y.repeat((self.hparams[\"n_graph_iters\"]))\n",
    "            val_loss = F.binary_cross_entropy_with_logits(output, y, pos_weight = weight)\n",
    "\n",
    "        result = pl.EvalResult(checkpoint_on=val_loss)\n",
    "        result.log('val_loss', val_loss)\n",
    "\n",
    "        #Edge filter performance\n",
    "        preds = F.sigmoid(output) > 0.5 #Maybe send to CPU??\n",
    "        edge_positive = preds.sum().float()\n",
    "\n",
    "        if ('pid' in self.hparams[\"regime\"]):\n",
    "            y_pid = batch.pid[batch.edge_index[0]] == batch.pid[batch.edge_index[1]]\n",
    "            y_pid = y_pid.repeat((self.hparams[\"n_graph_iters\"]))\n",
    "            edge_true = y_pid.sum().float()\n",
    "            edge_true_positive = (y_pid & preds).sum().float()\n",
    "        else:\n",
    "            edge_true = y.sum()\n",
    "            edge_true_positive = (y.bool() & preds).sum().float()\n",
    "\n",
    "        result.log_dict({'eff': torch.tensor(edge_true_positive/edge_true), 'pur': torch.tensor(edge_true_positive/edge_positive)})\n",
    "\n",
    "        return result\n",
    "    \n",
    "class CheckpointedResAGNN(GNNBase):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        '''\n",
    "        Initialise the Lightning Module that can scan over different GNN training regimes\n",
    "        '''\n",
    "\n",
    "        # Setup input network\n",
    "        self.node_encoder = make_mlp(hparams[\"in_channels\"], [hparams[\"hidden\"]],\n",
    "                                      output_activation=hparams[\"hidden_activation\"],\n",
    "                                      layer_norm=hparams[\"layernorm\"])\n",
    "   \n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_network = make_mlp(2*(hparams[\"in_channels\"] + hparams[\"hidden\"]),\n",
    "                                     [hparams[\"hidden\"]]*hparams[\"nb_edge_layer\"]+[1],\n",
    "                                     layer_norm=hparams[\"layernorm\"],\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hparams[\"hidden_activation\"])\n",
    "\n",
    "        # The node network computes new node features\n",
    "        self.node_network = make_mlp((hparams[\"in_channels\"] + hparams[\"hidden\"])*2,\n",
    "                                     [hparams[\"hidden\"]]*hparams[\"nb_node_layer\"],\n",
    "                                     layer_norm=hparams[\"layernorm\"],\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hparams[\"hidden_activation\"])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        # Encode the graph features into the hidden space\n",
    "        input_x = x\n",
    "        x = self.node_encoder(x)\n",
    "        x = torch.cat([x, input_x], dim=-1)\n",
    "        \n",
    "        start, end = edge_index\n",
    "\n",
    "        # Loop over iterations of edge and node networks\n",
    "        for i in range(self.hparams[\"n_graph_iters\"]):\n",
    "            # Previous hidden state\n",
    "            x0 = x\n",
    "\n",
    "            # Compute new edge score\n",
    "            edge_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "            e = checkpoint(self.edge_network, edge_inputs)\n",
    "            e = torch.sigmoid(e)\n",
    "            \n",
    "            # Sum weighted node features coming into each node\n",
    "#             weighted_messages_in = scatter_add(e * x[start], end, dim=0, dim_size=x.shape[0])\n",
    "#             weighted_messages_out = scatter_add(e * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "            \n",
    "            weighted_messages = scatter_add(e * x[start], end, dim=0, dim_size=x.shape[0]) + scatter_add(e * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "\n",
    "            # Compute new node features\n",
    "#             node_inputs = torch.cat([x, weighted_messages_in, weighted_messages_out], dim=1)\n",
    "            node_inputs = torch.cat([x, weighted_messages], dim=1)\n",
    "            x = checkpoint(self.node_network, node_inputs)\n",
    "\n",
    "            # Residual connection\n",
    "            x = torch.cat([x, input_x], dim=-1)\n",
    "            x = x + x0\n",
    "\n",
    "        # Compute final edge scores; use original edge directions only\n",
    "        clf_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "        return checkpoint(self.edge_network, clf_inputs).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../configs/dev_gnn.yaml\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.reset_max_memory_allocated()\n",
    "# wandb_logger = WandbLogger(project=config[\"project\"], group=\"SubgraphSampling\", log_model=True, save_dir = config[\"wandb_save_dir\"])\n",
    "model = CheckpointedResAGNN(config)\n",
    "# wandb_logger.log_hyperparams(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gpus=1, max_epochs=1)\n",
    "# trainer = Trainer(max_epochs=1) #callbacks=[lr_logger],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | node_encoder | Sequential | 768   \n",
      "1 | edge_network | Sequential | 50 K  \n",
      "2 | node_network | Sequential | 50 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd9d4a147334919a656b054c075f0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 4.76 s, sys: 2.3 s, total: 7.06 s\n",
      "Wall time: 7.89 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.939926147460938"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated() / 1024**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Precision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "code_folding": [
     5,
     20,
     26,
     32,
     38,
     84
    ]
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities import AMPType\n",
    "from LightningModules.GNN.utils import load_dataset\n",
    "\n",
    "class MixedGNNBase(LightningModule):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Initialise the Lightning Module that can scan over different GNN training regimes\n",
    "        '''\n",
    "        # Assign hyperparameters\n",
    "        self.hparams = hparams\n",
    "        self.hparams[\"posted_alert\"] = False\n",
    "        \n",
    "        # Handle any subset of [train, val, test] data split, assuming that ordering\n",
    "        input_dirs = [None, None, None]\n",
    "        input_dirs[:len(hparams[\"datatype_names\"])] = [os.path.join(hparams[\"input_dir\"], datatype) for datatype in hparams[\"datatype_names\"]]\n",
    "        self.trainset, self.valset, self.testset = [load_dataset(input_dir, hparams[\"datatype_split\"][i], hparams[\"pt_min\"]) for i, input_dir in enumerate(input_dirs)]\n",
    "        print(\"Data processed\")\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        if self.trainset is not None:\n",
    "            return DataLoader(self.trainset, batch_size=1, num_workers=1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.valset is not None:\n",
    "            return DataLoader(self.valset, batch_size=1, num_workers=1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.testset is not None:\n",
    "            return DataLoader(self.testset, batch_size=1, num_workers=1)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = [torch.optim.AdamW(self.parameters(), lr=(self.hparams[\"lr\"]), betas=(0.9, 0.999), eps=1e-08, amsgrad=True)]\n",
    "#         scheduler = [\n",
    "#             {\n",
    "#                 'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer[0], factor=self.hparams[\"factor\"], patience=self.hparams[\"patience\"]),\n",
    "#                 'monitor': 'val_loss',\n",
    "#                 'interval': 'epoch',\n",
    "#                 'frequency': 1\n",
    "#             }\n",
    "#         ]\n",
    "        scheduler = [\n",
    "            {\n",
    "                'scheduler': torch.optim.lr_scheduler.StepLR(optimizer[0], step_size=self.hparams[\"patience\"], gamma=self.hparams[\"factor\"]),\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        ]\n",
    "        return optimizer, scheduler\n",
    "        \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        weight = (torch.tensor(self.hparams[\"weight\"]) if (\"weight\" in self.hparams)\n",
    "                      else torch.tensor((~batch.y_pid.bool()).sum() / batch.y_pid.sum()))\n",
    "        x = batch.x\n",
    "        print(\"X out pre:\", x.type())\n",
    "        with torch.cuda.amp.autocast():\n",
    "            print(\"X out post:\", x.type())\n",
    "            output = (self(torch.cat([batch.cell_data, batch.x], axis=-1), \n",
    "                           batch.edge_index.int()).squeeze()\n",
    "                      if ('ci' in self.hparams[\"regime\"])\n",
    "                      else self(batch.x, batch.edge_index.int()).squeeze())\n",
    "\n",
    "            if ('pid' in self.hparams[\"regime\"]):\n",
    "                y_pid = (batch.pid[batch.edge_index[0]] == batch.pid[batch.edge_index[1]]).float()\n",
    "                loss = F.binary_cross_entropy_with_logits(output, y_pid.float(), pos_weight = weight)\n",
    "            else:\n",
    "                loss = F.binary_cross_entropy_with_logits(output, batch.y, pos_weight = weight)\n",
    "            \n",
    "            self.log('train_loss', loss)\n",
    "\n",
    "#         self.manual_backward(loss, opt_a)\n",
    "#         self.manual_optimizer_step(opt_a)\n",
    "\n",
    "            return loss\n",
    "\n",
    "    def shared_evaluation(self, batch, batch_idx):\n",
    "\n",
    "        weight = (torch.tensor(self.hparams[\"weight\"]) if (\"weight\" in self.hparams)\n",
    "                      else torch.tensor((~batch.y_pid.bool()).sum() / batch.y_pid.sum()))\n",
    "\n",
    "        output = (self(torch.cat([batch.cell_data, batch.x], axis=-1), batch.edge_index).squeeze()\n",
    "                  if ('ci' in self.hparams[\"regime\"])\n",
    "                  else self(batch.x, batch.edge_index).squeeze())\n",
    "        \n",
    "        truth = (batch.pid[batch.edge_index[0]] == batch.pid[batch.edge_index[1]]).float() if 'pid' in self.hparams[\"regime\"] else batch.y\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(output, truth.float(), pos_weight = weight)\n",
    "\n",
    "        #Edge filter performance\n",
    "        preds = F.sigmoid(output) > self.hparams[\"edge_cut\"]\n",
    "        edge_positive = preds.sum().float()\n",
    "\n",
    "        edge_true = truth.sum().float()\n",
    "        edge_true_positive = (truth.bool() & preds).sum().float()\n",
    "    \n",
    "        eff = torch.tensor(edge_true_positive/edge_true)\n",
    "        pur = torch.tensor(edge_true_positive/edge_positive)\n",
    "        \n",
    "        if (eff > 0.99) and (pur > 0.99) and not self.hparams[\"posted_alert\"] and self.hparams[\"slack_alert\"]:\n",
    "            self.logger.experiment.alert(title=\"High Performance\", \n",
    "                        text=\"Efficiency and purity have both cracked 99%. Great job, Dan! You're having a great Thursday, and I think you've earned a celebratory beer.\",\n",
    "                        wait_duration=timedelta(minutes=60))\n",
    "            self.hparams[\"posted_alert\"] = True\n",
    "        \n",
    "        current_lr = self.optimizers().param_groups[0]['lr']\n",
    "        self.log_dict({'val_loss': loss, 'eff': eff, 'pur': pur, \"current_lr\": current_lr})\n",
    "        \n",
    "        return {\"loss\": loss, \"preds\": preds.cpu().numpy(), \"truth\": truth.cpu().numpy()}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        outputs = self.shared_evaluation(batch, batch_idx)\n",
    "            \n",
    "        return outputs[\"loss\"]\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        outputs = self.shared_evaluation(batch, batch_idx)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "#     def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n",
    "#         # warm up lr\n",
    "#         if (self.hparams[\"warmup\"] is not None) and (self.trainer.global_step < self.hparams[\"warmup\"]):\n",
    "#             lr_scale = min(1., float(self.trainer.global_step + 1) / self.hparams[\"warmup\"])\n",
    "#             for pg in optimizer.param_groups:\n",
    "#                 pg['lr'] = lr_scale * self.hparams[\"lr\"]\n",
    "        \n",
    "#         # update params\n",
    "#         print(self.trainer.scaler.get_scale())\n",
    "#         if self.trainer.amp_backend == AMPType.NATIVE:\n",
    "#             # native amp does not yet support closures.\n",
    "#             # TODO: pass the closure to the step ASAP\n",
    "#             optimizer_closure()\n",
    "#             self.trainer.scaler.step(optimizer)\n",
    "#             self.trainer.scaler.update()\n",
    "#         else:\n",
    "#             optimizer.step(closure=optimizer_closure)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MixedCheckpointedResAGNN(MixedGNNBase):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        '''\n",
    "        Initialise the Lightning Module that can scan over different GNN training regimes\n",
    "        '''\n",
    "\n",
    "        # Setup input network\n",
    "        self.node_encoder = make_mlp(hparams[\"in_channels\"], [hparams[\"hidden\"]],\n",
    "                                      output_activation=hparams[\"hidden_activation\"],\n",
    "                                      layer_norm=hparams[\"layernorm\"])\n",
    "   \n",
    "        # The edge network computes new edge features from connected nodes\n",
    "        self.edge_network = make_mlp(2*(hparams[\"in_channels\"] + hparams[\"hidden\"]),\n",
    "                                     [hparams[\"hidden\"]]*hparams[\"nb_edge_layer\"]+[1],\n",
    "                                     layer_norm=hparams[\"layernorm\"],\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hparams[\"hidden_activation\"])\n",
    "\n",
    "        # The node network computes new node features\n",
    "        self.node_network = make_mlp((hparams[\"in_channels\"] + hparams[\"hidden\"])*2,\n",
    "                                     [hparams[\"hidden\"]]*hparams[\"nb_node_layer\"],\n",
    "                                     layer_norm=hparams[\"layernorm\"],\n",
    "                                     output_activation=None,\n",
    "                                     hidden_activation = hparams[\"hidden_activation\"])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        # Encode the graph features into the hidden space\n",
    "        input_x = x\n",
    "        print(\"X type:\", input_x.type())\n",
    "        print(\"Edge type pre:\", edge_index.type())\n",
    "        edge_index = edge_index.long()\n",
    "        print(\"Edge type post:\", edge_index.type())\n",
    "        x = self.node_encoder(x)\n",
    "        x = torch.cat([x, input_x], dim=-1)\n",
    "        \n",
    "        start, end = edge_index\n",
    "\n",
    "        # Loop over iterations of edge and node networks\n",
    "        for i in range(self.hparams[\"n_graph_iters\"]):\n",
    "            # Previous hidden state\n",
    "            x0 = x\n",
    "\n",
    "            # Compute new edge score\n",
    "            edge_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "            e = checkpoint(self.edge_network, edge_inputs)\n",
    "            e = torch.sigmoid(e)\n",
    "            \n",
    "            # Sum weighted node features coming into each node\n",
    "#             weighted_messages_in = scatter_add(e * x[start], end, dim=0, dim_size=x.shape[0])\n",
    "#             weighted_messages_out = scatter_add(e * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "            \n",
    "            weighted_messages = scatter_add(e * x[start], end, dim=0, dim_size=x.shape[0]) + scatter_add(e * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "\n",
    "            # Compute new node features\n",
    "#             node_inputs = torch.cat([x, weighted_messages_in, weighted_messages_out], dim=1)\n",
    "            node_inputs = torch.cat([x, weighted_messages], dim=1)\n",
    "            x = checkpoint(self.node_network, node_inputs)\n",
    "\n",
    "            # Residual connection\n",
    "            x = torch.cat([x, input_x], dim=-1)\n",
    "            x = x + x0\n",
    "\n",
    "        # Compute final edge scores; use original edge directions only\n",
    "        clf_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "        return checkpoint(self.edge_network, clf_inputs).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Precision Lightning Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../configs/dev_gnn.yaml\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.reset_max_memory_allocated()\n",
    "# wandb_logger = WandbLogger(project=config[\"project\"], group=\"SubgraphSampling\", log_model=True, save_dir = config[\"wandb_save_dir\"])\n",
    "model = MixedCheckpointedResAGNN(config)\n",
    "# wandb_logger.log_hyperparams(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gpus=1, max_epochs=1, amp_level='O2', precision=16)\n",
    "# trainer = Trainer(max_epochs=1) #callbacks=[lr_logger],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | node_encoder | Sequential | 768   \n",
      "1 | edge_network | Sequential | 50 K  \n",
      "2 | node_network | Sequential | 50 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.LongTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.LongTensor\n",
      "Edge type post: torch.cuda.LongTensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/d/danieltm/.local/lib/python3.7/site-packages/ipykernel_launcher.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/homes/d/danieltm/.local/lib/python3.7/site-packages/ipykernel_launcher.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e32429f688c4b83a3cdd31996ccb78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X out pre: torch.cuda.FloatTensor\n",
      "X out post: torch.cuda.FloatTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.IntTensor\n",
      "Edge type post: torch.cuda.LongTensor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.LongTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.LongTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.LongTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.LongTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "X type: torch.cuda.FloatTensor\n",
      "Edge type pre: torch.cuda.LongTensor\n",
      "Edge type post: torch.cuda.LongTensor\n",
      "\n",
      "CPU times: user 5.16 s, sys: 2.11 s, total: 7.27 s\n",
      "Wall time: 8.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.865897178649902"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated() / 1024**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision Base Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor\n",
      "torch.cuda.HalfTensor\n",
      "torch.cuda.FloatTensor\n",
      "torch.cuda.HalfTensor\n",
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "b_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "c_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "d_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "print(a_float32.type())\n",
    "\n",
    "with autocast():\n",
    "    # torch.mm is on autocast's list of ops that should run in float16.\n",
    "    # Inputs are float32, but the op runs in float16 and produces float16 output.\n",
    "    # No manual casts are required.\n",
    "    e_float16 = torch.mm(a_float32, b_float32)\n",
    "    print(e_float16.type())\n",
    "    print(a_float32.type())\n",
    "    # Also handles mixed input types\n",
    "    f_float16 = torch.mm(d_float32, e_float16)\n",
    "    print(f_float16.type())\n",
    "\n",
    "# After exiting autocast, calls f_float16.float() to use with d_float32\n",
    "g_float32 = torch.mm(d_float32, f_float16.float())\n",
    "print(g_float32.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = model.trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor\n",
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "with autocast(enabled=True):\n",
    "    enc = node_encoder(x)\n",
    "    print(enc.type())\n",
    "    x2 = x + x\n",
    "    print(x2.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_fwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-fd0b0dbe78ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mcustom_fwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnode_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_fwd' is not defined"
     ]
    }
   ],
   "source": [
    "@staticmethod\n",
    "@custom_fwd\n",
    "def forward(a, b):\n",
    "    return node_encoder(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The node network computes new node features\n",
    "hparams = model.hparams\n",
    "node_network = make_mlp((hparams[\"in_channels\"] + hparams[\"hidden\"])*2,\n",
    "                             [hparams[\"hidden\"]]*hparams[\"nb_node_layer\"],\n",
    "                             layer_norm=hparams[\"layernorm\"],\n",
    "                             output_activation=None,\n",
    "                             hidden_activation = hparams[\"hidden_activation\"]).to(device)\n",
    "node_encoder = make_mlp(hparams[\"in_channels\"], [hparams[\"hidden\"]],\n",
    "                                      output_activation=hparams[\"hidden_activation\"],\n",
    "                                      layer_norm=hparams[\"layernorm\"]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_label = \"rm3047br\"\n",
    "wandb_dir = \"/global/cscratch1/sd/danieltm/ExaTrkX/wandb_data\"\n",
    "best_run_path = get_best_run(run_label,wandb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpnt = torch.load(best_run_path)\n",
    "model = CheckpointedResAGNN.load_from_checkpoint(best_run_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    edge_total_positive, edge_total_true, edge_total_true_positive, edge_total_true_ground = 0, 0, 0, 0\n",
    "    for i, batch in enumerate(model.val_dataloader().dataset[:5]):\n",
    "        data = batch.to(device)\n",
    "\n",
    "        output = (model(torch.cat([data.cell_data, data.x], axis=-1), data.edge_index).squeeze()\n",
    "                  if ('ci' in model.hparams[\"regime\"])\n",
    "                  else model(data.x, data.edge_index).squeeze())\n",
    "\n",
    "        #Edge filter performance\n",
    "        preds = F.sigmoid(output) > 0.9 #Maybe send to CPU??\n",
    "        edge_positive = preds.sum().float()\n",
    "\n",
    "        if ('pid' in model.hparams[\"regime\"]):\n",
    "            y_pid = data.pid[data.edge_index[0]] == data.pid[data.edge_index[1]]\n",
    "            edge_true = y_pid.sum().float()\n",
    "            edge_true_positive = (y_pid & preds).sum().float()\n",
    "        else:\n",
    "            edge_true = data.y.sum()\n",
    "            edge_true_ground = data.layerless_true_edges.shape[1]\n",
    "            edge_true_positive = (data.y.bool() & preds).sum().float()\n",
    "            \n",
    "        edge_total_positive += edge_positive\n",
    "        edge_total_true_positive += edge_true_positive\n",
    "        edge_total_true += edge_true\n",
    "        edge_total_true_ground += edge_true_ground\n",
    "        \n",
    "        print(i)\n",
    "\n",
    "    edge_eff = (edge_total_true_positive / max(edge_total_true, 1))\n",
    "    edge_ground_eff = (edge_total_true_positive / max(edge_total_true_ground, 1))\n",
    "    edge_pur = (edge_total_true_positive / max(edge_total_positive, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff: tensor(0.8070, device='cuda:0') Pur: tensor(0.9520, device='cuda:0') Ground eff: tensor(0.6865, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Eff:\", edge_eff, \"Pur:\", edge_pur, \"Ground eff:\", edge_ground_eff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Truth Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = model.val_dataloader().dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(cell_data=[71081, 9], edge_index=[2, 987461], event_file=/global/cscratch1/sd/danieltm/ExaTrkX/trackml/train_all/event000001193, hid=[71081], layerless_true_edges=[2, 85024], layers=[71081], pid=[71081], x=[71081, 3], y=[987461], y_pid=[987461])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2534,  7691, 13022,  ..., 79374, 89338, 93484],\n",
       "        [ 7691, 13022, 19078,  ..., 89338, 93484, 96979]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.layerless_true_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,    323,    527,  ..., 103304, 103122, 103304],\n",
       "        [   287,      0,      0,  ..., 103120, 103304, 103131]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(117942.)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 117942])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.edge_index[:,sample.y.bool()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 123429])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.layerless_true_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555452932455095"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.edge_index[:,sample.y.bool()].shape[1]/sample.layerless_true_edges.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TrackML Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Ground Truth Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = torch.load(\"/global/cscratch1/sd/danieltm/ExaTrkX/trackml_processed/filter_processed/0_pt_cut_endcaps_connected_high_eff/train/1000\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "event_file = '/global/cscratch1/sd/danieltm/ExaTrkX/trackml/train_all/event000001000'\n",
    "hits, particles, truth = trackml.dataset.load_event(\n",
    "        event_file, parts=['hits', 'particles', 'truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove noise and assign track_id\n",
    "hits = hits.merge(truth[['hit_id', 'weight', 'particle_id']], on='hit_id')\n",
    "hits = hits[hits.particle_id != 0]\n",
    "hids = sample.hid.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "truth_graph = (sample.layerless_true_edges).cpu().numpy()\n",
    "truth_graph = hids[truth_graph]\n",
    "truth_graph = np.hstack([truth_graph, truth_graph[::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "truth_graph_sp = sp.sparse.coo_matrix(([0.1]*truth_graph.shape[1], (truth_graph[0], truth_graph[1])), shape=(truth_graph.max()+1, truth_graph.max()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.1, metric=\"precomputed\", min_samples=1).fit_predict(truth_graph_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000080094655"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_list = np.vstack([np.unique(truth_graph), clustering[np.unique(truth_graph)]])\n",
    "track_list = pd.DataFrame(track_list.T)\n",
    "track_list.columns = [\"hit_id\", \"track_id\"]\n",
    "score_event(hits, track_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Filter Truth Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = torch.load(\"/global/cscratch1/sd/danieltm/ExaTrkX/trackml_processed/filter_processed/0_pt_cut_endcaps_connected_high_eff/train/1000\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(cell_data=[103305, 9], edge_index=[2, 1831684], event_file=/global/cscratch1/sd/danieltm/ExaTrkX/trackml/train_all/event000001000, hid=[103305], layerless_true_edges=[2, 123429], layers=[103305], pid=[103305], x=[103305, 3], y=[1831684], y_pid=[1831684])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "truth_graph = (sample.edge_index[:,sample.y_pid.bool()]).cpu().numpy()\n",
    "truth_graph = hids[truth_graph]\n",
    "truth_graph = np.hstack([truth_graph, truth_graph[::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "truth_graph_sp = sp.sparse.coo_matrix(([0.1]*truth_graph.shape[1], (truth_graph[0], truth_graph[1])), shape=(truth_graph.max()+1, truth_graph.max()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.1, metric=\"precomputed\", min_samples=1).fit_predict(truth_graph_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9349574380163364"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_list = np.vstack([np.unique(truth_graph), clustering[np.unique(truth_graph)]])\n",
    "track_list = pd.DataFrame(track_list.T)\n",
    "track_list.columns = [\"hit_id\", \"track_id\"]\n",
    "score_event(hits, track_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From ground truth, IF they have a true edge in the filtered set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "truth_graph = (sample.edge_index[:,sample.y_pid.bool()]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9592538354264974"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_list = np.vstack([hids[np.unique(truth_graph)], sample.pid[np.unique(truth_graph)]])\n",
    "track_list = pd.DataFrame(track_list.T)\n",
    "track_list.columns = [\"hit_id\", \"track_id\"]\n",
    "score_event(hits, track_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Noise Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = torch.load(\"/global/cscratch1/sd/danieltm/ExaTrkX/trackml_processed/filter_processed/0_pt_cut_endcaps_connected_high_eff/train/1000\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "event_file = '/global/cscratch1/sd/danieltm/ExaTrkX/trackml/train_all/event000001000'\n",
    "hits, particles, truth = trackml.dataset.load_event(\n",
    "        event_file, parts=['hits', 'particles', 'truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assign track_id\n",
    "hits = hits.merge(truth[['hit_id', 'weight', 'particle_id']], on='hit_id')\n",
    "hids = sample.hid.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "truth_graph = (sample.layerless_true_edges).cpu().numpy()\n",
    "truth_graph = hids[truth_graph]\n",
    "truth_graph = np.hstack([truth_graph, truth_graph[::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "truth_graph_sp = sp.sparse.coo_matrix(([0.1]*truth_graph.shape[1], (truth_graph[0], truth_graph[1])), shape=(truth_graph.max()+1, truth_graph.max()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.1, metric=\"precomputed\", min_samples=1).fit_predict(truth_graph_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000080094655"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_list = np.vstack([np.unique(truth_graph), clustering[np.unique(truth_graph)]])\n",
    "track_list = pd.DataFrame(track_list.T)\n",
    "track_list.columns = [\"hit_id\", \"track_id\"]\n",
    "score_event(hits, track_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "noise_idx = hits.hit_id[~hits.hit_id.isin(track_list.hit_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "random_noise = np.random.choice(hits.particle_id, (len(noise_idx),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "noise_idx = pd.DataFrame(noise_idx).assign(track_id=random_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "noise_joined = pd.concat([track_list, noise_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000080094655"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_event(hits, noise_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_id</th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120908</th>\n",
       "      <td>120909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120912</th>\n",
       "      <td>120913</td>\n",
       "      <td>801644513243168768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120925</th>\n",
       "      <td>120926</td>\n",
       "      <td>589977461060534272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120928</th>\n",
       "      <td>120929</td>\n",
       "      <td>396324463789998080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120930</th>\n",
       "      <td>120931</td>\n",
       "      <td>4528338638995456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120939 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hit_id            track_id\n",
       "0            2                   2\n",
       "1            4                   4\n",
       "2            5                   5\n",
       "3            6                   6\n",
       "4            7                   7\n",
       "...        ...                 ...\n",
       "120908  120909                   0\n",
       "120912  120913  801644513243168768\n",
       "120925  120926  589977461060534272\n",
       "120928  120929  396324463789998080\n",
       "120930  120931    4528338638995456\n",
       "\n",
       "[120939 rows x 2 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExatrkxTest",
   "language": "python",
   "name": "exatrkx-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
