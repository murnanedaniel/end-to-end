{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Architecture Performance Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "from time import time as tt\n",
    "\n",
    "# External imports\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import chain\n",
    "from itertools import permutations\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter, segment_csr, scatter_add\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_cluster import knn_graph, radius_graph\n",
    "from torch_sparse import SparseTensor\n",
    "import torch_sparse\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pygnn\n",
    "\n",
    "import trackml.dataset\n",
    "\n",
    "# Limit CPU usage on Jupyter\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Pick up local packages\n",
    "sys.path.append('..')\n",
    "\n",
    "# Local imports\n",
    "from prepare_utils import prepare_event\n",
    "from toy_utils import *\n",
    "from models import *\n",
    "from trainers import *\n",
    "%matplotlib inline\n",
    "\n",
    "# Get rid of RuntimeWarnings, gross\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import wandb\n",
    "import faiss\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "torch_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "from apex.fp16_utils import *\n",
    "from apex import amp, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['toy_utils'])\n",
    "from toy_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['models'])\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_cut = 0.5\n",
    "train_number = 1000\n",
    "test_number = 100\n",
    "load_dir = \"/global/cscratch1/sd/danieltm/ExaTrkX/trackml_processed/\"\n",
    "gnn_train_path = os.path.join(load_dir, str(pt_cut) + \"_pt_cut\", str(train_number) + \"_graphs_train\")\n",
    "gnn_test_path = os.path.join(load_dir, str(pt_cut) + \"_pt_cut\", str(test_number) + \"_graphs_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 154 ms, sys: 3.21 s, total: 3.36 s\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gnn_train_dataset = torch.load(gnn_train_path)\n",
    "gnn_test_dataset = torch.load(gnn_test_path)\n",
    "gnn_train_loader = DataLoader(gnn_train_dataset, batch_size=1, shuffle=True)\n",
    "gnn_test_loader = DataLoader(gnn_test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "New model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/murnanedaniel/GravNet\" target=\"_blank\">https://app.wandb.ai/murnanedaniel/GravNet</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/murnanedaniel/GravNet/runs/edkif7ke\" target=\"_blank\">https://app.wandb.ai/murnanedaniel/GravNet/runs/edkif7ke</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "m_configs = {\"in_channels\": 3, \"hidden_dim\": 64, \"n_graph_iters\": 6}\n",
    "torch.manual_seed(torch_seed)\n",
    "model = ResAGNN(**m_configs).to(device)\n",
    "other_configs = {\"weight\": 2}\n",
    "m_configs.update(other_configs)\n",
    "model_name = wandb.init(project=\"GravNet\", group=\"ResAGNN\", config=m_configs)\n",
    "wandb.watch(model, log='all')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...or load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/murnanedaniel/GravNet\" target=\"_blank\">https://app.wandb.ai/murnanedaniel/GravNet</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/murnanedaniel/GravNet/runs/04shu00d\" target=\"_blank\">https://app.wandb.ai/murnanedaniel/GravNet/runs/04shu00d</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x2aab7ffc7b10>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_configs = {\"in_channels\": 3, \"hidden_dim\": 64, \"n_graph_iters\": 6}\n",
    "torch.manual_seed(torch_seed)\n",
    "model = ResAGNN(**m_configs).to(device)\n",
    "other_configs = {\"weight\": 2}\n",
    "m_configs.update(other_configs)\n",
    "checkpoint = torch.load('../model_comparisons/ResAGNN/ethereal-wood-503.tar')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_name = wandb.init(project=\"GravNet\", group=\"ResAGNN\", config=m_configs)\n",
    "wandb.watch(model, log='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=10)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 25.063142077997327\n",
      "Epoch: 0, Accuracy: 0.9929, Purity: 0.9604, Efficiency: 0.9898, Loss: 2.3208, LR: 0.001 in time 211.51566171646118\n",
      "Training loss: 24.163852674886584\n",
      "Epoch: 1, Accuracy: 0.9930, Purity: 0.9612, Efficiency: 0.9890, Loss: 2.3506, LR: 0.001 in time 211.6275191307068\n",
      "Training loss: 23.775235688313842\n",
      "Epoch: 2, Accuracy: 0.9937, Purity: 0.9663, Efficiency: 0.9894, Loss: 2.1385, LR: 0.001 in time 211.61263060569763\n",
      "Training loss: 23.52497413009405\n",
      "Epoch: 3, Accuracy: 0.9879, Purity: 0.9304, Efficiency: 0.9864, Loss: 3.7791, LR: 0.001 in time 211.66961789131165\n",
      "Training loss: 23.31285702250898\n",
      "Epoch: 4, Accuracy: 0.9931, Purity: 0.9628, Efficiency: 0.9884, Loss: 2.3456, LR: 0.001 in time 211.37088584899902\n",
      "Training loss: 23.0078675923869\n",
      "Epoch: 5, Accuracy: 0.9938, Purity: 0.9652, Efficiency: 0.9910, Loss: 2.0477, LR: 0.001 in time 211.63346529006958\n",
      "Training loss: 22.563860021531582\n",
      "Epoch: 6, Accuracy: 0.9934, Purity: 0.9631, Efficiency: 0.9901, Loss: 2.2067, LR: 0.001 in time 211.4949290752411\n",
      "Training loss: 22.3869682662189\n",
      "Epoch: 7, Accuracy: 0.9931, Purity: 0.9622, Efficiency: 0.9892, Loss: 2.2784, LR: 0.001 in time 211.50483536720276\n",
      "Training loss: 22.072958749718964\n",
      "Epoch: 8, Accuracy: 0.9935, Purity: 0.9666, Efficiency: 0.9868, Loss: 2.3074, LR: 0.001 in time 211.28554511070251\n",
      "Training loss: 21.767722367309034\n",
      "Epoch: 9, Accuracy: 0.9942, Purity: 0.9693, Efficiency: 0.9894, Loss: 2.0300, LR: 0.001 in time 211.45901703834534\n",
      "Training loss: 21.647122226655483\n",
      "Epoch: 10, Accuracy: 0.9934, Purity: 0.9652, Efficiency: 0.9877, Loss: 2.3058, LR: 0.001 in time 211.45027661323547\n",
      "Training loss: 21.105086783878505\n",
      "Epoch: 11, Accuracy: 0.9941, Purity: 0.9685, Efficiency: 0.9899, Loss: 2.0072, LR: 0.001 in time 211.7552032470703\n",
      "Training loss: 21.231809652410448\n",
      "Epoch: 12, Accuracy: 0.9947, Purity: 0.9735, Efficiency: 0.9889, Loss: 1.9320, LR: 0.001 in time 211.81041741371155\n",
      "Training loss: 20.70195770356804\n",
      "Epoch: 13, Accuracy: 0.9943, Purity: 0.9695, Efficiency: 0.9897, Loss: 1.9904, LR: 0.001 in time 211.91563963890076\n",
      "Training loss: 20.52923735510558\n",
      "Epoch: 14, Accuracy: 0.9939, Purity: 0.9672, Efficiency: 0.9896, Loss: 2.0790, LR: 0.001 in time 211.59614968299866\n",
      "Training loss: 20.49484298657626\n",
      "Epoch: 15, Accuracy: 0.9941, Purity: 0.9688, Efficiency: 0.9892, Loss: 2.0624, LR: 0.001 in time 211.64261937141418\n",
      "Training loss: 20.062509536743164\n",
      "Epoch: 16, Accuracy: 0.9933, Purity: 0.9641, Efficiency: 0.9882, Loss: 2.3058, LR: 0.001 in time 211.45388984680176\n",
      "Training loss: 19.911372325383127\n",
      "Epoch: 17, Accuracy: 0.9940, Purity: 0.9661, Efficiency: 0.9914, Loss: 1.9677, LR: 0.001 in time 211.71480345726013\n",
      "Training loss: 19.436999411322176\n",
      "Epoch: 18, Accuracy: 0.9942, Purity: 0.9687, Efficiency: 0.9898, Loss: 2.0113, LR: 0.001 in time 211.48726558685303\n",
      "Training loss: 19.640552633441985\n",
      "Epoch: 19, Accuracy: 0.9946, Purity: 0.9706, Efficiency: 0.9912, Loss: 1.8197, LR: 0.001 in time 211.3999364376068\n",
      "Training loss: 19.448899035342038\n",
      "Epoch: 20, Accuracy: 0.9948, Purity: 0.9721, Efficiency: 0.9908, Loss: 1.8056, LR: 0.001 in time 211.40052032470703\n",
      "Training loss: 19.07460310868919\n",
      "Epoch: 21, Accuracy: 0.9950, Purity: 0.9727, Efficiency: 0.9915, Loss: 1.7307, LR: 0.001 in time 211.30075311660767\n",
      "Training loss: 18.730561712756753\n",
      "Epoch: 22, Accuracy: 0.9942, Purity: 0.9696, Efficiency: 0.9892, Loss: 2.0238, LR: 0.001 in time 211.33354330062866\n",
      "Training loss: 18.887419553473592\n",
      "Epoch: 23, Accuracy: 0.9946, Purity: 0.9705, Efficiency: 0.9909, Loss: 1.8617, LR: 0.001 in time 211.39725184440613\n",
      "Training loss: 18.53031276445836\n",
      "Epoch: 24, Accuracy: 0.9946, Purity: 0.9705, Efficiency: 0.9910, Loss: 1.8430, LR: 0.001 in time 211.51339149475098\n",
      "Training loss: 18.559652214869857\n",
      "Epoch: 25, Accuracy: 0.9950, Purity: 0.9735, Efficiency: 0.9906, Loss: 1.7442, LR: 0.001 in time 211.46139359474182\n",
      "Training loss: 18.029883097857237\n",
      "Epoch: 26, Accuracy: 0.9951, Purity: 0.9743, Efficiency: 0.9909, Loss: 1.7094, LR: 0.001 in time 211.47259068489075\n",
      "Training loss: 18.277735456824303\n",
      "Epoch: 27, Accuracy: 0.9949, Purity: 0.9727, Efficiency: 0.9907, Loss: 1.7918, LR: 0.001 in time 211.38833904266357\n",
      "Training loss: 18.052422777749598\n",
      "Epoch: 28, Accuracy: 0.9943, Purity: 0.9697, Efficiency: 0.9901, Loss: 1.9260, LR: 0.001 in time 211.42615604400635\n",
      "Training loss: 17.64347195252776\n",
      "Epoch: 29, Accuracy: 0.9945, Purity: 0.9705, Efficiency: 0.9906, Loss: 1.8890, LR: 0.001 in time 211.30760884284973\n",
      "Training loss: 17.858991398476064\n",
      "Epoch: 30, Accuracy: 0.9945, Purity: 0.9709, Efficiency: 0.9902, Loss: 1.8822, LR: 0.001 in time 211.4199938774109\n",
      "Training loss: 17.615839410573244\n",
      "Epoch: 31, Accuracy: 0.9952, Purity: 0.9745, Efficiency: 0.9915, Loss: 1.6651, LR: 0.001 in time 211.45146107673645\n",
      "Training loss: 17.32537085749209\n",
      "Epoch: 32, Accuracy: 0.9954, Purity: 0.9749, Efficiency: 0.9924, Loss: 1.5907, LR: 0.001 in time 214.95112943649292\n",
      "Training loss: 17.663130324333906\n",
      "Epoch: 33, Accuracy: 0.9952, Purity: 0.9745, Efficiency: 0.9912, Loss: 1.6989, LR: 0.001 in time 211.20474791526794\n",
      "Training loss: 17.40852375421673\n",
      "Epoch: 34, Accuracy: 0.9954, Purity: 0.9745, Efficiency: 0.9925, Loss: 1.5797, LR: 0.001 in time 211.363667011261\n",
      "Training loss: 16.993438905104995\n",
      "Epoch: 35, Accuracy: 0.9950, Purity: 0.9728, Efficiency: 0.9913, Loss: 1.7438, LR: 0.001 in time 211.2378273010254\n",
      "Training loss: 16.965738172642887\n",
      "Epoch: 36, Accuracy: 0.9951, Purity: 0.9736, Efficiency: 0.9915, Loss: 1.6841, LR: 0.001 in time 211.10089468955994\n",
      "Training loss: 16.804565113037825\n",
      "Epoch: 37, Accuracy: 0.9955, Purity: 0.9762, Efficiency: 0.9920, Loss: 1.5541, LR: 0.001 in time 211.4029939174652\n",
      "Training loss: 16.476992474868894\n",
      "Epoch: 38, Accuracy: 0.9951, Purity: 0.9737, Efficiency: 0.9918, Loss: 1.6733, LR: 0.001 in time 211.22866868972778\n",
      "Training loss: 16.83181445300579\n",
      "Epoch: 39, Accuracy: 0.9949, Purity: 0.9721, Efficiency: 0.9920, Loss: 1.7281, LR: 0.001 in time 211.26645350456238\n",
      "Training loss: 16.395101345144212\n",
      "Epoch: 40, Accuracy: 0.9953, Purity: 0.9748, Efficiency: 0.9913, Loss: 1.6440, LR: 0.001 in time 211.35136342048645\n",
      "Training loss: 16.44512451160699\n",
      "Epoch: 41, Accuracy: 0.9955, Purity: 0.9749, Efficiency: 0.9927, Loss: 1.5465, LR: 0.001 in time 211.25127911567688\n",
      "Training loss: 16.55293292272836\n",
      "Epoch: 42, Accuracy: 0.9953, Purity: 0.9751, Efficiency: 0.9916, Loss: 1.6587, LR: 0.001 in time 211.18855166435242\n",
      "Training loss: 16.076877303421497\n",
      "Epoch: 43, Accuracy: 0.9956, Purity: 0.9775, Efficiency: 0.9909, Loss: 1.6337, LR: 0.001 in time 211.16777992248535\n",
      "Training loss: 16.22437680605799\n",
      "Epoch: 44, Accuracy: 0.9949, Purity: 0.9726, Efficiency: 0.9911, Loss: 1.7801, LR: 0.001 in time 211.92070770263672\n",
      "Training loss: 16.04494675900787\n",
      "Epoch: 45, Accuracy: 0.9955, Purity: 0.9757, Efficiency: 0.9922, Loss: 1.5595, LR: 0.001 in time 211.2009358406067\n",
      "Training loss: 15.802577206864953\n",
      "Epoch: 46, Accuracy: 0.9955, Purity: 0.9753, Efficiency: 0.9925, Loss: 1.5242, LR: 0.001 in time 211.19397044181824\n",
      "Training loss: 15.65528264734894\n",
      "Epoch: 47, Accuracy: 0.9957, Purity: 0.9766, Efficiency: 0.9927, Loss: 1.4752, LR: 0.001 in time 211.2371904850006\n",
      "Training loss: 15.921709625050426\n",
      "Epoch: 48, Accuracy: 0.9959, Purity: 0.9782, Efficiency: 0.9924, Loss: 1.4442, LR: 0.001 in time 211.12917709350586\n",
      "Training loss: 15.638870569877326\n",
      "Epoch: 49, Accuracy: 0.9955, Purity: 0.9768, Efficiency: 0.9912, Loss: 1.5886, LR: 0.001 in time 211.27532696723938\n",
      "Training loss: 15.880149876698852\n",
      "Epoch: 50, Accuracy: 0.9951, Purity: 0.9729, Efficiency: 0.9926, Loss: 1.6318, LR: 0.001 in time 211.25448608398438\n",
      "Training loss: 15.563544905744493\n",
      "Epoch: 51, Accuracy: 0.9952, Purity: 0.9731, Efficiency: 0.9928, Loss: 1.6028, LR: 0.001 in time 211.26694178581238\n",
      "Training loss: 15.416720042005181\n",
      "Epoch: 52, Accuracy: 0.9958, Purity: 0.9769, Efficiency: 0.9929, Loss: 1.4533, LR: 0.001 in time 211.60607290267944\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8d6e73d47909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/u2/d/danieltm/ExaTrkX/end-to-end/trainers.py\u001b[0m in \u001b[0;36mtrain_gnn\u001b[0;34m(model, gnn_train_loader, optimizer, m_configs)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/u2/d/danieltm/ExaTrkX/end-to-end/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# Apply edge network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# Apply node network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/u2/d/danieltm/ExaTrkX/end-to-end/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0medge_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNodeNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x2aab7fe45d40> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/exatrkx-test/lib/python3.7/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_orig_post_run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post_run_cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_post_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/exatrkx-test/lib/python3.7/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/exatrkx-test/lib/python3.7/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/exatrkx-test/lib/python3.7/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;34m\"\"\"Stops system stats, streaming handlers, and uploads files without output, used by wandb.monitor\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutting down system stats and metadata service\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensorboard_watchers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/exatrkx-test/lib/python3.7/site-packages/wandb/stats.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Incase we never start it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/exatrkx-test/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/exatrkx-test/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    tic = tt()  \n",
    "    model.train()\n",
    "    acc, train_loss = train_gnn(model, gnn_train_loader, optimizer, m_configs)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc, eff, pur, val_loss = evaluate_gnn(model, gnn_test_loader, m_configs)\n",
    "    scheduler.step(val_loss)\n",
    "    wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss, \"val_acc\": acc, \"val_pur\": pur, \"val_eff\": eff, \"lr\": scheduler._last_lr[0]})\n",
    "\n",
    "    save_model(epoch, model, optimizer, scheduler, val_loss, m_configs, 'ResAGNN/'+model_name._name+'.tar')\n",
    "\n",
    "    print('Epoch: {}, Accuracy: {:.4f}, Purity: {:.4f}, Efficiency: {:.4f}, Loss: {:.4f}, LR: {} in time {}'.format(epoch, acc, pur, eff, val_loss, scheduler._last_lr[0], tt()-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ROC Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "total_pred = []\n",
    "total_y = []\n",
    "\n",
    "for batch in gnn_test_loader:\n",
    "    data = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(data)   \n",
    "    y = batch.pid[batch.e[0]] == batch.pid[batch.e[1]]\n",
    "\n",
    "    total_pred += pred.detach().cpu().numpy().tolist()\n",
    "    total_y += y.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(total_y, total_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'AGNN over Biadjacent Embedding')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr)\n",
    "ax.text(0.4, 0.95, \"AUC: {:.4f}\".format(metrics.roc_auc_score(total_y, total_pred)), transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "ax.set_xlabel(\"fpr\"), ax.set_ylabel(\"tpr\")\n",
    "ax.set_title(\"AGNN over Biadjacent Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'AGNN over Biadjacent Embedding')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr*cluster_eff)\n",
    "ax.text(0.2, 0.8, \"AUC (with cluster eff.): {:.4f}\".format(metrics.roc_auc_score(total_y, total_pred)*cluster_eff), transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "ax.set_xlabel(\"fpr\"), ax.set_ylabel(\"tpr\")\n",
    "ax.set_title(\"AGNN over Biadjacent Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### F1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4455550437586366, 0.9999556613115385),\n",
       " (0.445601105481345, 0.9999556613115385),\n",
       " (0.45520497466605253, 0.9999556613115385),\n",
       " (0.45525103638876097, 0.9999556613115385),\n",
       " (0.45794564716720404, 0.9999556613115385),\n",
       " (0.4579917088899125, 0.9999556613115385),\n",
       " (0.46722708429295257, 0.9999556613115385),\n",
       " (0.467273146015661, 0.9999556613115385),\n",
       " (0.48224320589590053, 0.9999556613115385),\n",
       " (0.48224320589590053, 0.9999704408743589),\n",
       " (0.4847305389221557, 0.9999704408743589),\n",
       " (0.4847766006448641, 0.9999704408743589),\n",
       " (0.49456471672040536, 0.9999704408743589),\n",
       " (0.4946107784431138, 0.9999704408743589),\n",
       " (0.5251497005988024, 0.9999704408743589),\n",
       " (0.5251957623215108, 0.9999704408743589),\n",
       " (0.5346614463380931, 0.9999704408743589),\n",
       " (0.5347075080608015, 0.9999704408743589),\n",
       " (0.5854905573468447, 0.9999704408743589),\n",
       " (0.5855366190695532, 0.9999704408743589),\n",
       " (0.5894288346384154, 0.9999704408743589),\n",
       " (0.5894748963611239, 0.9999704408743589),\n",
       " (0.5923767848917549, 0.9999704408743589),\n",
       " (0.5924228466144634, 0.9999704408743589),\n",
       " (0.5979041916167664, 0.9999704408743589),\n",
       " (0.5979502533394749, 0.9999704408743589),\n",
       " (0.6018654997696914, 0.9999704408743589),\n",
       " (0.6019115614923998, 0.9999704408743589),\n",
       " (0.6043528327959465, 0.9999704408743589),\n",
       " (0.604398894518655, 0.9999704408743589),\n",
       " (0.607623215108245, 0.9999704408743589),\n",
       " (0.607623215108245, 0.9999852204371795),\n",
       " (0.6121142330723169, 0.9999852204371795),\n",
       " (0.6121602947950253, 0.9999852204371795),\n",
       " (0.6194610778443114, 0.9999852204371795),\n",
       " (0.6195071395670199, 0.9999852204371795),\n",
       " (0.6223859972362966, 0.9999852204371795),\n",
       " (0.6224320589590051, 0.9999852204371795),\n",
       " (0.6239981575310917, 0.9999852204371795),\n",
       " (0.6240442192538, 0.9999852204371795),\n",
       " (0.6342008291110087, 0.9999852204371795),\n",
       " (0.6342468908337172, 0.9999852204371795),\n",
       " (0.6361354214647628, 0.9999852204371795),\n",
       " (0.6361814831874713, 0.9999852204371795),\n",
       " (0.6370566559189313, 0.9999852204371795),\n",
       " (0.6371027176416398, 0.9999852204371795),\n",
       " (0.6423307231690465, 0.9999852204371795),\n",
       " (0.642376784891755, 0.9999852204371795),\n",
       " (0.6565868263473054, 0.9999852204371795),\n",
       " (0.6566328880700139, 0.9999852204371795),\n",
       " (0.6668585905112852, 0.9999852204371795),\n",
       " (0.6669046522339935, 0.9999852204371795),\n",
       " (0.67081989866421, 0.9999852204371795),\n",
       " (0.6708659603869185, 0.9999852204371795),\n",
       " (0.6725011515430677, 0.9999852204371795),\n",
       " (0.6725472132657762, 0.9999852204371795),\n",
       " (0.6729387379087978, 0.9999852204371795),\n",
       " (0.6729847996315063, 0.9999852204371795),\n",
       " (0.6732151082450484, 0.9999852204371795),\n",
       " (0.6732611699677568, 0.9999852204371795),\n",
       " (0.6792491939198526, 0.9999852204371795),\n",
       " (0.6793182865039152, 0.9999852204371795),\n",
       " (0.6797789037309996, 0.9999852204371795),\n",
       " (0.679824965453708, 0.9999852204371795),\n",
       " (0.6800092123445417, 0.9999852204371795),\n",
       " (0.6800552740672501, 0.9999852204371795),\n",
       " (0.6887379087977891, 0.9999852204371795),\n",
       " (0.6887839705204974, 0.9999852204371795),\n",
       " (0.6891294334408107, 0.9999852204371795),\n",
       " (0.6891754951635192, 0.9999852204371795),\n",
       " (0.6944495624136343, 0.9999852204371795),\n",
       " (0.6944956241363427, 0.9999852204371795),\n",
       " (0.7038461538461539, 0.9999852204371795),\n",
       " (0.7038922155688623, 0.9999852204371795),\n",
       " (0.7085214187010594, 0.9999852204371795),\n",
       " (0.7085674804237678, 0.9999852204371795),\n",
       " (0.711262091202211, 0.9999852204371795),\n",
       " (0.7113081529249194, 0.9999852204371795),\n",
       " (0.718770152003685, 0.9999852204371795),\n",
       " (0.7188162137263934, 0.9999852204371795),\n",
       " (0.7225472132657761, 0.9999852204371795),\n",
       " (0.7225932749884846, 0.9999852204371795),\n",
       " (0.7226623675725472, 0.9999852204371795),\n",
       " (0.7227084292952557, 0.9999852204371795),\n",
       " (0.7241824044219254, 0.9999852204371795),\n",
       " (0.7242284661446338, 0.9999852204371795),\n",
       " (0.7251266697374482, 0.9999852204371795),\n",
       " (0.7251727314601566, 0.9999852204371795),\n",
       " (0.7266236757254722, 0.9999852204371795),\n",
       " (0.7266697374481805, 0.9999852204371795),\n",
       " (0.7271533855366191, 0.9999852204371795),\n",
       " (0.7271994472593275, 0.9999852204371795),\n",
       " (0.7313680331644403, 0.9999852204371795),\n",
       " (0.7313680331644403, 1.0),\n",
       " (0.7332335329341317, 1.0),\n",
       " (0.7332795946568401, 1.0),\n",
       " (0.7347305389221557, 1.0),\n",
       " (0.7347766006448642, 1.0),\n",
       " (0.7583141409488715, 1.0),\n",
       " (0.7583602026715799, 1.0),\n",
       " (0.759649930907416, 1.0),\n",
       " (0.7596959926301243, 1.0),\n",
       " (0.768125287885767, 1.0),\n",
       " (0.7681713496084753, 1.0),\n",
       " (0.7689543988945187, 1.0),\n",
       " (0.769000460617227, 1.0),\n",
       " (0.7741133118378627, 1.0),\n",
       " (0.7741593735605712, 1.0),\n",
       " (0.775103638876094, 1.0),\n",
       " (0.7751497005988024, 1.0),\n",
       " (0.7798710271764164, 1.0),\n",
       " (0.7799170888991248, 1.0),\n",
       " (0.7847996315062183, 1.0),\n",
       " (0.7848456932289267, 1.0),\n",
       " (0.7852372178719484, 1.0),\n",
       " (0.7852832795946568, 1.0),\n",
       " (0.787770612620912, 1.0),\n",
       " (0.7878166743436205, 1.0),\n",
       " (0.7909488714877937, 1.0),\n",
       " (0.7909949332105021, 1.0),\n",
       " (0.8015430677107324, 1.0),\n",
       " (0.8015891294334409, 1.0),\n",
       " (0.8025103638876094, 1.0),\n",
       " (0.8025564256103178, 1.0),\n",
       " (0.8064947029018885, 1.0),\n",
       " (0.806540764624597, 1.0),\n",
       " (0.8179410409949333, 1.0),\n",
       " (0.8179871027176416, 1.0),\n",
       " (0.8196453247351451, 1.0),\n",
       " (0.8196913864578536, 1.0),\n",
       " (0.822201750345463, 1.0),\n",
       " (0.8222478120681713, 1.0),\n",
       " (0.8339705204974666, 1.0),\n",
       " (0.834016582220175, 1.0),\n",
       " (0.8390142791340396, 1.0),\n",
       " (0.8390603408567481, 1.0),\n",
       " (0.8420082911100876, 1.0),\n",
       " (0.842054352832796, 1.0),\n",
       " (0.84398894518655, 1.0),\n",
       " (0.8440350069092584, 1.0),\n",
       " (0.8494933210502073, 1.0),\n",
       " (0.8495393827729157, 1.0),\n",
       " (0.8519115614923998, 1.0),\n",
       " (0.8519576232151083, 1.0),\n",
       " (0.8535237217871948, 1.0),\n",
       " (0.8535697835099033, 1.0),\n",
       " (0.8656149239981575, 1.0),\n",
       " (0.8656609857208659, 1.0),\n",
       " (0.8657070474435744, 1.0),\n",
       " (0.8657531091662828, 1.0),\n",
       " (0.8687931828650391, 1.0),\n",
       " (0.8688392445877475, 1.0),\n",
       " (0.8778673422385997, 1.0),\n",
       " (0.8779134039613081, 1.0),\n",
       " (0.8782358360202671, 1.0),\n",
       " (0.8782818977429756, 1.0),\n",
       " (0.8820128972823583, 1.0),\n",
       " (0.8820589590050668, 1.0),\n",
       " (0.8827959465684017, 1.0),\n",
       " (0.88284200829111, 1.0),\n",
       " (0.8915707047443574, 1.0),\n",
       " (0.8916167664670659, 1.0),\n",
       " (0.8951404882542607, 1.0),\n",
       " (0.8951865499769691, 1.0),\n",
       " (0.8977199447259328, 1.0),\n",
       " (0.8977660064486411, 1.0),\n",
       " (0.8998848456932289, 1.0),\n",
       " (0.8999309074159374, 1.0),\n",
       " (0.9133118378627361, 1.0),\n",
       " (0.9133578995854444, 1.0),\n",
       " (0.9242745278673422, 1.0),\n",
       " (0.9243205895900507, 1.0),\n",
       " (0.9300552740672501, 1.0),\n",
       " (0.9301013357899586, 1.0),\n",
       " (0.9367572547213265, 1.0),\n",
       " (0.936803316444035, 1.0),\n",
       " (0.9386688162137264, 1.0),\n",
       " (0.9387148779364348, 1.0),\n",
       " (0.9478120681713496, 1.0),\n",
       " (0.947858129894058, 1.0),\n",
       " (0.9608245048364809, 1.0),\n",
       " (0.9608705665591893, 1.0),\n",
       " (0.9640948871487793, 1.0),\n",
       " (0.9641409488714878, 1.0),\n",
       " (0.983694150161216, 1.0),\n",
       " (0.9837402118839245, 1.0),\n",
       " (1.0, 1.0)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = 0.4\n",
    "list(zip(fpr[(fpr) > thresh], tpr[fpr > thresh]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "total_pred = []\n",
    "total_y = []\n",
    "\n",
    "for batch in gnn_test_loader:\n",
    "    data = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(data)   \n",
    "    y = batch.pid[batch.e[0]] == batch.pid[batch.e[1]]\n",
    "\n",
    "    total_pred += sig(pred).detach().cpu().numpy().tolist()\n",
    "    total_y += y.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'GNN on Biadjacent Embedding')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "f1 = [metrics.f1_score(total_y, np.array(total_pred)>thresh) for thresh in np.arange(0,1,0.1)]\n",
    "ax.plot(np.arange(0,1,0.1), f1)\n",
    "ax.text(0.4, 0.2, \"Best F1: {:.4f} @ r= {:.2f}\".format(max(f1), 0.1*np.argmax(f1)), transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "ax.set_xlabel(\"edge score threshold\"), ax.set_ylabel(\"F1 measure\")\n",
    "ax.set_title(\"GNN on Biadjacent Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter-Gather PyG Original Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class EdgeNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which computes weights for edges of the graph.\n",
    "    For each edge, it selects the associated nodes' features\n",
    "    and applies some fully-connected network layers with a final\n",
    "    sigmoid activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=8, hidden_activation='Tanh',\n",
    "                 layer_norm=True):\n",
    "        super(EdgeNetwork, self).__init__()\n",
    "        self.network = make_mlp(input_dim*2,\n",
    "                                [hidden_dim, hidden_dim, hidden_dim, 1],\n",
    "                                hidden_activation=hidden_activation,\n",
    "                                output_activation=None,\n",
    "                                layer_norm=layer_norm)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Select the features of the associated nodes\n",
    "        start, end = edge_index\n",
    "        x1, x2 = x[start], x[end]\n",
    "        edge_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "        return self.network(edge_inputs).squeeze(-1)\n",
    "\n",
    "class NodeNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which computes new node features on the graph.\n",
    "    For each node, it aggregates the neighbor node features\n",
    "    (separately on the input and output side), and combines\n",
    "    them with the node's previous features in a fully-connected\n",
    "    network to compute the new features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_activation='Tanh',\n",
    "                 layer_norm=True):\n",
    "        super(NodeNetwork, self).__init__()\n",
    "        self.network = make_mlp(input_dim*3, [output_dim]*4,\n",
    "                                hidden_activation=hidden_activation,\n",
    "                                output_activation=None,\n",
    "                                layer_norm=layer_norm)\n",
    "\n",
    "    def forward(self, x, e, edge_index):\n",
    "        start, end = edge_index\n",
    "        # Aggregate edge-weighted incoming/outgoing features\n",
    "        mi = scatter_add(e[:, None] * x[start], end, dim=0, dim_size=x.shape[0])\n",
    "        mo = scatter_add(e[:, None] * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "        node_inputs = torch.cat([mi, mo, x], dim=1)\n",
    "        return self.network(node_inputs)\n",
    "    \n",
    "class GATGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Segment classification graph neural network model.\n",
    "    Consists of an input network, an edge network, and a node network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, hidden_dim=8, n_graph_iters=3,\n",
    "                 hidden_activation=torch.nn.Tanh, layer_norm=True):\n",
    "        super(GATGNN, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "        # Setup the input network\n",
    "        self.input_network = make_mlp(in_channels, [hidden_dim],\n",
    "                                      output_activation=hidden_activation,\n",
    "                                      layer_norm=layer_norm)\n",
    "        # Setup the edge network\n",
    "        self.edge_network = EdgeNetwork(in_channels + hidden_dim, in_channels + hidden_dim,\n",
    "                                        hidden_activation, layer_norm=layer_norm)\n",
    "        # Setup the node layers\n",
    "        self.node_network = NodeNetwork(in_channels + hidden_dim, hidden_dim,\n",
    "                                        hidden_activation, layer_norm=layer_norm)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"Apply forward pass of the model\"\"\"\n",
    "        x_in = x\n",
    "        x = self.input_network(x)\n",
    "        # Shortcut connect the inputs onto the hidden representation\n",
    "        x = torch.cat([x, x_in], dim=-1)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # Loop over iterations of edge and node networks\n",
    "        for i in range(self.n_graph_iters):\n",
    "            x_inital = x\n",
    "            \n",
    "            # Apply edge network\n",
    "            e = torch.sigmoid(self.edge_network(x, edge_index))\n",
    "        \n",
    "            # Apply node network\n",
    "            x = self.node_network(x, e, edge_index)\n",
    "            \n",
    "            # Shortcut connect the inputs onto the hidden representation\n",
    "            x = torch.cat([x, x_in], dim=-1)  \n",
    "            \n",
    "            x = x_inital + x\n",
    "        \n",
    "        return self.edge_network(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": [
     0,
     51
    ]
   },
   "outputs": [],
   "source": [
    "def train_gnn(model, gnn_train_loader, optimizer, m_configs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    data_time, pred_time, loss_time, loss_back_time, optimizer_step = 0, 0, 0, 0, 0\n",
    "    for i, batch in enumerate(gnn_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        tic = tt()\n",
    "        data = batch.to(device)\n",
    "        data_time += tt()-tic\n",
    "        tic = tt()\n",
    "        pred = model(data.x, data.edge_index)\n",
    "        pred_time += tt()-tic\n",
    "        \n",
    "        y = batch.y\n",
    "        tic = tt()\n",
    "        loss = F.binary_cross_entropy_with_logits(pred.float(), y, pos_weight=torch.tensor(m_configs[\"weight\"]))\n",
    "        loss_time += tt()-tic\n",
    "        tic = tt()\n",
    "        loss.backward()\n",
    "        loss_back_time += tt()-tic\n",
    "        tic = tt()\n",
    "        optimizer.step()\n",
    "        optimizer_step += tt()-tic\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        true, false = y.float() > 0.5, y.float() < 0.5\n",
    "        positive, negative = sig(pred) > 0.5, sig(pred) < 0.5\n",
    "\n",
    "        correct += ((sig(pred) > 0.5) == (y.float() > 0.5)).sum().item()\n",
    "        total += len(pred)\n",
    "        if(i%200==0): print(i, \"batches\")\n",
    "        \n",
    "    acc = correct/max(total, 1)\n",
    "    print(\"data_time, pred_time, loss_time, loss_back_time, optimizer_step\")\n",
    "    print(data_time, pred_time, loss_time, loss_back_time, optimizer_step)\n",
    "    return acc, total_loss\n",
    "\n",
    "def evaluate_gnn(model, gnn_test_loader, m_configs):\n",
    "    correct, total_positive, total_true, true_positive, total, total_loss = 0, 1, 0, 0, 0, 0\n",
    "    \n",
    "    for batch in gnn_test_loader:\n",
    "        data = batch.to(device)\n",
    "#         pred = model(data.x, data.edge_index.int())\n",
    "        pred = model(data.x, data.edge_index)\n",
    "              \n",
    "        y = batch.y\n",
    "        loss = F.binary_cross_entropy_with_logits(pred.float(), y.float(), pos_weight=torch.tensor(m_configs[\"weight\"]))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        true, false = y.float() > 0.5, y.float() < 0.5\n",
    "        positive, negative = sig(pred) > 0.5, sig(pred) < 0.5\n",
    "        true_positive += (true & positive).sum().item()\n",
    "        total_positive += max(positive.sum().item(), 1)\n",
    "\n",
    "        correct += ((sig(pred) > 0.5) == (y.float() > 0.5)).sum().item()\n",
    "        total_true += max(true.sum().item(), 1)\n",
    "        total += len(pred)\n",
    "\n",
    "    acc = correct/max(total, 1)\n",
    "    eff = (true_positive / max(total_true, 1))\n",
    "    pur = (true_positive / max(total_positive, 1))\n",
    "     \n",
    "    return acc, eff, pur, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_configs = {\"in_channels\": 3, \"hidden_dim\": 32, \"n_graph_iters\": 8}\n",
    "model = GATGNN(**m_configs).to(device)\n",
    "other_configs = {\"weight\": 3}\n",
    "torch.manual_seed(torch_seed)\n",
    "m_configs.update(other_configs)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "# optimizer = optimizers.FusedAdam(model.parameters(), lr = 0.001, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=10)\n",
    "torch.cuda.reset_max_memory_cached(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.9896533489227295 13.077362775802612 0.39206933975219727 181.28339552879333 4.446962356567383\n",
      "Training loss: 340.7721998691559\n",
      "End of epoch mem: 6.1396484375 MB\n",
      "Epoch: 0, Accuracy: 0.9348, Purity: 0.6970, Efficiency: 0.9367, Loss: 19.7048, LR: 0.001 in time 208.73812127113342\n",
      "0 batches\n",
      "200 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.9938678741455078 12.994366645812988 0.39365077018737793 181.22496461868286 4.3604230880737305\n",
      "Training loss: 181.9768833965063\n",
      "End of epoch mem: 6.1396484375 MB\n",
      "Epoch: 1, Accuracy: 0.9377, Purity: 0.7030, Efficiency: 0.9527, Loss: 17.9121, LR: 0.001 in time 208.51447772979736\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.998703956604004 12.937515497207642 0.3961021900177002 181.26568222045898 4.330440521240234\n",
      "Training loss: 177.05522321164608\n",
      "End of epoch mem: 6.1396484375 MB\n",
      "Epoch: 2, Accuracy: 0.9535, Purity: 0.7642, Efficiency: 0.9607, Loss: 13.9892, LR: 0.001 in time 208.40431952476501\n",
      "0 batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-419928ee1b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-31d20dfeaaf0>\u001b[0m in \u001b[0;36mtrain_gnn\u001b[0;34m(model, gnn_train_loader, optimizer, m_configs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#             scaled_loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mloss_back_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#         torch.nn.utils.clip_grad_norm_(model.parameters(), m_configs[\"clip\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    tic = tt()  \n",
    "    model.train()\n",
    "    acc, train_loss = train_gnn(model, gnn_train_loader, optimizer, m_configs)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc, eff, pur, val_loss = evaluate_gnn(model, gnn_test_loader, m_configs)\n",
    "    scheduler.step(val_loss)\n",
    "#     wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss, \"val_acc\": acc, \"val_pur\": pur, \"val_eff\": eff, \"lr\": scheduler._last_lr[0]})\n",
    "\n",
    "#     save_model(epoch, model, optimizer, scheduler, val_loss, m_configs, 'ResAGNN/'+model_name._name+'.tar')\n",
    "    print(\"End of epoch mem:\", torch.cuda.memory_allocated(device=device)/(1024**2), \"MB\")\n",
    "    print('Epoch: {}, Accuracy: {:.4f}, Purity: {:.4f}, Efficiency: {:.4f}, Loss: {:.4f}, LR: {} in time {}'.format(epoch, acc, pur, eff, val_loss, scheduler._last_lr[0], tt()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    1704 MB |    2364 MB |  110892 GB |  110890 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    1704 MB |    2364 MB |  110892 GB |  110890 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10120 MB |   10120 MB |   10120 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  110338 KB |  227577 KB |   82418 GB |   82418 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1654    |    1791    |   61297 K  |   61296 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1654    |    1791    |   61297 K  |   61296 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     378    |     378    |     378    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     153    |     189    |   28576 K  |   28576 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=device, abbreviated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scatter-Gather PyG Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EdgeNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which computes weights for edges of the graph.\n",
    "    For each edge, it selects the associated nodes' features\n",
    "    and applies some fully-connected network layers with a final\n",
    "    sigmoid activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=8, hidden_activation='Tanh',\n",
    "                 layer_norm=True):\n",
    "        super(EdgeNetwork, self).__init__()\n",
    "        self.network = make_mlp(input_dim*2,\n",
    "                                [hidden_dim, hidden_dim, hidden_dim, 1],\n",
    "                                hidden_activation=hidden_activation,\n",
    "                                output_activation=None,\n",
    "                                layer_norm=layer_norm)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Select the features of the associated nodes\n",
    "        start, end = edge_index\n",
    "        x1, x2 = x[start], x[end]\n",
    "        edge_inputs = torch.cat([x[start], x[end]], dim=1)\n",
    "        return self.network(edge_inputs).squeeze(-1)\n",
    "\n",
    "class NodeNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which computes new node features on the graph.\n",
    "    For each node, it aggregates the neighbor node features\n",
    "    (separately on the input and output side), and combines\n",
    "    them with the node's previous features in a fully-connected\n",
    "    network to compute the new features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_activation='Tanh',\n",
    "                 layer_norm=True):\n",
    "        super(NodeNetwork, self).__init__()\n",
    "        self.network = make_mlp(input_dim*3, [output_dim]*4,\n",
    "                                hidden_activation=hidden_activation,\n",
    "                                output_activation=None,\n",
    "                                layer_norm=layer_norm)\n",
    "\n",
    "    def forward(self, x, e, edge_index):\n",
    "        start, end = edge_index\n",
    "        # Aggregate edge-weighted incoming/outgoing features\n",
    "        mi = scatter_add(e[:, None] * x[start], end, dim=0, dim_size=x.shape[0])\n",
    "        mo = scatter_add(e[:, None] * x[end], start, dim=0, dim_size=x.shape[0])\n",
    "        node_inputs = torch.cat([mi, mo, x], dim=1)\n",
    "        return self.network(node_inputs)\n",
    "    \n",
    "class GATGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Segment classification graph neural network model.\n",
    "    Consists of an input network, an edge network, and a node network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, hidden_dim=8, n_graph_iters=3,\n",
    "                 hidden_activation=torch.nn.Tanh, layer_norm=True):\n",
    "        super(GATGNN, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "        # Setup the input network\n",
    "        self.input_network = make_mlp(in_channels, [hidden_dim],\n",
    "                                      output_activation=hidden_activation,\n",
    "                                      layer_norm=layer_norm)\n",
    "        \n",
    "        self.GATConv = pygnn.GATConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Setup the node layers\n",
    "        self.node_network = make_mlp(hidden_dim,\n",
    "                                        [hidden_dim]*4,\n",
    "                                        output_activation=None,\n",
    "                                           layer_norm=layer_norm)\n",
    "        \n",
    "        # The edge classifier computes final edge scores\n",
    "        self.edge_classifier = make_mlp(2*hidden_dim,\n",
    "                                        [hidden_dim, hidden_dim, hidden_dim, 1],\n",
    "                                        output_activation=None)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"Apply forward pass of the model\"\"\"\n",
    "\n",
    "        x = self.input_network(x)\n",
    "\n",
    "        # Loop over iterations of edge and node networks\n",
    "        for i in range(self.n_graph_iters):\n",
    "            x_inital = x\n",
    "            \n",
    "            # Apply edge network\n",
    "            x = self.GATConv(x, edge_index)\n",
    "            x = self.node_network(x)\n",
    "            \n",
    "            x = x_inital + x        \n",
    "    \n",
    "        start_idx, end_idx = edge_index\n",
    "        return self.edge_classifier(torch.cat([x[start_idx], x[end_idx]], dim=1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     0,
     51
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_gnn(model, gnn_train_loader, optimizer, m_configs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    data_time, pred_time, loss_time, loss_back_time, optimizer_step = 0, 0, 0, 0, 0\n",
    "    for i, batch in enumerate(gnn_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        tic = tt()\n",
    "        data = batch.to(device)\n",
    "        data_time += tt()-tic\n",
    "#         print(\"Edge float\", data.e.max().item(), data.e)\n",
    "#         print(\"Edge long\", data.e.long().max().item(), data.e.long())\n",
    "#         print(\"Edge shape\", data.e.shape)\n",
    "        \n",
    "#         pred = model(data.x, data.edge_index.int())\n",
    "        tic = tt()\n",
    "        pred = model(data.x, data.edge_index)\n",
    "        pred_time += tt()-tic\n",
    "        \n",
    "        y = batch.y\n",
    "#         print(\"y\", np.isnan(y.cpu().detach().numpy()).sum())\n",
    "#         if (np.isnan(pred.cpu().detach().numpy()).sum() == 0):\n",
    "        tic = tt()\n",
    "        loss = F.binary_cross_entropy_with_logits(pred.float(), y, pos_weight=torch.tensor(m_configs[\"weight\"]))\n",
    "        loss_time += tt()-tic\n",
    "#         with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward()\n",
    "        tic = tt()\n",
    "        loss.backward()\n",
    "        loss_back_time += tt()-tic\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), m_configs[\"clip\"])\n",
    "        tic = tt()\n",
    "        optimizer.step()\n",
    "        optimizer_step += tt()-tic\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        true, false = y.float() > 0.5, y.float() < 0.5\n",
    "        positive, negative = sig(pred) > 0.5, sig(pred) < 0.5\n",
    "\n",
    "        correct += ((sig(pred) > 0.5) == (y.float() > 0.5)).sum().item()\n",
    "        total += len(pred)\n",
    "#         else:\n",
    "#             print(\"Broke!\")\n",
    "        if(i%200==0): print(i, \"batches\")\n",
    "        \n",
    "    acc = correct/max(total, 1)\n",
    "    print(\"data_time, pred_time, loss_time, loss_back_time, optimizer_step\")\n",
    "    print(data_time, pred_time, loss_time, loss_back_time, optimizer_step)\n",
    "    return acc, total_loss\n",
    "\n",
    "def evaluate_gnn(model, gnn_test_loader, m_configs):\n",
    "    correct, total_positive, total_true, true_positive, total, total_loss = 0, 1, 0, 0, 0, 0\n",
    "    \n",
    "    for batch in gnn_test_loader:\n",
    "        data = batch.to(device)\n",
    "#         pred = model(data.x, data.edge_index.int())\n",
    "        pred = model(data.x, data.edge_index)\n",
    "              \n",
    "        y = batch.y\n",
    "\n",
    "#         print(\"y\", y, \"length\", len(y))\n",
    "#         if (np.isnan(pred.cpu().detach().numpy()).sum() == 0):\n",
    "        loss = F.binary_cross_entropy_with_logits(pred.float(), y.float(), pos_weight=torch.tensor(m_configs[\"weight\"]))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        true, false = y.float() > 0.5, y.float() < 0.5\n",
    "#         print(\"true\", true, \"sum\", true.float().sum().item())\n",
    "        positive, negative = sig(pred) > 0.5, sig(pred) < 0.5\n",
    "        true_positive += (true & positive).sum().item()\n",
    "        total_positive += max(positive.sum().item(), 1)\n",
    "\n",
    "        correct += ((sig(pred) > 0.5) == (y.float() > 0.5)).sum().item()\n",
    "        total_true += max(true.sum().item(), 1)\n",
    "\n",
    "#         print(\"True positive:\", (true & positive).sum().item(), \"True:\", true.sum().item(), \"Positive\", positive.sum().item())\n",
    "\n",
    "        total += len(pred)\n",
    "\n",
    "    acc = correct/max(total, 1)\n",
    "    eff = (true_positive / max(total_true, 1))\n",
    "    pur = (true_positive / max(total_positive, 1))\n",
    "    \n",
    "#     print(\"Num total_true:\", total_true, \"Num total_positive:\", total_positive, \"Num true_positive:\", true_positive, \"Total edges:\", total)\n",
    "        \n",
    "    return acc, eff, pur, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m_configs = {\"in_channels\": 3, \"hidden_dim\": 32, \"n_graph_iters\": 8}\n",
    "model = GATGNN(**m_configs).to(device)\n",
    "other_configs = {\"weight\": 3}\n",
    "torch.manual_seed(torch_seed)\n",
    "m_configs.update(other_configs)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "# optimizer = optimizers.FusedAdam(model.parameters(), lr = 0.001, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.820138931274414 12.623053789138794 0.35951733589172363 28.219863653182983 3.8809192180633545\n",
      "Training loss: 758.5221596360207\n",
      "End of epoch mem: 97.51318359375 MB\n",
      "Epoch: 0, Accuracy: 0.8217, Purity: 0.2654, Efficiency: 0.1629, Loss: 72.1881, LR: 0.001 in time 51.52574038505554\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.8527512550354004 12.566985368728638 0.3548462390899658 28.18063735961914 3.8620123863220215\n",
      "Training loss: 691.3220467567444\n",
      "End of epoch mem: 97.51318359375 MB\n",
      "Epoch: 1, Accuracy: 0.7814, Purity: 0.3583, Efficiency: 0.7312, Loss: 57.0352, LR: 0.001 in time 51.51437950134277\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.8375728130340576 12.673328638076782 0.3487412929534912 28.170597314834595 3.8666555881500244\n",
      "Training loss: 465.4721156358719\n",
      "End of epoch mem: 97.51318359375 MB\n",
      "Epoch: 2, Accuracy: 0.8281, Purity: 0.4402, Efficiency: 0.8889, Loss: 42.4142, LR: 0.001 in time 51.64340281486511\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.8451581001281738 12.598452091217041 0.3412959575653076 28.211230039596558 3.8940415382385254\n",
      "Training loss: 407.7664702832699\n",
      "End of epoch mem: 97.51318359375 MB\n",
      "Epoch: 3, Accuracy: 0.8331, Purity: 0.4497, Efficiency: 0.9189, Loss: 39.6646, LR: 0.001 in time 51.54716515541077\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.836395263671875 12.600089311599731 0.33626866340637207 28.20947027206421 3.8589627742767334\n",
      "Training loss: 385.2891854047775\n",
      "End of epoch mem: 97.51318359375 MB\n",
      "Epoch: 4, Accuracy: 0.8481, Purity: 0.4751, Efficiency: 0.9281, Loss: 36.8494, LR: 0.001 in time 51.34600496292114\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.8267459869384766 12.603410482406616 0.3415544033050537 28.139522552490234 3.848558187484741\n",
      "Training loss: 359.11473593115807\n",
      "End of epoch mem: 97.51318359375 MB\n",
      "Epoch: 5, Accuracy: 0.8672, Purity: 0.5112, Efficiency: 0.9246, Loss: 34.2250, LR: 0.001 in time 51.493857860565186\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.8356373310089111 12.589422941207886 0.34561848640441895 28.170442581176758 3.8716061115264893\n",
      "Training loss: 336.28184017539024\n",
      "End of epoch mem: 97.51318359375 MB\n",
      "Epoch: 6, Accuracy: 0.8807, Purity: 0.5400, Efficiency: 0.9344, Loss: 32.1562, LR: 0.001 in time 51.52289056777954\n",
      "0 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-51-419928ee1b43>\", line 4, in <module>\n",
      "    acc, train_loss = train_gnn(model, gnn_train_loader, optimizer, m_configs)\n",
      "  File \"<ipython-input-49-31d20dfeaaf0>\", line 29, in train_gnn\n",
      "    loss.backward()\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 98, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/inspect.py\", line 745, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-419928ee1b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-31d20dfeaaf0>\u001b[0m in \u001b[0;36mtrain_gnn\u001b[0;34m(model, gnn_train_loader, optimizer, m_configs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mloss_back_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2044\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2046\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2047\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1436\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1193\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    tic = tt()  \n",
    "    model.train()\n",
    "    acc, train_loss = train_gnn(model, gnn_train_loader, optimizer, m_configs)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc, eff, pur, val_loss = evaluate_gnn(model, gnn_test_loader, m_configs)\n",
    "    scheduler.step(val_loss)\n",
    "#     wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss, \"val_acc\": acc, \"val_pur\": pur, \"val_eff\": eff, \"lr\": scheduler._last_lr[0]})\n",
    "\n",
    "#     save_model(epoch, model, optimizer, scheduler, val_loss, m_configs, 'ResAGNN/'+model_name._name+'.tar')\n",
    "    print(\"End of epoch mem:\", torch.cuda.memory_allocated(device=device)/(1024**2), \"MB\")\n",
    "    print('Epoch: {}, Accuracy: {:.4f}, Purity: {:.4f}, Efficiency: {:.4f}, Loss: {:.4f}, LR: {} in time {}'.format(epoch, acc, pur, eff, val_loss, scheduler._last_lr[0], tt()-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Memory Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    4927 KB |  754480 KB |   21728 GB |   21728 GB |\n",
      "|       from large pool |    2847 KB |  748023 KB |   19646 GB |   19646 GB |\n",
      "|       from small pool |    2080 KB |   46028 KB |    2081 GB |    2081 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    4927 KB |  754480 KB |   21728 GB |   21728 GB |\n",
      "|       from large pool |    2847 KB |  748023 KB |   19646 GB |   19646 GB |\n",
      "|       from small pool |    2080 KB |   46028 KB |    2081 GB |    2081 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    1662 MB |    1662 MB |    1662 MB |       0 B  |\n",
      "|       from large pool |    1614 MB |    1614 MB |    1614 MB |       0 B  |\n",
      "|       from small pool |      48 MB |      48 MB |      48 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   25792 KB |  201894 KB |   16541 GB |   16541 GB |\n",
      "|       from large pool |   17632 KB |  195186 KB |   14289 GB |   14289 GB |\n",
      "|       from small pool |    8160 KB |   16058 KB |    2251 GB |    2251 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     182    |     325    |   12474 K  |   12474 K  |\n",
      "|       from large pool |       1    |     112    |    4002 K  |    4002 K  |\n",
      "|       from small pool |     181    |     259    |    8471 K  |    8471 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     182    |     325    |   12474 K  |   12474 K  |\n",
      "|       from large pool |       1    |     112    |    4002 K  |    4002 K  |\n",
      "|       from small pool |     181    |     259    |    8471 K  |    8471 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      71    |      71    |      71    |       0    |\n",
      "|       from large pool |      47    |      47    |      47    |       0    |\n",
      "|       from small pool |      24    |      24    |      24    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      11    |      56    |    5827 K  |    5827 K  |\n",
      "|       from large pool |       2    |      24    |    2448 K  |    2448 K  |\n",
      "|       from small pool |       9    |      37    |    3378 K  |    3378 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=device, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.reset_max_memory_cached(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated(device=device)/(1024**2), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test Batch Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fix_y_batch(batch):\n",
    "    \n",
    "    data = Data(x = batch.x, y = batch.y, edge_index = batch.e.long())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 607 ms, sys: 1.33 s, total: 1.94 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gnn_train_dataset_fix = [fix_y_batch(batch) for batch in gnn_train_dataset]\n",
    "gnn_test_dataset_fix = [fix_y_batch(batch) for batch in gnn_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 87 µs, sys: 0 ns, total: 87 µs\n",
      "Wall time: 89.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gnn_train_loader = DataLoader(gnn_train_dataset_fix, batch_size=1, shuffle=False, num_workers=1)\n",
    "gnn_test_loader = DataLoader(gnn_test_dataset_fix, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples evaluated\n",
      "CPU times: user 31.3 s, sys: 11.4 s, total: 42.7 s\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(gnn_train_loader):            \n",
    "#             print(batch)\n",
    "            data = batch.to(device)\n",
    "            pred = model(data.x, data.edge_index.int())\n",
    "            if(i%100==0): print(i, \"samples evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter-Gather Built-in PyG Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GATGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Segment classification graph neural network model.\n",
    "    Consists of an input network, an edge network, and a node network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, hidden_dim=8, n_graph_iters=3,\n",
    "                 hidden_activation=torch.nn.Tanh, layer_norm=True):\n",
    "        super(GATGNN, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "        # Setup the input network\n",
    "        self.input_network = make_mlp(in_channels, [hidden_dim],\n",
    "                                      output_activation=hidden_activation,\n",
    "                                      layer_norm=layer_norm)\n",
    "        \n",
    "        self.GATConv = pygnn.GATConv(hidden_dim, hidden_dim, add_self_loops=True)\n",
    "        \n",
    "        # Setup the node layers\n",
    "        self.node_network = node_network = make_mlp(hidden_dim,\n",
    "                                        [hidden_dim]*2,\n",
    "                                        output_activation=None,\n",
    "                                           layer_norm=layer_norm)\n",
    "        \n",
    "        # The edge classifier computes final edge scores\n",
    "        self.edge_classifier = make_mlp(2*hidden_dim,\n",
    "                                        [hidden_dim, 1],\n",
    "                                        output_activation=None)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"Apply forward pass of the model\"\"\"\n",
    "\n",
    "        x_in = x\n",
    "        x = self.input_network(x)\n",
    "\n",
    "        # Loop over iterations of edge and node networks\n",
    "        for i in range(self.n_graph_iters):\n",
    "            x_inital = x\n",
    "            \n",
    "            # Apply edge network\n",
    "            x = self.GATConv(x, edge_index)\n",
    "            x = self.node_network(x)\n",
    "            \n",
    "            x = x_inital + x        \n",
    "    \n",
    "        _, weights = self.GATConv(x, edge_index, return_attention_weights=True)\n",
    "        \n",
    "        return weights[0], weights[1].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "code_folding": [
     0,
     51
    ]
   },
   "outputs": [],
   "source": [
    "def train_gnn(model, gnn_train_loader, optimizer, m_configs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    data_time, pred_time, loss_time, loss_back_time, optimizer_step = 0, 0, 0, 0, 0\n",
    "    for i, batch in enumerate(gnn_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        tic = tt()\n",
    "        data = batch.to(device)\n",
    "        data_time += tt()-tic\n",
    "#         print(\"Edge float\", data.e.max().item(), data.e)\n",
    "#         print(\"Edge long\", data.e.long().max().item(), data.e.long())\n",
    "#         print(\"Edge shape\", data.e.shape)\n",
    "        \n",
    "#         pred = model(data.x, data.edge_index.int())\n",
    "        tic = tt()\n",
    "        edges, pred = model(data.x, data.edge_index)\n",
    "        pred_time += tt()-tic\n",
    "        \n",
    "        pred = pred[edges[0] != edges[1]]\n",
    "        edges = edges[:, edges[0] != edges[1]]\n",
    "        y = batch.y\n",
    "        \n",
    "\n",
    "#         print(edges.shape, pred.shape, data.edge_index.shape, y.shape)\n",
    "        weight = y + (y==0).float()*(1/m_configs[\"weight\"])\n",
    "#         print(weight)\n",
    "        \n",
    "        tic = tt()\n",
    "        loss = F.binary_cross_entropy(pred.float(), y, weight=weight)\n",
    "        loss_time += tt()-tic\n",
    "        tic = tt()\n",
    "        loss.backward()\n",
    "        loss_back_time += tt()-tic\n",
    "        tic = tt()\n",
    "        optimizer.step()\n",
    "        optimizer_step += tt()-tic\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        true, false = y.float() > 0.5, y.float() < 0.5\n",
    "        positive, negative = pred > 0.5, pred < 0.5\n",
    "\n",
    "        correct += ((pred > 0.5) == (y.float() > 0.5)).sum().item()\n",
    "        total += len(pred)\n",
    "#         else:\n",
    "#             print(\"Broke!\")\n",
    "        if(i%200==0): print(i, \"batches\")\n",
    "        \n",
    "    acc = correct/max(total, 1)\n",
    "    print(\"data_time, pred_time, loss_time, loss_back_time, optimizer_step\")\n",
    "    print(data_time, pred_time, loss_time, loss_back_time, optimizer_step)\n",
    "    return acc, total_loss\n",
    "\n",
    "def evaluate_gnn(model, gnn_test_loader, m_configs):\n",
    "    correct, total_positive, total_true, true_positive, total, total_loss = 0, 1, 0, 0, 0, 0\n",
    "    \n",
    "    for batch in gnn_test_loader:\n",
    "        data = batch.to(device)\n",
    "#         pred = model(data.x, data.edge_index.int())\n",
    "        edges, pred = model(data.x, data.edge_index)\n",
    "        pred = pred[edges[0] != edges[1]]\n",
    "        edges = edges[:, edges[0] != edges[1]]\n",
    "        y = batch.y\n",
    "\n",
    "#         print(\"y\", y, \"length\", len(y))\n",
    "#         if (np.isnan(pred.cpu().detach().numpy()).sum() == 0):\n",
    "        weight = y + (y==0).float()*(1/m_configs[\"weight\"])\n",
    "        loss = F.binary_cross_entropy(pred.float(), y, weight=weight)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        true, false = y.float() > 0.5, y.float() < 0.5\n",
    "#         print(\"true\", true, \"sum\", true.float().sum().item())\n",
    "        positive, negative = pred > 0.5, pred < 0.5\n",
    "        true_positive += (true & positive).sum().item()\n",
    "        total_positive += max(positive.sum().item(), 1)\n",
    "\n",
    "        correct += ((pred > 0.5) == (y.float() > 0.5)).sum().item()\n",
    "        total_true += max(true.sum().item(), 1)\n",
    "\n",
    "#         print(\"True positive:\", (true & positive).sum().item(), \"True:\", true.sum().item(), \"Positive\", positive.sum().item())\n",
    "\n",
    "        total += len(pred)\n",
    "\n",
    "    acc = correct/max(total, 1)\n",
    "    eff = (true_positive / max(total_true, 1))\n",
    "    pur = (true_positive / max(total_positive, 1))\n",
    "    \n",
    "#     print(\"Num total_true:\", total_true, \"Num total_positive:\", total_positive, \"Num true_positive:\", true_positive, \"Total edges:\", total)\n",
    "        \n",
    "    return acc, eff, pur, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_configs = {\"in_channels\": 3, \"hidden_dim\": 32, \"n_graph_iters\": 8}\n",
    "model = GATGNN(**m_configs).to(device)\n",
    "other_configs = {\"weight\": 3}\n",
    "torch.manual_seed(torch_seed)\n",
    "m_configs.update(other_configs)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "# optimizer = optimizers.FusedAdam(model.parameters(), lr = 0.001, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=10)\n",
    "torch.cuda.reset_max_memory_cached(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.8630015850067139 11.298678159713745 0.16671490669250488 17.794898748397827 1.762826919555664\n",
      "Training loss: 321.11012029647827\n",
      "End of epoch mem: 1693.509765625 MB\n",
      "Epoch: 0, Accuracy: 0.8618, Purity: 0.5254, Efficiency: 0.0187, Loss: 32.1886, LR: 0.001 in time 37.27451252937317\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.9733922481536865 11.325664520263672 0.1690375804901123 17.76284694671631 1.7714216709136963\n",
      "Training loss: 319.9882273674011\n",
      "End of epoch mem: 1693.509765625 MB\n",
      "Epoch: 1, Accuracy: 0.8618, Purity: 0.5276, Efficiency: 0.0190, Loss: 32.1732, LR: 0.001 in time 37.467103242874146\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n",
      "800 batches\n",
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "1.9007835388183594 11.335787534713745 0.1678924560546875 17.780579328536987 1.7727999687194824\n",
      "Training loss: 319.9193052351475\n",
      "End of epoch mem: 1693.509765625 MB\n",
      "Epoch: 2, Accuracy: 0.8619, Purity: 0.5295, Efficiency: 0.0193, Loss: 32.1649, LR: 0.001 in time 37.39362716674805\n",
      "0 batches\n",
      "200 batches\n",
      "400 batches\n",
      "600 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x2aab21f61280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/multiprocessing/popen_fork.py\", line 47, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-419928ee1b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-1e4e7d64d1f0>\u001b[0m in \u001b[0;36mtrain_gnn\u001b[0;34m(model, gnn_train_loader, optimizer, m_configs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mloss_back_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    tic = tt()  \n",
    "    model.train()\n",
    "    acc, train_loss = train_gnn(model, gnn_train_loader, optimizer, m_configs)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc, eff, pur, val_loss = evaluate_gnn(model, gnn_test_loader, m_configs)\n",
    "    scheduler.step(val_loss)\n",
    "#     wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss, \"val_acc\": acc, \"val_pur\": pur, \"val_eff\": eff, \"lr\": scheduler._last_lr[0]})\n",
    "\n",
    "#     save_model(epoch, model, optimizer, scheduler, val_loss, m_configs, 'ResAGNN/'+model_name._name+'.tar')\n",
    "    print(\"End of epoch mem:\", torch.cuda.memory_allocated(device=device)/(1024**2), \"MB\")\n",
    "    print('Epoch: {}, Accuracy: {:.4f}, Purity: {:.4f}, Efficiency: {:.4f}, Loss: {:.4f}, LR: {} in time {}'.format(epoch, acc, pur, eff, val_loss, scheduler._last_lr[0], tt()-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse PyG Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_y_batch(batch):\n",
    "    \n",
    "    data = Data(x = batch.x, y = batch.y, edge_index = batch.e.long())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 640 ms, sys: 1.26 s, total: 1.9 s\n",
      "Wall time: 994 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gnn_train_dataset_fix = [fix_y_batch(batch) for batch in gnn_train_dataset]\n",
    "gnn_test_dataset_fix = [fix_y_batch(batch) for batch in gnn_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70 µs, sys: 0 ns, total: 70 µs\n",
      "Wall time: 73 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gnn_train_loader = DataLoader(gnn_train_dataset_fix, batch_size=1, shuffle=False, num_workers=1)\n",
    "gnn_test_loader = DataLoader(gnn_test_dataset_fix, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sparse(batch):\n",
    "    e = batch.e.long()\n",
    "    data = Data(x = batch.x, y = batch.y, edge_index = e, adj_t = SparseTensor(row=e[0], col=e[1]).t())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sparse(batch):\n",
    "    \n",
    "    data = Data(x = batch.x, y = batch.y, edge_index = batch.e.long(), \n",
    "                adj_t = SparseTensor(row=batch.e.long()[0], col=batch.e.long()[1], sparse_sizes=(len(batch.x), len(batch.x))).t())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 13.9 s, total: 1min 15s\n",
      "Wall time: 56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gnn_train_dataset_fix = [add_sparse(batch) for batch in gnn_train_dataset]\n",
    "gnn_test_dataset_fix = [add_sparse(batch) for batch in gnn_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.6 ms, sys: 26.4 ms, total: 50.9 ms\n",
      "Wall time: 461 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gnn_train_loader = DataLoader(gnn_train_dataset_fix, batch_size=1, shuffle=False, num_workers=1)\n",
    "gnn_test_loader = DataLoader(gnn_test_dataset_fix, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_gnn(model, gnn_train_loader, optimizer, m_configs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    data_time, pred_time, loss_time, loss_back_time, optimizer_step = 0, 0, 0, 0, 0\n",
    "    for i, batch in enumerate(gnn_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        tic = tt()\n",
    "        data = batch.to(device)\n",
    "        data_time += tt()-tic\n",
    "#         print(\"Edge float\", data.e.max().item(), data.e)\n",
    "#         print(\"Edge long\", data.e.long().max().item(), data.e.long())\n",
    "#         print(\"Edge shape\", data.e.shape)\n",
    "        \n",
    "#         pred = model(data.x, data.edge_index.int())\n",
    "        tic = tt()\n",
    "#         pred = model(data.x, data.edge_index)\n",
    "        pred = model(data.x, data.adj_t, data.edge_index)\n",
    "        pred_time += tt()-tic\n",
    "#         print(\"Pred type\", pred.type())\n",
    "        y = batch.y\n",
    "#         print(\"y\", np.isnan(y.cpu().detach().numpy()).sum())\n",
    "#         if (np.isnan(pred.cpu().detach().numpy()).sum() == 0):\n",
    "        tic = tt()\n",
    "        loss = F.binary_cross_entropy_with_logits(pred.float(), y, pos_weight=torch.tensor(m_configs[\"weight\"]))\n",
    "        loss_time += tt()-tic\n",
    "        tic = tt()\n",
    "#         with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward()\n",
    "        loss.backward()\n",
    "        loss_back_time += tt()-tic\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), m_configs[\"clip\"])\n",
    "        tic = tt()\n",
    "        optimizer.step()\n",
    "        optimizer_step += tt()-tic\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        true, false = y.float() > 0.5, y.float() < 0.5\n",
    "        positive, negative = sig(pred) > 0.5, sig(pred) < 0.5\n",
    "\n",
    "        correct += ((sig(pred) > 0.5) == (y.float() > 0.5)).sum().item()\n",
    "        total += len(pred)\n",
    "#         else:\n",
    "#             print(\"Broke!\")\n",
    "#         if(i%50==0): print(i, \"batches\")\n",
    "        \n",
    "    acc = correct/max(total, 1)\n",
    "    print(\"data_time, pred_time, loss_time, loss_back_time, optimizer_step\")\n",
    "    print(data_time, pred_time, loss_time, loss_back_time, optimizer_step)\n",
    "    return acc, total_loss\n",
    "\n",
    "def evaluate_gnn(model, gnn_test_loader, m_configs):\n",
    "    correct, total_positive, total_true, true_positive, total, total_loss = 0, 1, 0, 0, 0, 0\n",
    "    \n",
    "    for batch in gnn_test_loader:\n",
    "        data = batch.to(device)\n",
    "#         pred = model(data.x, data.edge_index.int())\n",
    "#         pred = model(data.x, data.edge_index)\n",
    "        pred = model(data.x, data.adj_t, data.edge_index)\n",
    "              \n",
    "        y = batch.y\n",
    "\n",
    "#         print(\"y\", y, \"length\", len(y))\n",
    "#         if (np.isnan(pred.cpu().detach().numpy()).sum() == 0):\n",
    "        loss = F.binary_cross_entropy_with_logits(pred.float(), y.float(), pos_weight=torch.tensor(m_configs[\"weight\"]))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        true, false = y.float() > 0.5, y.float() < 0.5\n",
    "#         print(\"true\", true, \"sum\", true.float().sum().item())\n",
    "        positive, negative = sig(pred) > 0.5, sig(pred) < 0.5\n",
    "        true_positive += (true & positive).sum().item()\n",
    "        total_positive += max(positive.sum().item(), 1)\n",
    "\n",
    "        correct += ((sig(pred) > 0.5) == (y.float() > 0.5)).sum().item()\n",
    "        total_true += max(true.sum().item(), 1)\n",
    "\n",
    "#         print(\"True positive:\", (true & positive).sum().item(), \"True:\", true.sum().item(), \"Positive\", positive.sum().item())\n",
    "\n",
    "        total += len(pred)\n",
    "\n",
    "    acc = correct/max(total, 1)\n",
    "    eff = (true_positive / max(total_true, 1))\n",
    "    pur = (true_positive / max(total_positive, 1))\n",
    "    \n",
    "#     print(\"Num total_true:\", total_true, \"Num total_positive:\", total_positive, \"Num true_positive:\", true_positive, \"Total edges:\", total)\n",
    "        \n",
    "    return acc, eff, pur, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GinGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Segment classification graph neural network model.\n",
    "    Consists of an input network, an edge network, and a node network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, hidden_dim=8, n_graph_iters=3,\n",
    "                 hidden_activation=torch.nn.Tanh, layer_norm=True):\n",
    "        super(GinGNN, self).__init__()\n",
    "        self.n_graph_iters = n_graph_iters\n",
    "        # Setup the input network\n",
    "        self.input_network = make_mlp(in_channels, [hidden_dim],\n",
    "                                      output_activation=hidden_activation,\n",
    "                                      layer_norm=layer_norm)\n",
    "        \n",
    "        # Setup the node layers\n",
    "        node_network = make_mlp(hidden_dim,\n",
    "                                        [hidden_dim]*2,\n",
    "                                        output_activation=None,\n",
    "                                           layer_norm=layer_norm)\n",
    "        \n",
    "        # The GIN Conv layer\n",
    "        self.gin_conv = pygnn.GINConv(nn=node_network)\n",
    "                \n",
    "        # The edge classifier computes final edge scores\n",
    "        self.edge_classifier = make_mlp(2*hidden_dim,\n",
    "                                        [hidden_dim, 1],\n",
    "                                        output_activation=None)\n",
    "        \n",
    "    def forward(self, x, adj_t, edge_index):\n",
    "        \n",
    "        \"\"\"Apply forward pass of the model\"\"\"\n",
    "        \n",
    "        x = self.input_network(x)\n",
    "        \n",
    "        # Loop over iterations of edge and node networks\n",
    "        for i in range(self.n_graph_iters):\n",
    "            x_inital = x\n",
    "            \n",
    "            # Apply GIN Conv\n",
    "            x = self.gin_conv(x, edge_index)\n",
    "#             print(x.shape)\n",
    "#                                 \n",
    "            x = x_inital + x\n",
    "#         if type(adj_t) is SparseTensor:\n",
    "#             end_idx, start_idx, _ = adj_t.coo()\n",
    "#         else:\n",
    "        start_idx, end_idx = edge_index\n",
    "#         clf_inputs = torch.cat([x[start_idx], x[end_idx]], dim=1)\n",
    "#         print(x.shape)\n",
    "#         print(clf_inputs.shape)\n",
    "        return self.edge_classifier(torch.cat([x[start_idx], x[end_idx]], dim=1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_configs = {\"in_channels\": 3, \"hidden_dim\": 512, \"n_graph_iters\": 6}\n",
    "model = GinGNN(**m_configs).to(device)\n",
    "other_configs = {\"weight\": 3}\n",
    "torch.manual_seed(torch_seed)\n",
    "m_configs.update(other_configs)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
    "# optimizer = optimizers.FusedAdam(model.parameters(), lr = 0.001, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_time, pred_time, loss_time, loss_back_time, optimizer_step\n",
      "4.200277805328369 3.403604030609131 0.29317760467529297 64.32442736625671 1.3901522159576416\n",
      "Training loss: 744.6156910657883\n",
      "Epoch: 0, Accuracy: 0.8275, Purity: 0.3730, Efficiency: 0.3615, Loss: 65.1918, LR: 0.001 in time 118.89290928840637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x2aab33250820>\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/multiprocessing/popen_fork.py\", line 47, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/global/homes/d/danieltm/.conda/envs/latest-pyg/lib/python3.8/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-7b9e12308348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-a088b0391e3c>\u001b[0m in \u001b[0;36mtrain_gnn\u001b[0;34m(model, gnn_train_loader, optimizer, m_configs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#         with amp.scale_loss(loss, optimizer) as scaled_loss:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#             scaled_loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss_back_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#         torch.nn.utils.clip_grad_norm_(model.parameters(), m_configs[\"clip\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/latest-pyg/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# torch.backends.cudnn.benchmark = True\n",
    "for epoch in range(100):\n",
    "    tic = tt()  \n",
    "    model.train()\n",
    "    acc, train_loss = train_gnn(model, gnn_train_loader, optimizer, m_configs)\n",
    "    print(\"Training loss:\", train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc, eff, pur, val_loss = evaluate_gnn(model, gnn_test_loader, m_configs)\n",
    "    scheduler.step(val_loss)\n",
    "#     wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss, \"val_acc\": acc, \"val_pur\": pur, \"val_eff\": eff, \"lr\": scheduler._last_lr[0]})\n",
    "\n",
    "#     save_model(epoch, model, optimizer, scheduler, val_loss, m_configs, 'ResAGNN/'+model_name._name+'.tar')\n",
    "#     print(\"End of epoch mem:\", torch.cuda.memory_allocated(device=device)/(1024**2), \"MB\")\n",
    "    print('Epoch: {}, Accuracy: {:.4f}, Purity: {:.4f}, Efficiency: {:.4f}, Loss: {:.4f}, LR: {} in time {}'.format(epoch, acc, pur, eff, val_loss, scheduler._last_lr[0], tt()-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 ms, sys: 212 ms, total: 223 ms\n",
      "Wall time: 687 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch = next(iter(gnn_train_loader))\n",
    "data = batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 3 µs, total: 22 µs\n",
      "Wall time: 24.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = batch.x\n",
    "e = batch.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.68 ms, sys: 434 µs, total: 3.12 ms\n",
      "Wall time: 3.17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_nodes = len(x)\n",
    "adj_t = SparseTensor(row=e[0], col=e[1], sparse_sizes=(num_nodes, num_nodes)).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22780, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(row=tensor([ 4061,  4061,  4061,  ..., 22779, 22779, 22779], device='cuda:0'),\n",
       "             col=tensor([   88,    98,   110,  ..., 21655, 21656, 21657], device='cuda:0'),\n",
       "             size=(22780, 22780), nnz=179362, density=0.03%)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end, _ = adj_t.t().coo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21747, device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(row=tensor([ 4061,  4061,  4061,  ..., 22779, 22779, 22779], device='cuda:0'),\n",
       "             col=tensor([   88,    98,   110,  ..., 21655, 21656, 21657], device='cuda:0'),\n",
       "             size=(22780, 21748), nnz=179362, density=0.04%)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(adj_t) is SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 985 µs, sys: 0 ns, total: 985 µs\n",
      "Wall time: 2.49 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([    0,     0,     0,  ..., 21746, 21746, 21747], device='cuda:0'),\n",
       " tensor([ 4089,  4097,  4105,  ..., 22743, 22744, 22506], device='cuda:0'),\n",
       " None)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "adj_t.t().coo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the node layers\n",
    "node_network = make_mlp(3, [512]*2, output_activation=None, layer_norm=True)\n",
    "\n",
    "# The GIN Conv layer\n",
    "gin_conv = pygnn.GINConv(nn=node_network).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 ms, sys: 156 µs, total: 1.82 ms\n",
      "Wall time: 1.64 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6134, -0.0714,  1.0419,  ...,  0.1455, -0.2651, -0.1499],\n",
       "        [ 0.2874,  0.2170,  0.9679,  ...,  0.0421, -0.1022, -0.1671],\n",
       "        [ 0.6880, -0.2493,  0.3290,  ...,  0.0938, -0.4068,  0.4703],\n",
       "        ...,\n",
       "        [ 0.2621,  0.6093,  0.2323,  ..., -0.1145, -0.3346,  0.4759],\n",
       "        [ 0.2544,  0.6024,  0.2821,  ..., -0.0802, -0.3579,  0.4966],\n",
       "        [ 0.2563,  0.6002,  0.2796,  ..., -0.0773, -0.3572,  0.4993]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gin_conv(x, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 ms, sys: 0 ns, total: 1.39 ms\n",
      "Wall time: 1.29 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6134, -0.0714,  1.0419,  ...,  0.1455, -0.2651, -0.1499],\n",
       "        [ 0.2874,  0.2170,  0.9679,  ...,  0.0421, -0.1022, -0.1671],\n",
       "        [ 0.6880, -0.2493,  0.3290,  ...,  0.0938, -0.4068,  0.4703],\n",
       "        ...,\n",
       "        [ 0.2621,  0.6093,  0.2323,  ..., -0.1145, -0.3346,  0.4759],\n",
       "        [ 0.2544,  0.6024,  0.2821,  ..., -0.0802, -0.3579,  0.4966],\n",
       "        [ 0.2563,  0.6002,  0.2796,  ..., -0.0773, -0.3572,  0.4993]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gin_conv(x, adj_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGL Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as dglfn\n",
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 447 µs, sys: 134 ms, total: 134 ms\n",
      "Wall time: 337 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch = next(iter(gnn_train_loader))\n",
    "# data = batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 6 µs, total: 9 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = batch.x\n",
    "e = batch.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0316, -0.2973, -0.3713],\n",
       "        [ 0.0336, -0.9930, -0.3232],\n",
       "        [ 0.0343,  0.9876, -0.2920],\n",
       "        ...,\n",
       "        [ 1.0184,  0.8549,  0.9814],\n",
       "        [ 1.0223,  0.8899,  1.0030],\n",
       "        [ 1.0234,  0.9314,  0.9922]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   88,    98,   110,  ..., 21655, 21656, 21657],\n",
       "        [ 4061,  4061,  4061,  ..., 22779, 22779, 22779]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = DGLGraph((e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.ndata['x'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DGLGraph(num_nodes=22780, num_edges=179362,\n",
       "         ndata_schemes={'x': Scheme(shape=(3,), dtype=torch.float32)}\n",
       "         edata_schemes={})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0316, -0.2973, -0.3713])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.ndata['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Latest-PyG",
   "language": "python",
   "name": "latest-pyg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
